[
  {
    "objectID": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#defining-the-problem",
    "href": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#defining-the-problem",
    "title": "Matrix representation of quantum circuits - notations and gotchas",
    "section": "Defining the problem",
    "text": "Defining the problem\nOK, so what is the problem? Consider the following simple circuit built with qiskit:\n\n\nCode\nimport numpy as np\nfrom qiskit import QuantumCircuit\nfrom qiskit.quantum_info import Operator, Statevector\n\nqc = QuantumCircuit(2)\nqc.x(0)\nqc.y(1)\nqc.cx(0,1)\nqc.draw(output='mpl')\n\n\n\n\n\nIt is not hard or ambiguous to interpret what this circuit does by inspecting the diagram. Say the input state is \\(q_0=|0\\rangle\\), \\(q_1=|1\\rangle\\). After \\(X\\) acts on \\(q_0\\) it becomes \\(q_0\\to X |0\\rangle=|1\\rangle\\). Similarly, \\(q_1\\) after \\(Y\\) becomes \\(q_1\\to Y|1\\rangle=-i |0\\rangle\\). Since now \\(q_0\\) is “on” the CNOT gate switches the state of \\(q_1\\) further to \\(q_0 \\to -i|1\\rangle\\). So the end result is that \\(q_0=|0\\rangle, q_1=|1\\rangle\\) is transformed to \\(q_0=|1\\rangle, q_1=-i|1\\rangle\\). Or perhaps a picture says it better\n\nSimilarly, we can work out what the circuit does for other computational basis states which by linearity fully fixes the action of the circuit. Although quite explicit, this is a clumsy description. This is why the matrix notation is usually used. And indeed, we can obtain the matrix corresponding to our quantum circuit quite easily in qiskit:\n\nU_qs = Operator(qc).data\nU_qs\n\narray([[0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j],\n       [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n       [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n       [0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j]])\n\n\nIt is important to realize that a number of conventions must be chosen before such explicit matrix representation can be written down. In particular, I will emphasize two points I tripped over while studying this: ordering of the qubit states in the tensor product or “vertical ordering” and ordering of operators or “horizontal ordering”.\n\nIn the rest of the post I will clarify what are the conventions used in qiskit and how to reproduce the circuit with the tensornetwork library."
  },
  {
    "objectID": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#single-qubit-states",
    "href": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#single-qubit-states",
    "title": "Matrix representation of quantum circuits - notations and gotchas",
    "section": "Single qubit states",
    "text": "Single qubit states\nFirst we need to give matrix representations to two basis states of a single qubit. Here I think it is quite uncontroversial to choose \\[\\begin{align}\n|0\\rangle = \\begin{pmatrix}1\\\\0\\end{pmatrix},\\qquad |1\\rangle = \\begin{pmatrix}0\\\\1\\end{pmatrix} \\label{kets}\n\\end{align}\\]\nThese are the “ket” vectors. Their “bra” counterparts are \\[\\begin{align}\n\\langle 0| = \\begin{pmatrix}1 & 0\\end{pmatrix}, \\qquad \\langle 1| = \\begin{pmatrix}0 & 1\\end{pmatrix} \\label{bras}\n\\end{align}\\]\nWith these, the following operators can be computed \\[\\begin{align}\n|0\\rangle\\langle 0| = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix},\\qquad |0\\rangle\\langle 1| = \\begin{pmatrix}0 & 1 \\\\ 0 & 0\\end{pmatrix} \\nonumber\\\\ |1\\rangle\\langle 0| = \\begin{pmatrix}0 & 0 \\\\ 1 & 0\\end{pmatrix},\\qquad |1\\rangle\\langle 1| = \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix} \\label{ketbras}\n\\end{align}\\] ## Multiple qubit states When there is more than a single qubit things become a bit more interesting and potentially confusing. For example, the combined Hilbert space of two qubits \\(\\mathcal{H}_2\\) is a tensor product of single-qubit Hilbert spaces \\(\\mathcal{H}_2 = \\mathcal{H}_1 \\otimes \\mathcal{H}_1\\) but we need to decide which qubit goes first and which goes second. In qiskit a convention is adopted that additional qubits join from the left, i.e. when we have two qubits as here\n\n\nCode\nqc01 = QuantumCircuit(2)\nqc01.draw(output='mpl')\n\n\n\n\n\nThe state of the system is \\(|q_1\\rangle\\otimes |q_0\\rangle\\) (this is of course only true literally for non-entangled states but we can define everything only on the computational basis states ). OK, but how do we translate this into the matrix representation? The states in the tensor product of vector spaces can be represented by the Kronecker product which is not symmetric with respect to permutation arguments. Best way to explain how Kronecker product works is, as usual, through examples:\n\\[\\begin{align}\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\otimes \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\ 0 \\\\ 0 \\end{pmatrix},\\qquad \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\otimes \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 0\\\\ a \\\\ b \\end{pmatrix}\n\\end{align}\\] Result for generic left vector can be obtained by linearity \\[\\begin{align}\n\\begin{pmatrix} x \\\\ y \\end{pmatrix} \\otimes \\begin{pmatrix} a \\\\ b \\end{pmatrix} = x \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\otimes \\begin{pmatrix} a \\\\ b\\end{pmatrix} +y\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\otimes \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} x a\\\\ x b\\\\ y a \\\\ y b \\end{pmatrix}  = \\begin{pmatrix} x \\begin{pmatrix} a\\\\ b\\end{pmatrix} \\\\ y  \\begin{pmatrix} a \\\\  b\\end{pmatrix} \\end{pmatrix}\n\\end{align}\\]\nThe last notation here is a bit informal but it shows what happens. One just substitutes the right vector into all elements of the left vector, multiplied by the corresponding components of the left vector. The Kronecker product is defined in the same way for matrices of arbitrary size, not just for two vectors.\nSo, now we can compute matrix representations of states in the computation basis of two-qubit system\n\\[\\begin{align}\n|00\\rangle = \\begin{pmatrix}1\\\\0 \\end{pmatrix} \\otimes \\begin{pmatrix}1\\\\0 \\end{pmatrix} = \\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix},\\quad |01\\rangle = \\begin{pmatrix}1\\\\0 \\end{pmatrix} \\otimes \\begin{pmatrix}0\\\\1 \\end{pmatrix} = \\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix} \\label{01}\\\\\n|10\\rangle = \\begin{pmatrix}0\\\\1\\end{pmatrix} \\otimes \\begin{pmatrix}1\\\\0 \\end{pmatrix} = \\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix},\\quad |11\\rangle = \\begin{pmatrix}0\\\\1\\end{pmatrix} \\otimes \\begin{pmatrix}0\\\\1 \\end{pmatrix} = \\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}\n\\end{align}\\]\nThere is a useful relation between the index of the non-zero element \\(n\\) in the four-dimensional representation and the computational basis bitstring \\(q_1q_0\\), namely \\(n=2q_1+q_0\\). I.e. the bitstring \\(q_1q_0\\) is the binary representation of the index \\(n\\). This extends to arbitrary number of qubits, for example since \\(101\\) is \\(5\\) in binary representation it follows \\[\\begin{align}\n|101\\rangle = \\begin{pmatrix}0\\\\0\\\\0\\\\0\\\\0\\\\1\\\\0\\\\0 \\end{pmatrix} \\label{101}\n\\end{align}\\] (try to obtain this from the two tensor products!)\nDon’t believe me? OK, let’s check! In qiskit there is a convenient function to construct a vector representation from a bit string which we will take advantage of. First start with a two-qubit example:\n\ns01 = Statevector.from_label('01')\ns01.data\n\narray([0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j])\n\n\nComparing to \\(\\eqref{01}\\) we find agreement. Similarly,\n\ns101 = Statevector.from_label('101')\ns101.data\n\narray([0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j])\n\n\nAgain, this is in agreement with \\(\\eqref{101}\\).\nHowever, I am not sure that this relation is sufficient to justify the ordering of the tensor products. To me it is much more natural to read the circuit from top to bottom and construct the Hilbert spaces accordingly, say \\(\\mathcal{H}_0\\otimes \\mathcal{H}_1 \\otimes \\mathcal{H}_2 \\dots\\) instead of \\(\\cdots \\mathcal{H}_2\\otimes \\mathcal{H}_1\\otimes \\mathcal{H}_0\\). Later I will change the ordering of the tensor product to my liking, but for now we stick with the qiskit one. Now, with conventions for states in place we can proceed to operators."
  },
  {
    "objectID": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#an-aside-about-reshaping",
    "href": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#an-aside-about-reshaping",
    "title": "Matrix representation of quantum circuits - notations and gotchas",
    "section": "An aside about reshaping",
    "text": "An aside about reshaping\nNote that as usually written, \\(CNOT\\) is a \\(4\\times4\\) matrix. Since as a quantum gate it acts on two qubits, so it should rather be a four-legged tensor. This is the purpose of the reshaping operation. At first the reshaping might be a bit tricky, so let me illustrate it with an example. Introduce two \\(4\\times4\\) matrices and define their product:\n\nA = np.random.rand(4,4)\nB = np.random.rand(4,4)\n\nAB = A @ B\n\nNow define the corresponding four-legged tensors.\n\nimport tensornetwork as tn\n\na = tn.Node(A.reshape(2,2,2,2))\nb = tn.Node(B.reshape(2,2,2,2))\n\nBy contracting the legs (or “edges” in terminology of tensornetworks) appropriately, we can reproduce the matrix multiplication. First the code:\n\na[2] ^ b[0]\na[3] ^ b[1]\n\nab = tn.contractors.greedy([a, b], output_edge_order=[a[0], a[1], b[2], b[3]]).tensor\n\nWe can check that the contraction performed in this way exactly reproduces the matrix multiplication of original \\(4\\times4\\) matrices:\n\nnp.allclose(AB, ab.reshape(4,4))\n\nTrue\n\n\nThis can be interpreted graphically as follows. First, the reshaping procedure can be thought of as splitting each of two four-dimensional legs of the original matrix into two two-dimensional ones\n\nThe labels on the legs have nothing to do with qubit states, these are just indices of edges as assigned by tn.Node operation on our matrices. The matrix multiplication of the original matrices in terms of four-legged tensors then can be drawn as follows\n\nThe index arrangements in the last part explain why we connected the edges in our code the way we did. This is something to watch out for. For example, connecting edges of two identity tensors in the wrong way may produce a \\(SWAP\\) gate."
  },
  {
    "objectID": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#tensor-product-ordering",
    "href": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#tensor-product-ordering",
    "title": "Matrix representation of quantum circuits - notations and gotchas",
    "section": "Tensor product ordering",
    "text": "Tensor product ordering\nThe matrix representation of a tensor diagram like this\n\nalso comes with a convention for the ordering of tensor products. In tensornetwork as well as in my opinion it is natural to order top-down, i.e. the above diagram is \\(U\\otimes \\mathbb{1}\\) instead of \\(\\mathbb{1}\\otimes U\\) as is adopted in qiskit."
  },
  {
    "objectID": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#circuit-from-tensor-network",
    "href": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#circuit-from-tensor-network",
    "title": "Matrix representation of quantum circuits - notations and gotchas",
    "section": "Circuit from tensor network",
    "text": "Circuit from tensor network\nAlright, not we are in a position to reproduce the circuit unitary from the tensor network with nodes x, y and cnot:\n\n\n# Make tensors from matrices\nx, y, cnot = list(map(tn.Node, [X, Y, CNOT]))\n\n# Connect edges properly\ncnot[2] ^ y[0]\ncnot[3] ^ x[0]\n\n# Perform the contraction ~ matrix multiplication\nU_tn = tn.contractors.greedy([cnot, x, y], output_edge_order=[cnot[0], cnot[1], y[1], x[1]]).tensor\n\nThis way of contracting the edges corresponds to the following diagram:\n\nNote that this is basically the original circuit with both the vertical and the horizontal directions reversed. The horizontal reversal is due to mathematical vs circuit notation (circuit is better!) and the vertical reversal is due to the mismatch between qiskit and tensornetwork ordering of tensor product (tensornetwork’s is better!). We can check that the unitary we obtain from this tensor network agrees with qiskit’s\n\nnp.allclose(U_tn.reshape(4,4), U_qs)\n\nTrue"
  },
  {
    "objectID": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#a-better-way",
    "href": "posts/matrix_representation_of_quantum_circuits/2021-08-18-matrix representation of quantum circuits.html#a-better-way",
    "title": "Matrix representation of quantum circuits - notations and gotchas",
    "section": "A better way",
    "text": "A better way\nI find all this misalignment very inconvenient and hard to debug. Ideally I want to look at the quantum circuit and construct the corresponding tensor network just as I read a text: from left to right and from top to bottom. Here I propose a solution which seems much more satisfactory to me. We will deal with horizontal reversal by first defining edges and then applying gates to them. This way we can read the circuit from left to right and simply add new gates, just as in qiskit. I will not try to revert the vertical direction directly, because I find it hard to think upside down. Instead, for comparison with qiskit I will use a built-in reverse_bits method.\nSo let’s start by defining a function that applies a given gate to the collection of qubits (this is a slight modification of an example from tensornetwork docs) :\n\ndef apply_gate(qubits, gate_tensor, positions):\n    \n    gate = tn.Node(gate_tensor)\n    assert len(gate.edges) == 2*len(positions), 'Gate size does not match positions provided.'\n    \n\n    for i, p in enumerate(positions):\n        # Connect RIGHT legs of the gate to the active qubits\n        gate[i+len(positions)] ^ qubits[p] \n        # Reassing active qubits to the corresponding LEFT legs of the gate \n        qubits[p] = gate[i]\n\nImportantly, here, in contrast to the official docs, we append the gate from the left, so that a sequence of application of some \\(G_1\\) followed by \\(G_2\\) is equivalent to the application of \\(G_2\\cdot G_1\\). Now there is one more subtlety. Previously we used matrix representation of \\(CNOT\\) assuming that the uppermost qubit comes last in the tensor product. Now that we decided to turn this convention upside down our matrix representation of \\(CNOT\\) must be \\(CNOT =|0\\rangle\\langle 0|\\otimes \\mathbb{1}+|1\\rangle\\langle 1|\\otimes X\\) or explicitly\n\nCNOT = np.array([[1, 0, 0, 0],\n                 [0, 1, 0, 0],\n                 [0, 0, 0, 1],\n                 [0, 0, 1, 0]]).reshape(2,2,2,2)\n\nWith that we are ready to reconstruct our original circuit in a convenient way:\n\n\n# The context manager  `NodeCollection` is a bit of a magic trick\n# which keeps track of all tensors in the network automatically.\n\nall_nodes = []\nwith tn.NodeCollection(all_nodes): \n    # I do not know how to create 'abstract' edges in `tensornetworks`.\n    # Instead, I create an identity tensor and use its edges to apply new gates to.\n    id0 = tn.Node(np.identity(4).reshape(2,2,2,2))\n    qubits0 = id0.edges[2:4]\n    qubits = id0.edges[0:2]\n\n    apply_gate(qubits, X, [0])\n    apply_gate(qubits, Y, [1])\n    apply_gate(qubits, CNOT, [0,1])\n\nNow let us check!\n\nU_tn = tn.contractors.greedy(all_nodes, output_edge_order=qubits+qubits0).tensor.reshape(4,4)\nU_reversed_qs = Operator(qc.reverse_bits()).data\n\nnp.allclose(U_tn, U_reversed_qs)\n\nTrue\n\n\nWohoo, it worked! If that looked simple to you I’m happy. It took me several hours of debugging to finally match the two matrices. Just to make sure, let me conclude with a more complicated example.\n\nqc3 = QuantumCircuit(3)\nqc3.x(0)\nqc3.cx(0, 1)\nqc3.y(1)\nqc3.x(2)\nqc3.cx(2, 1)\nqc3.y(2)\n\nqc3.draw(output='mpl')\n\n\n\n\nAs you can see, constructing the tensor network analog now works more or less identically:\n\nall_nodes = []\nwith tn.NodeCollection(all_nodes):\n    id0 = tn.Node(np.identity(8).reshape(2,2,2,2,2,2))\n    qubits0 = id0.edges[3:6]\n    qubits = id0.edges[0:3]\n    \n    # The essential part\n    apply_gate(qubits, X, [0])\n    apply_gate(qubits, CNOT, [0, 1])\n    apply_gate(qubits, Y, [1])\n    apply_gate(qubits, X, [2])\n    apply_gate(qubits, CNOT, [2, 1])\n    apply_gate(qubits, Y, [2])\n\nAnd now we compare:\n\nU3_tn = tn.contractors.greedy(all_nodes, output_edge_order=qubits+qubits0).tensor.reshape(8,8)\nU3_qs_reversed = Operator(qc3.reverse_bits()).data\n\nnp.allclose(U3_tn, U3_qs_reversed)\n\nTrue\n\n\nAlright, this resounding True is the best way to conclude that comes to mind. I own many thanks to Ilia Luchnikov for the help with tensornetwork library. Any questions are welcome in the comments!"
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "",
    "text": "Code\nimport numpy as np\nfrom scipy.stats import unitary_group"
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#elementary-single-qubit-rotations",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#elementary-single-qubit-rotations",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "Elementary single-qubit rotations",
    "text": "Elementary single-qubit rotations\nMe (and probably you too) usually take for granted that an arbitrary single-qubit gate \\(U\\) can be decomposed into a product of three elementary rotations. The rotations are usually chosen as\n\\[\\begin{align}\n&R_X(\\theta)=e^{-i\\theta X/2}=\\begin{pmatrix} \\cos\\frac{\\theta}2 & -i\\sin\\frac{\\theta}2 \\\\  -i\\sin\\frac{\\theta}2 & \\cos\\frac{\\theta}2\\end{pmatrix}\\\\\n&R_Y(\\theta)=e^{-i\\theta Y/2}=\\begin{pmatrix} \\cos\\frac{\\theta}2 & -\\sin\\frac{\\theta}2 \\\\  \\sin\\frac{\\theta}2 & \\cos\\frac{\\theta}2\\end{pmatrix}\\\\\n&R_Z(\\theta)=e^{-i\\theta Z/2}=\\begin{pmatrix} e^{-i \\theta/2} & 0 \\\\ 0 & e^{i\\theta/2}\\end{pmatrix}\n\\end{align}\\]\nand a sample decomposition reads\n\\[\\begin{align}\nU = R_X(\\theta_1)R_Z(\\theta_2)R_X(\\theta_3) \\label{XZX} \\ .\n\\end{align}\\]\nThis one is known as the Euler decomposition. Choosing angles \\(\\theta_i\\) appropriately one can obtain any \\(2\\times 2\\) special (with unit determinant) unitary matrix \\(U\\in SU(2)\\)."
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#making-sense-of-eulers-decomposition",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#making-sense-of-eulers-decomposition",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "Making sense of Euler’s decomposition",
    "text": "Making sense of Euler’s decomposition\n\nLet me make a couple of remarks in an attempt to make relation \\(\\eqref{XZX}\\) a bit more intuitive. First of all, the dimension of the group \\(U(n)\\), i.e. a group of \\(n\\times n\\) unitary matrices is equal to \\[\\operatorname{dim} U(n)=n^2 \\ .\\] If you also factor out the global phase, i.e. impose \\(\\operatorname{det}U=1\\), you get the special unitary group \\(SU(n)\\) which has one dimension less \\[\\operatorname{dim} SU(n)=n^2-1\\ .\\] For a single-qubit gate \\(n=2\\) and \\(\\operatorname{dim}=3\\). This implies that the right-hand side in eq.\\(\\eqref{XZX}\\) needs to have at least three parameters and hence three elementary rotations. Good, this explains why are there three terms in Euler’s decomposition. (This technique of unsubtle parameter counting in fact extends far more generally. For example, it can be used to deduce a minimum amount of \\(CNOT\\) gates necessary to compile an arbitrary \\(n\\)-qubit unitary, as I discussed here)."
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#a-word-about-cartan-decomposition-kak",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#a-word-about-cartan-decomposition-kak",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "A word about Cartan decomposition (KAK)",
    "text": "A word about Cartan decomposition (KAK)\nNow, is there anything special about this particular combination of rotations \\(XZX\\)? Well, yes and no. In fact, almost any three single-parameter unitaries would work. Other possible decompositions are \\(XYX\\), \\(YZY\\) etc, but also less common options exist such as \\(XYZ\\).\nHowever, decompositions like \\(XZX\\) are special in a sense that they are examples of Cartan (aka KAK) decompositions, hinging on the structure of the underlying \\(su(2)\\) algebra. Decompositions of two-qubit gates into \\(CNOT\\) gates and single-qubit rotations is another important example where KAK arise. I will not cover the subject here, but can recommend a quite readable introduction with applications to quantum gates https://arxiv.org/abs/quant-ph/0010100."
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#finding-angles-in-eulers-decomposition",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#finding-angles-in-eulers-decomposition",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "Finding angles in Euler’s decomposition",
    "text": "Finding angles in Euler’s decomposition\nWriting decomposition with generic parameters is one thing, fixing them explicitly for a given matrix \\(U\\) is another. Here how you can do it for Euler’s decomposition. The key observation is that because \\(XZ=-XZ\\) we have \\[X R_Z(\\theta)X=R_Z(-\\theta),\\quad X R_X(\\theta)X=R_X(\\theta) \\ .\\] Multiplying \\(\\eqref{XZX}\\) by \\(X\\) from left and right we get \\[XUX=R_X(\\theta_1)R_Z(-\\theta_2)R_X(\\theta_3) \\ .\\] Therefore \\[M:= UXU^\\dagger X=R_x(\\theta_1)R_Z(2\\theta_2)R_x^\\dagger(\\theta_1) \\ .\\]\nSince \\(R_Z\\) is a diagonal gate, the right hand side is nothing else but a unitary diagonalization of matrix \\(M\\). Therefore, finding \\(\\theta_1\\) and \\(\\theta_2\\) reduces to the standard diagonalization procedure. The last parameter \\(\\theta_3\\) is then read off from \\(R_X(\\theta_3)=R_Z^\\dagger(\\theta_2) R_X^\\dagger(\\theta_1)U\\).\nHere is a quick and dirty code that implements the procedure. It won’t handle some edge cases and may not be the most efficient, but it will work with random unitaries.\n\n# Define X, RX, and RZ.\n\nX = np.array([[0, 1], [1, 0]])\n\ndef RX(theta):\n    return np.array(\n        [[np.cos(theta/2), -1j*np.sin(theta/2)],\n         [-1j*np.sin(theta/2), np.cos(theta/2)]])\n\ndef RZ(theta):\n    return np.array(\n        [[np.exp(-1j*theta/2), 0],\n         [0, np.exp(1j*theta/2)]])\n\n# Recovering angle from cos and sin.\ndef angle_from_cos_and_sin(c, s):\n    assert np.allclose(c**2+s**2, 1), 'Input does not satisfy c**2+s**2=1'\n    phi = np.arccos(c)\n    if np.allclose(np.sin(phi), s):\n        return phi\n    else:\n        return -phi\n        \n\n# Decomposition routine.\n\ndef XZX_decomposition(U):\n    \n    assert np.allclose(U.conj().T @ U, np.eye(2)), 'Input matrix is not unitary.'\n    assert np.allclose(np.linalg.det(U), 1), 'Input matrix has non-unit determinant.'\n    \n    \n    # Construct the matrix to diagonalize.\n    M = U @ X @ U.conj().T @ X\n    \n    # Diagonalization.\n    RZ_squared, RX1 = np.linalg.eig(M)\n    assert np.allclose(RX1 @ np.diag(RZ_squared) @ RX1.conj().T, M)\n    \n    # Matrix RX1 is not necessarily of RX(theta) form yet, but it can be made so by multiplying with a diagonal unitary.\n    phase_00 = RX1[0, 0]/np.abs(RX1[0, 0])\n    RX1 /= phase_00 # Now RX1[0,0] is real and RX[1,0] has the correct phase.\n    phase_10 = RX1[1, 0]/np.abs(RX1[1, 0])\n    phase_01 = RX1[0, 1]/np.abs(RX1[0, 1])\n    \n    RX1 = RX1 @ np.diag([1, phase_10/phase_01]) # Now RX1[0,1] and RX1[1,1] have the correct phase as well.\n    \n    # Determine theta_1.\n    cos = RX1[0, 0]\n    sin = 1j*RX1[0, 1]\n    theta_1 = 2*angle_from_cos_and_sin(cos, sin)\n    assert np.allclose(RX(theta_1), RX1)\n\n    # Theta_2 from RZ squared.\n    theta_2 = 1j*np.log(RZ_squared[0]) \n    assert np.allclose(RZ(theta_2), RZ(theta_2))\n        \n    # Determine RX(theta_3) and theta_3. \n    RX2 = RZ(-theta_2) @ RX(-theta_1) @ U\n    cos = RX2[0, 0]\n    sin = 1j*RX2[0, 1]\n    theta_3 = 2*angle_from_cos_and_sin(cos, sin)\n    \n    assert np.allclose(RX(theta_3), RX2)\n\n    if np.allclose(RX(theta_1) @ RZ(theta_2) @ RX(theta_3), U):\n        return theta_1, theta_2, theta_3\n\n    raise TypeError('Something went wrong during decomposition.')\n\nAnd now let’s test.\n\n# Draw a random unitary and normalize its det to 1.\nU = unitary_group.rvs(2, random_state=0) \nU = U / np.linalg.det(U)**(1/2) \n\n# Decompose and check\ntheta_1, theta_2, theta_3 = XZX_decomposition(U)\ncheck = np.allclose(RX(theta_1) @ RZ(theta_2) @ RX(theta_3), U)\n\nprint(f'Does it work? {check}')\n\nDoes it work? True\n\n\nFrom \\(XZX\\) decompositions we can get related ones. For example, since conjugating by the Hadamard gate \\(H\\) swaps \\(X\\) and \\(Z\\) we can reduce \\(ZXZ\\) decomposition of \\(U\\) to \\(XZX\\) decomposition of \\(HUH\\).We’ll need it a bit later so let’s implement this one also.\n\ndef ZXZ_decomposition(U):\n    H = np.array([[1, 1], [1, -1]])/np.sqrt(2)\n    return XZX_decomposition(H@U@H)\n\nA simple check.\n\nU = unitary_group.rvs(2, random_state=2)\nU /= np.sqrt(np.linalg.det(U))\n\ntheta_1, theta_2, theta_3 = ZXZ_decomposition(U)\nprint('Check:', np.allclose(RZ(theta_1) @ RX(theta_2) @ RZ(theta_3), U))\n\nCheck: True\n\n\n # Decomposing arbitrary single-qutrit gate ## Elementary qutrit gates Qutrit is an abstraction for any three-level quantum system and it’s is probably less familiar than a qubit. General qutrit gate belongs to \\(SU(3)\\) and elementary rotation gates for a qutrit are commonly chosen as\n\\[\\begin{align}\nR^{01}_X(\\theta)=\\begin{pmatrix}\\cos\\frac{\\theta}2&-i\\sin\\frac{\\theta}2&0\\\\-i\\sin\\frac{\\theta}2&\\cos\\frac{\\theta}2&0\\\\0&0&1\\end{pmatrix},\\quad\nR^{01}_Z(\\theta)=\\begin{pmatrix}e^{-i\\theta/2}&0&0\\\\0&e^{i\\theta/2}&0\\\\0&0&1\\end{pmatrix} \\ ,\n\\end{align}\\]\nand\n\\[\\begin{align}\nR^{12}_X(\\theta)=\\begin{pmatrix}1&0&0\\\\0&cos\\frac{\\theta}2&-i\\sin\\frac{\\theta}2\\\\0&-i\\sin\\frac{\\theta}2&\\cos\\frac{\\theta}2\\end{pmatrix},\\quad\nR^{12}_Z(\\theta)=\\begin{pmatrix}1&0&0\\\\0&e^{-i\\theta/2}&0\\\\0&0&e^{i\\theta/2}\\end{pmatrix} \\ .\n\\end{align}\\]\nSimilar expression hold to \\(R_Y^{01}\\) and \\(R_Y^{12}\\) but we won’t need them. So these are basically single-qubit rotation gates which touch only two out of the three levels of a qutrit. By the way, gates that act only on levels 1 and 3 look bit awkward, and here we will do without them. ## Is there an off-the-shelf result? (I’m bad at googling) A general single-qutrit transformation lives in \\(SU(3)\\) and hence has \\(3^2-1=8\\) real parameters. Thus, we expect to have a decomposition which is a product of 8 elementary rotations. But how exactly will it look, and how to determine the parameters? To my surprise, the only explicit result I found is equation (4) in this paper https://arxiv.org/abs/1105.5485 which goes something like \\[U = R_Y^{01}R_Y^{02}R_Y^{01}R_Z^{01}R_Z^{02}R_Y^{01}R_Y^{02}R_Y^{01} \\ .\\]\nThe result is based on a particular Cartan decomposition of \\(SU(3)\\) (which is non-unique) and does not look very intuitive. Moreover, apparently no explicit algorithm to find the parameters of these rotations is given."
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#decomposition-sketch",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#decomposition-sketch",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "Decomposition sketch",
    "text": "Decomposition sketch\nI will now construct the explicit decomposition for a general single-qutrit gate advertised in the beginning. First, let us separate a general \\(U\\in SU(3)\\) into four blocks \\[U=\\begin{pmatrix}A&B\\\\C&D\\end{pmatrix}=\\begin{pmatrix} a_{00}& a_{01} & b_0\\\\ a_{10} & a_{11} & b_1\\\\ c_0&c_1&d\\end{pmatrix} \\ .\\]\nHere \\(A\\) is a \\(2\\times2\\) matrix, \\(B\\) and \\(C^\\dagger\\) are \\(2\\times1\\) column vectors and \\(D\\) is a scalar. Now comes the key point. We factorize matrix \\(A\\) using the singular value decomposition (SVD) \\[A = V \\Sigma W \\ . \\label{A SVD}\\] Here \\(V\\) and \\(W\\) are unitary, while \\(\\Sigma=\\operatorname{diag}(\\sigma_0,\\sigma_1)\\) is diagonal with \\(\\sigma_i\\ge0\\). SVD is a very useful factorization that works for arbitrary matrices, not necessarily hermitian or unitary (or even square). Using the SVD of \\(A\\) we can factorize \\(U\\) as follows \\[\\begin{align}\nU=\\begin{pmatrix}V&0\\\\0&1\\end{pmatrix}\\begin{pmatrix}\\Sigma&V^\\dagger B\\\\ CW^\\dagger&D\\end{pmatrix}\\begin{pmatrix}W&0\\\\0&1\\end{pmatrix}=V^{01}\\begin{pmatrix}\\Sigma&V^\\dagger B\\\\ CW^\\dagger&D\\end{pmatrix}W^{01} \\label{U part} \\ .\n\\end{align}\\] The left and right matrices act only within the first two levels. The middle matrix looks like it acts on all levels of the qutrit, but in fact it doesn’t. I will leave at as an exercise to show that matrix \\(A\\) necessarily has a unit singular value (Hint: unitarity condition for \\(U\\) implies \\(A^\\dagger A+C^\\dagger C=\\mathbb{1}\\). Because \\(C\\) is a vector \\(C^\\dagger C\\) is not full rank and has a zero eigenvalue). I will assume that \\(\\sigma_{0}=1\\). This has consequences. In a unitary matrix each row and column has a unit norm. Therefore, if some entry in a unitary matrix is equal to one, all other elements in the corresponding row and column must be zero. Thus, the middle matrix in \\(\\eqref{U part}\\) in fact looks like \\[H^{12}:=\\begin{pmatrix}1&0&0\\\\ 0&\\sigma_{1}& b'\\\\ 0&c'&d\\end{pmatrix}\\] and hence only acts on levels 1 and 2. The overall decomposition can be sketched as follows.  (Here is a catch question. Are the lines on this diagram qubits or qutrit levels? Right, they aren’t qubits.)\nTherefore, we have split a general qutrit gate into three gates touching two levels only. Each of these can be decomposed into the elementary rotations using Euler’s method for single-qubit gates. That’s good. However, as it stands the decomposition is not optimal. So far all matrices \\(V, H, W\\) are generic \\(U(2)\\) matrices. Each comes with 3 parameters and also a global phase. We should be able to do better. Wait, but why is global phase a problem?"
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#a-word-about-global-phases",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#a-word-about-global-phases",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "A word about global phases",
    "text": "A word about global phases\nSure, global phases are normally ignored, and rightfully so, but there are situations where they are important. In our case, a global phase for a two-qubit gate leads to a non-trivial transformation when embedded in a qutrit. For example, \\[(e^{i\\phi})^{01}=\\begin{pmatrix}e^{i\\phi}&0&0\\\\0&e^{i\\phi}&0\\\\0&0&1\\end{pmatrix}\\] is surely not a global phase for a qutrit. You may try to decompose this gate into the elementary qutrit rotations introduced above+a true global phase gate for a qutrit. The result won’t be trivial and actually involve more than one qutrit rotation. Therefore, we will take extra care to avoid global phases in the two-level gates."
  },
  {
    "objectID": "posts/qutrit_gate/2022-11-15-Qutrit.html#decomposition-refinement",
    "href": "posts/qutrit_gate/2022-11-15-Qutrit.html#decomposition-refinement",
    "title": "Decomposition of a general single–qutrit gate",
    "section": "Decomposition refinement",
    "text": "Decomposition refinement\nWe would like to have a decomposition where all two-level matrices lie in \\(SU(2)\\), i.e. have a unit determinant. This can be achieved as follows. First we factor out global phases from matrices \\(V\\) and \\(W\\), denote \\(V=e^{i\\phi_V}V_0, W=e^{i\\phi_W}W_0\\). Then we write \\[A=V'\\Sigma' W' \\ ,\\] where \\[\\begin{align}\n&V'=V_0\\\\\n&\\Sigma'=e^{i\\phi_V}e^{i\\phi_W}\\Sigma R_Z(2\\phi_V+2\\phi_W)\\\\\n&W'=R_Z(-2\\phi_V-2\\phi_W)W_0 \\ .\n\\end{align}\\] Unitary matrices \\(V',W'\\) now have unit determinant. Matrix \\(\\Sigma'\\) is diagonal and importantly \\(\\Sigma'_{00}=1\\). In order to enforce this constraint we included extra \\(R_Z\\) factors in \\(\\Sigma'\\) and \\(W'\\). Note that \\(\\Sigma'_{11}\\) is in general no longer real, but this will not pose a problem.\nWe will introduce another factorization of matrix \\(A\\) \\[A=V'\\Sigma'W'\\] and require \\(\\operatorname{det}V'=\\operatorname{det}W'=1\\) and also \\(\\Sigma'_{00}=1\\). Note that \\(\\Sigma'_{11}\\) can not in general be real in such decomposition, because \\(\\operatorname{det}A\\) is not real in general, but this won’t pose a problem. This new factorization of \\(A\\) leads to the following decomposition of \\(U\\) \\[\\begin{align} U=(V')^{01} (H')^{12}(W')^{12} \\label{U prime} \\ . \\end{align}\\] Because \\(\\operatorname{det}U=1\\) it follows \\(\\operatorname{det}H'=1\\). Therefore, the all two-level matrices now lie in \\(SU(2)\\) and can be decomposed into three elementary rotations using e.g. Euler’s method.\nNote that three matrices from the \\(SU(2)\\) have 9 parameters in total, one more than dimension of \\(SU(3)\\). En extra redundancy can be described as follows\nYou can check directly that \\(\\eqref{U prime}\\) remains invariant. Using this freedom, we can fix one parameter in any of the three matrices.\n ## Summary and implementation In the end, we get the following decomposition for an arbitrary single-qutrit gate \\[U=(V')^{01} (H')^{12}(W')^{12}\\] where \\(U\\in SU(3)\\) and \\(V', H', W'\\in SU(2)\\). It can be computed as follows\nThe algorithm can be summarized as follows. 1. Cut out of matrix \\(U\\) a \\(2\\times2\\) submatrix \\(A\\) and find its singular value decomposition \\(A=V \\Sigma W\\). 1. Factor out global phases from matrices \\(V\\) and \\(W\\) and denote \\(V=e^{i\\phi_V}V_0, W=e^{i\\phi_W}W_0\\). 1. Introduce \\[\\begin{align}\n&V'=V_0\\\\\n&\\Sigma'=e^{i\\phi_V}e^{i\\phi_W}\\Sigma R_Z(2\\phi_V+2\\phi_W)\\\\\n&W'=R_Z(-2\\phi_V-2\\phi_W)W_0 \\ .\n\\end{align}\\] Now \\(A=V'\\Sigma'W'\\) is another factorization of \\(A\\) where \\(V',W'\\in SU(2)\\) and \\(\\Sigma'_{00}=1\\). 1. Compute \\(V'^\\dagger U W'^\\dagger\\). It will be of the form \\(\\begin{pmatrix}1&0\\\\0&H'\\end{pmatrix}\\) for some matrix \\(H'\\in SU(2)\\). 1. Voilà 1. Note also that \\(U\\) does not change under transformation \\(V'\\to V' R_Z(2\\phi), W'\\to R_Z(-2\\phi)W', H'\\to R_Z(-\\phi) H' R_Z(\\phi)\\). This can be used to fix one degree of freedom in either \\(V', H'\\) or \\(W'\\).\nLet’s go ahead and implement this decomposition. I will use the following decomposition consisting of elementary rotations\n\nI.e. I will decompose all \\(SU(2)\\) matrices using \\(ZXZ\\) Euler decomposition and also choose \\(W'\\) so that \\(W'=XZ\\). Again, efficiency or potential edge cases will not be a concern.\n\n# Embedding qubit gates into qutrit.\n\ndef embed(U_qubit, levels):\n    U = np.eye(3, dtype=np.complex64)\n    if levels == '01':\n        U[:2, :2]=U_qubit\n    elif levels == '12':\n        U[1:, 1:]=U_qubit\n    else:\n        raise TypeError(f'Levels {levels} not supported.')\n        \n    return U\n\n# Decomposition routine\n\ndef qutrit_decomposition(U):\n    assert np.allclose(U.conj().T @ U, np.eye(3)), 'Input in not unitary.' \n    assert np.allclose(np.linalg.det(U), 1), 'Input is not special.'\n    \n    \n    A = U[:2, :2] # 2x2 submatrix\n    V, Sigma, W = np.linalg.svd(A) # Basic SVD\n    \n    # Setting det V = det W = 1\n    phase_V = np.sqrt(np.linalg.det(V))\n    phase_W = np.sqrt(np.linalg.det(W))\n    \n    V /= phase_V\n    W /= phase_W\n    \n    # RZ factor to remove from W\n    theta_W1, theta_W2, theta_W3 = ZXZ_decomposition(W)\n    ZW1 = RZ(theta_W1)\n    \n    # Computing the middle diagonal matrix\n    M = phase_V*phase_W*np.diag(Sigma) @ ZW1\n    M00_phase = -1j*np.log(M[0, 0])\n    \n    # Gauge factor to set M[0,0]=1\n    Z_gauge = RZ(2*M00_phase)\n    \n    # Final matrices\n    V_prime = V @ Z_gauge.conj().T\n    W_prime = RX(theta_W2) @ RZ(theta_W3)\n    H_prime_full = embed(V_prime.conj().T, '01') @ U @ embed(W_prime.conj().T, '01')\n    H_prime = H_prime_full[1:, 1:]\n    \n    theta_V1, theta_V2, theta_V3 = ZXZ_decomposition(V_prime)\n    theta_H1, theta_H2, theta_H3 = ZXZ_decomposition(H_prime)\n    \n    return theta_V1, theta_V2, theta_V3, theta_H1, theta_H2, theta_H3, theta_W2, theta_W3\n\nAnd, let’s verify.\n\nU = unitary_group.rvs(3, random_state=0)\nU = U/np.linalg.det(U)**(1/3)\n\ntheta = qutrit_decomposition(U)\n\nV01 = embed(RZ(theta[0]) @ RX(theta[1]) @ RZ(theta[2]), '01')\nH12 = embed(RZ(theta[3]) @ RX(theta[4]) @ RZ(theta[5]), '12')\nW01 = embed(RX(theta[6]) @ RZ(theta[7]), '01')\n\nprint('Does it work?', np.allclose(V01 @ H12 @ W01, U))\n\nDoes it work? True"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "",
    "text": "Code\n# If you are running this notebook in Colab, you might need to restart\n# the environment after the installations.\n\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport jax.numpy as jnp\nfrom jax import random, value_and_grad, jit, vmap, grad, lax\n\nfrom scipy.stats import unitary_group\n\ntry:\n  import optax\nexcept ImportError:\n  !pip install optax\n  import optax\n\ntry:\n  import qiskit\nexcept ImportError:\n  !pip install qiskit\n  !pip install pylatexenc # required for circuit drawing.\n  import qiskit\n    \n\nfrom qiskit import QuantumCircuit, transpile\nfrom qiskit.quantum_info import Operator, Statevector\nfrom qiskit.circuit import Parameter\nfrom qiskit.transpiler.passes.synthesis import UnitarySynthesis\nfrom qiskit.transpiler import PassManager\nfrom qiskit.converters import circuit_to_gate"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#motivation",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#motivation",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Motivation",
    "text": "Motivation\nEver since I read the paper by L.Madden and A.Simonetto (original preprint, my review) I knew I want to do this kind of experiments myself. At first I hoped that there is a well-developed software framework where I can easily build quantum circuits and then optimize them efficiently. However, I was not able to find a good fit for my problem. For example, to the best of my knowledge qiskit currently only provides acess to zero-order optimization routines. I later found quimb which might do what I want, but in the end I’m glad I worked things out from scratch. Eventually I went for numpy+JAX combination which while being quite low-level was not a big problem to get working and shows a decent speed. I owe a ton to Ilia Luchnikov for introducing me to the framework and helping throught.\nIn this post I will give a walk thorough this implementation and show experiments with compilation of random unitaries. However, in my opinion truly interesting stuff is concerned with the compilation of special gates, say multi-controlled Toffolis on restricted connectivity. I intend to look at this kind problems in detail in a future blog post. You may wish to take a look at this preprint for advances in that direction.\n\nNOTE: While I was working on my experiments another preprint appeared, by P.Rakyta and Z.Zimborás, which is very similar to the work of M&S in terms of numerical results. Despite the striking similarities these works are independent. As a bonus R&Z also provide a numerical package SQUANDER that allows to play with their framework for compilation of unitaries. You might want to check that out if you are interested in doing some experiments yourself."
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#the-problem",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#the-problem",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "The problem",
    "text": "The problem\nOK, so first a brief recap of what is the compilation problem. Given a quantum circuit we need to find an equivalent one, which satisfies certain requirements. A typical restrictions are to use only some specific two-qubits gates and to be compatible with limited connectivity. I gave a more detailed intro here. Here is a nearly-trivial example: a simple \\(CNOT\\) gate\n\n\nCode\nqc = QuantumCircuit(2)\nqc.cx(0, 1)\nqc.draw(output='mpl')\n\n\n\n\n\ncan be decomposed in terms of the entangling \\(cz\\) gate and single-qubit gates \\(rx, ry, rz\\) as follows\n\n\nCode\nqc_compiled = transpile(qc, basis_gates=['cz', 'rx', 'ry', 'rz'], optimization_level=3)\nqc_compiled.draw(output='mpl')\n\n\n\n\n\nNow, for generic \\(n\\)-qubit unitaries one needs exponentially many entangling gates for the compilation. More precisely, there is a theoretical lower bound \\(\\#CNOTs\\ge \\frac14 \\left(4^n-3n-1\\right)\\) on the amount of \\(CNOT\\)s required for compilation of any \\(n-\\)qubit unitary outside a measure zero set. Crucially, this measure zero set might in fact be of principal interest to quantum computing as it includes many operators featuring in most algorithms (such as multi-controlled gates). In this post I will only adress compilation of random unitaries and discuss compilation of special cases in a future post. For later reference here is the function computing the theoretical lower bound.\n\ndef TLB(n):\n    return int((4**n-3*n-1)/4 + 1)\n\nfor n in range(1, 7):\n    print('TLB for {}-qubit unitary is {}'.format(n, TLB(n)))\n\nTLB for 1-qubit unitary is 1\nTLB for 2-qubit unitary is 3\nTLB for 3-qubit unitary is 14\nTLB for 4-qubit unitary is 61\nTLB for 5-qubit unitary is 253\nTLB for 6-qubit unitary is 1020\n\n\nNow, there is an algorithm called quantum Shannon decomposition to decompose an arbitary \\(n\\)-qubit unitary into a sequence of \\(CNOT\\)s and single-qubit rotations which requires roughly twice as many \\(CNOT\\)s as the theoretical lower bound implies. In complexity-theoretic terms this is definitely good enoough, the overhead is just a small constant factor. However, for NISQ devices doubling the amount of gates is not a trivial matter. Is it possible to do better?"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#qubit-example",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#qubit-example",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "3-qubit example",
    "text": "3-qubit example\nAs papers M&S and R&Z show, one can do better and eliminate the 2x overhead, at least numerically. Namely, it seems that precisely at the theoretical lower bound the exact or nearly-exact compilation of any unitary is possible. Here is a real-life example. Consider the following 3-qubit circuit with \\(TLB(3)=14\\) \\(CNOT\\) gates\n\nThe claim is that with the appropriate choice of angles in rotation gates it can morhp into any 3-qubit unitary (and in fact at least this many \\(CNOT\\)s are needed for almost all 3-qubit unitaries). To find the corresponding angles it is sufficient to run a numerical optimization minimizing the fidelity between this circuit’s unitary and the target unitary. To me this is rather imressive, but raises several questions. Why choose \\(CNOT\\) gates of all entangling gates? Why place them in that exact order as shown at the figure? It appears to be an empirical fact that precise location of entangling gates as well as their choice (\\(CNOT\\), \\(cz\\), etc) makes little difference. Moreover, even restricted connectivity does not seem to force an overhead for compilation. It is my main goal to back up these claims with numerical experiments in an interactive way. In particular, I will illustrate the following points.\n\nExactly at the theoretical lower bound a nearly-exact compilation seems to always be possible (at least for up to 6 qubits). This is a 2x improvement over the best theoretical decomposition.\nBoth \\(cz\\) and \\(CNOT\\) gates perform equally well. It is tempting to guess that any entangling gate will perform similarly.\nThe maximum fidelity is a monotonic function of the number of entangling gates. This implies that simply counting 2-qubit gates gives a good measure of circuits expressivity.\nThe most remarkable for me is the fact that even a restricted topology seems to cause no overhead on compilation cost. I will show that even on a chain topology the same amount of \\(CNOT\\)s is sufficient to reach good fidelity."
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#what-youll-find-if-you-keep-reading",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#what-youll-find-if-you-keep-reading",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "What you’ll find if you keep reading",
    "text": "What you’ll find if you keep reading\nThe rest of this post is divided into two parts. In the first I write some numpy/JAX/qiskit code that allows to construct and efficiently optimize parametrized circuits. I try to give some explanations of the underlying numerical framework, but please take into account that my own understanding is rather limited. Still, the resulting performance seems to be good enough to reproduce results of the existing preprints. I advise to skip this part if you are only interested in the results.\nIn the second part of the post I will do a number of experiments compiling random unitaries with varying numbers of qubits, different types of entangling gates, restricted connectivity and try to draw some general lessons from them. I tried to make this part independent of the first, although I didn’t stop all the implementation details from sinking trough.\n\nNOTE: This blog post is also a fully functional jupyter notebook. You can open it in Colab or download locally and perform more experiments yourself!"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#entangling-blocks",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#entangling-blocks",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Entangling blocks",
    "text": "Entangling blocks\nFirst let us define the basic 1- and 2-qubit gates in matrix form. For now you can safely ignore the use jnp arrays instead of np arrays.\n\n# Matrix represntations of CNOT, CZ and single-qubit rotations\n\n# Controlled-NOT (or controlled-X gate)\ncx_mat = jnp.array([[1, 0, 0, 0],\n                    [0, 1, 0, 0],\n                    [0, 0, 0, 1],\n                    [0, 0, 1, 0]])\n\n# Controlled-Z gate\ncz_mat = jnp.array([[1, 0, 0, 0],\n                    [0, 1, 0, 0],\n                    [0, 0, 1, 0],\n                    [0, 0, 0, -1]])\n\n# Pauli matrices\nx_mat = jnp.array([[0, 1],\n                   [1, 0]])\n\ny_mat = jnp.array([[0, -1j],\n                   [1j, 0]], dtype=jnp.complex64)\n\nz_mat = jnp.array([[1, 0],\n                   [0, -1]])\n\n# Rotation gates\ndef rx_mat(a):\n    return jnp.cos(a/2)*jnp.identity(2)-1j*x_mat*jnp.sin(a/2)\n\ndef ry_mat(a):\n    return jnp.cos(a/2)*jnp.identity(2)-1j*y_mat*jnp.sin(a/2)\n\ndef rz_mat(a):\n    return jnp.cos(a/2)*jnp.identity(2)-1j*z_mat*jnp.sin(a/2)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nThe circuits that we are going to train will be built out of two types of 2-qubit blocks, the controlled-Z and the controlled-NOT. Here are the definitions:\n\nclass block():\n    \"\"\"Two-qubit entangling block.\n    \n    Methods:\n        circuit: gives equivalent `qiskit` circuit.\n        unitary: gives `jax.numpy` unitary matrix of the circuit.\n    \"\"\"\n    \n    def __init__(self, gate_name, angles):\n        self.gate_name = gate_name\n        self.angles = angles\n    \n    def circuit(self):\n        \"\"\"Quantum circuit in `qiskit` corresponding to our block.\"\"\"\n        \n        qc = QuantumCircuit(2)\n        if self.gate_name == 'cx':\n            qc.cx(0, 1)\n        elif self.gate_name == 'cz':\n            qc.cz(0, 1)\n        else:\n            print(\"Gate '{}' not yet supported'\".format(self.gate_name))\n        \n        angles = np.array(self.angles) # convert from JAX array to numpy array if applicable.\n        \n        qc.ry(angles[0], 0)\n        qc.rx(angles[1], 0)\n        qc.ry(angles[2], 1)\n        qc.rx(angles[3], 1)\n        \n        return qc\n    \n    def unitary(self):\n        \"\"\"JAX-compatible unitary corresponding to our block.\"\"\"\n        \n        if self.gate_name == 'cx':\n            entangling_matrix = cx_mat\n        elif self.gate_name == 'cz':\n            entangling_matrix = cz_mat\n        else:\n            print(\"Gate '{}' not yet supported'\".format(self.gate_name))\n        \n        x_rotations = jnp.kron(rx_mat(self.angles[1]), rx_mat(self.angles[3]))\n        y_rotations = jnp.kron(ry_mat(self.angles[0]), ry_mat(self.angles[2]))\n        \n        return x_rotations @ y_rotations @ entangling_matrix\n\nHere is how they look: cz block\n\na0, a1, a2, a3 = [Parameter(a) for a in ['a0', 'a1', 'a2', 'a3']]\nblock('cz', [a0, a1, a2, a3]).circuit().draw(output='mpl')\n\n\n\n\nand cx block\n\nblock('cx', [a0, a1, a2, a3]).circuit().draw(output='mpl')\n\n\n\n\nOur block class can return a qiskit circuit and the corresponding unitary matrix. Of course we could have extracted the unitary from the circuit itself via qiskit API, but this would make the matrix representation incompatible with JAX which will be our workhorse for optimization. To the best of my knowledge currently it is only possible to use zero-order methods directly from qiskit which is a serious limitation. So at this point we needed a bit of wheel reinvention. Let’s check that our implementation is consistent with qiskit:\n\n# That's how you use random numbers with JAX. Don't worry if this is not familiar, not essential for our purposes.\nangles = random.uniform(random.PRNGKey(0), shape=(4,), minval=0, maxval=2*jnp.pi)\n\nfor gate in ['cx', 'cz']:\n    b = block(gate, angles)\n    qc = b.circuit()\n    qs_unitary = Operator(qc.reverse_bits()).data # Yes, we need to reverse bits in qiskit to match our conventions.\n    our_unitary = b.unitary()\n    print('qiskit unitary is the same as our unitary for block with gate {}: {}'.format(gate, jnp.allclose(qs_unitary, our_unitary)))\n\nqiskit unitary is the same as our unitary for block with gate cx: True\nqiskit unitary is the same as our unitary for block with gate cz: True\n\n\nTo match matrix representations of quantum circuits might be a headache as I discussed in another post, so this was a necessary check to do.\nOur two building blocks (cz and cx) only differ by the type of the two-qubit gate. The circuits that we are going to build seem to do equally well for any choice of two-qubit gate. I will mostly use cz gate because it is symmetric under the swap of qubits, but I will also occasionally bring up the cx gate to illustrate that it has the same performance. Angles \\(a_0\\)-\\(a_3\\) are going to be optimized."
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#optimization-with-jax",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#optimization-with-jax",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Optimization with JAX",
    "text": "Optimization with JAX\n\nA word about JAX\nWhat is JAX? Well, I personally think of it as numpy on steroids. You can check out the official documentation or numerous nice overwievs on the web. For our purposes two key features of JAX are 1. Autograd.\n2. JIT or just-in-time compilation.\nAutograd allows to define functions the same way you do in numpy and have analytic derivatives available with no extra coding on your side. At the moment grad function can only be applied to real scalars. For example, let us define the absolute value of the trace of cx block as function of rotations gate angles\n\ndef block_tr_abs(angles):\n    b = block('cx', angles)\n    tr = jnp.trace(b.unitary())\n    return jnp.abs(tr)\n\nSince everything so far has been defined using jax.numpy we have immediate access to the gradient of this function\n\ngrad(block_tr_abs)([0.,1.,2.,3.])\n\n[DeviceArray(0.03655498, dtype=float32),\n DeviceArray(-0.25903472, dtype=float32),\n DeviceArray(-0.7384602, dtype=float32),\n DeviceArray(-7.450581e-09, dtype=float32)]\n\n\nAutograd feature of JAX allows us to just define the loss function associated with our circuit in plain numpy terms and use advanced first-order optimizers such as Adam out of the box.\nThe next crucial ingredient is jit-compilation. When used with a bit of care, it allows to speed up evaluation of similar expression by orders of magnitude. For example let us compare runtimes of the jitted and unjitted versions of our trace function. Let’s first define a sample of random angles\n\ntest_angles = random.uniform(random.PRNGKey(0), shape=(1000, 4), minval=0, maxval=2*jnp.pi)\n\nand now time evaluation of unjitted trace function\n\n%%time\nfor angles in test_angles:\n    block_tr_abs(angles)\n\nCPU times: user 12.3 s, sys: 1.02 s, total: 13.3 s\nWall time: 11.4 s\n\n\nNow awe to the power of jit!\n\n%%time\njit_block_tr_abs = jit(block_tr_abs)\nfor angles in test_angles:\n    jit_block_tr_abs(angles)\n\nCPU times: user 156 ms, sys: 7.94 ms, total: 164 ms\nWall time: 145 ms\n\n\nWhat happened here is that during the first call to the jitted function it’s efficient XLA version was compiled and then used to evaluate all subsequent calls.\n\n\nGradient descent\nWe will use the following measure of discrepancy between two unitaries \\(disc(U, V) = 1-\\frac1{N}\\operatorname{Tr}\\left( U^\\dagger V\\right)\\) where \\(U,V\\) are \\(N\\times N\\) matrices. It is normalized so that \\(disc(U,U)=0\\) and \\(disc(U,V)=0\\) when \\(U\\) and \\(V\\) are orthogonal. Note that this measure is insensitive to global phases.\n\ndef disc(U, U_target):\n    n = U_target.shape[0]\n    return 1-jnp.abs((U.conj() * U_target).sum())/n\n\nHere is the optimization routine that we are going to use. It is pretty straightforward and I will not give much explanations, but illustrate with an example.\n\n@partial(jit, static_argnums=(0, 1, )) # &lt;--- Here is where the magic happens! \n                                       # Remove this line and everything will run 1000 times slower:)\n    \ndef unitary_update(loss_and_grad, opt, opt_state, angles):\n    \"\"\"Single update step.\"\"\"\n    \n    loss, grads = loss_and_grad(angles)\n    updates, opt_state = opt.update(grads, opt_state)\n    angles = optax.apply_updates(angles, updates)\n    return angles, opt_state, loss\n\ndef unitary_learn(U_func, U_target, n_angles, \n                  init_angles=None, key=random.PRNGKey(0),\n                  learning_rate=0.01, num_iterations=5000, \n                  target_disc=1e-10):\n    \n    \"\"\"Use Adam optimizer to minimize discrepancy between pamaterzied unitary and targe unitary.\n    \n    Args:\n        U_func: function of angles returning univary matrix.\n        U_target: unitary matrix to approximate.\n        n_angles: total number of angles (parameters) in U_func.\n        init_angles: intial angles for gradient descent. If not provided chosen at random.\n        key: random seed to use for inizialization of initial angles.\n        learning_rate: learning rate in Adam optimizer.\n        num_iterations: maximum number of iterations.\n        target_disc: stop optimization if discrepancy drops below target_disc.\n    \n    Returns: tuple (angles_history, loss_history) where\n        angles_history: list of angles (parameters) at each iteration step.\n        loss_history: values of loss_function at each iteration step.\n    \n    \"\"\"\n    \n    # If initial angles are not provided generate them at random.\n    if init_angles is None:\n        key = random.PRNGKey(0)\n        angles = random.uniform(key, shape=(n_angles,), minval=0, maxval=2*jnp.pi)\n    else:\n        angles = init_angles\n    \n    # Loss function to minimize is dicrepancy defined above.\n    loss_func = lambda angles: disc(U_func(angles), U_target)\n    loss_and_grad = value_and_grad(loss_func)\n\n    # Optimizer is taken from the `optax` library and its use is self-explanotory.\n    opt = optax.adam(learning_rate)\n    opt_state = opt.init(angles)\n    \n    # Optimization cycle\n    angles_history=[]\n    loss_history=[]\n    for _ in range(num_iterations):\n        angles, opt_state, loss = unitary_update(loss_and_grad, opt, opt_state, angles)\n        angles_history.append(angles)\n        loss_history.append(loss)\n        if loss &lt; target_disc:\n            break\n    \n    return angles_history, loss_history\n\nOK, now a very simple example. Say we want to find a \\(ZXZ\\) decomposition of \\(Y\\)-gate. Define:\n\ndef zxz_ansatz(angles):\n    return rz_mat(angles[0]) @ rx_mat(angles[1]) @ rz_mat(angles[2])\n\nLearning is now very simple: we give unitary_learn the ansatz unitary as function of angles, the target unitary and also explicitly the number of parameters to be trained:\n\nangles_history, loss_history = unitary_learn(zxz_ansatz, y_mat, 3)\n\nWe can visualize the learning progress as follows:\n\nplt.plot(loss_history)\nplt.yscale('log')\n\n\n\n\nThe learned angles in \\(ZXZ\\) decomposition are\n\nangles_history[-1]\n\nDeviceArray([6.59216  , 3.1411407, 3.4505684], dtype=float32)\n\n\nIt is not difficult to check directly that the result is equal to the \\(Y\\) matrix up to a global phase with reasonable accuracy, indeed\n\njnp.around(1j*zxz_ansatz(angles_history[-1]), 3)\n\nDeviceArray([[0.+0.j, 0.-1.j],\n             [0.+1.j, 0.+0.j]], dtype=complex64)"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#quantum-circuits-with-numpy",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#quantum-circuits-with-numpy",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Quantum circuits with numpy",
    "text": "Quantum circuits with numpy\nNow it’s time to build full quantum circuits. We will think of a quantum circuit on \\(n\\) qubits as a tensor with \\(2*n\\) legs. First \\(n\\) legs correspond to output and last to \\(n\\) input. This is illustrated at the picture.\n\nIt is natural for input legs to be on the left because in matrix notation a unitary \\(U\\) acts on a state \\(\\psi\\) by left multiplication \\(U\\psi\\). On the other hand note that quantum circuits are usually drawn left-to-right and to compare the two descriptions a left-right reflection must be made.\nSuppose now that given an \\(n-\\)qubit circuit \\(U\\) we want to append an additional \\(m-\\)qubit gate \\(V\\) at the end. Here is a concrete example (a picture is worth a thousand words!)\n Several things to keep in mind:\n\nTo append gate \\(V\\) at the end in quantum circuit notation, we need to draw it on the left here.\nTensor legs are joined by numpy’s tensordot operation. Which axes to contract is clear from the picture – we need to join axes 2, 3 of \\(V\\) to 1, 3 of \\(U\\).\nIn the resulting tensor the output legs are not in the correct order. Instead of being numbered from top to bottom after tensordot first several axes are those of \\(V\\) and the remaining are uncontracted output axes of \\(U\\) (take a look at the leftmost column of numbers). This needs to be corrected by explicit transposition of output axes.\nThe final caveat is that if some of the legs connecting gate to the circuit are twisted the output legs needs to be transposed accordingly. Here is an example\n\n\nHere is the code that implements this program.\n\ndef gate_transposition(placement):\n    \"\"\"Determine transposition associated with initial placement of gate.\"\"\"\n    \n    position_index = [(placement[i], i) for i in range(len(placement))]\n    position_index.sort()\n    transposition = [i for _,i in position_index]\n    return transposition\n\ndef transposition(n_qubits, placement):\n    \"\"\"Return a transposition that relabels tensor axes correctly.\n    Example (from the figure above): n=6, placement=[1, 3] gives [2, 0, 3, 1, 4, 5].\n    Twiseted: n=6, placement=[3, 1] gives [2, 1, 3, 0, 4, 5].\"\"\"\n    \n    gate_width = len(placement)\n    \n    t = list(range(gate_width, n_qubits))\n    \n    for position, insertion in zip(sorted(placement), gate_transposition(placement)):\n        t.insert(position, insertion)\n\n    return t\n\ndef apply_gate_to_tensor(gate, tensor, placement):\n    \"\"\"Append `gate` to `tensor` along legs specified by `placement`. Transpose the output axes properly.\"\"\"\n    \n    gate_width = int(len(gate.shape)/2)\n    tensor_width = int(len(tensor.shape)/2)\n    \n    # contraction axes for `tensor` are input axes (=last half of all axes)\n    gate_contraction_axes = list(range(gate_width, 2*gate_width)) \n\n    contraction = jnp.tensordot(gate, tensor, axes=[gate_contraction_axes, placement])\n    \n    # input(=last half) indices are intact\n    t = transposition(tensor_width, placement) + list(range(tensor_width, 2*tensor_width)) \n\n    return jnp.transpose(contraction, axes=t)\n\nNow, using this tensor language we will construct unitary matrices corresponding to our ansatz circuits. To specify the ansatz we must supply the number of qubits in the circuit, type of entangling blocks to use and arrangement of these blocks.\nThe simplest way to specify arrangement would be to just give a list like [[0,1], [1, 3], [2, 1]] etc of pairs of qubits to put entangling blocks on to. However for performance reasons I need to make it more complicated. To construct a matrix for our quantum circuit we basically need to loop over all entangling gates and append them one by one. When using JAX plain python loops are simply unrolled and then compiled. For large loops this leads to very large compilation times. If there is no structure in how we place our gates in the circuit this is probably the best one can do. However, we can be more efficient than that if there is a structure. Take a look at this picture\n\n\nCode\nqc = QuantumCircuit(4)\ni = 0\nfor _ in range(11):\n    qc.cx(i,i+1)\n    i = (i+1) % 3\n    if i % 3 == 0:\n        qc.barrier()\n    \nqc.draw()\n\n\n                     ░                 ░                 ░           \nq_0: ──■─────────────░───■─────────────░───■─────────────░───■───────\n     ┌─┴─┐           ░ ┌─┴─┐           ░ ┌─┴─┐           ░ ┌─┴─┐     \nq_1: ┤ X ├──■────────░─┤ X ├──■────────░─┤ X ├──■────────░─┤ X ├──■──\n     └───┘┌─┴─┐      ░ └───┘┌─┴─┐      ░ └───┘┌─┴─┐      ░ └───┘┌─┴─┐\nq_2: ─────┤ X ├──■───░──────┤ X ├──■───░──────┤ X ├──■───░──────┤ X ├\n          └───┘┌─┴─┐ ░      └───┘┌─┴─┐ ░      └───┘┌─┴─┐ ░      └───┘\nq_3: ──────────┤ X ├─░───────────┤ X ├─░───────────┤ X ├─░───────────\n               └───┘ ░           └───┘ ░           └───┘ ░           \n\n\nHere \\(CNOT\\)s are just placeholders for any entangling block of our interest. There is a regular pattern. Most of the circuit consists of identical layers up to a couple of final gates. Construction and optimization of such circuits with JAX can be made way more efficient by using lax.fori_loop (see here for docs) or a similar construct. This allows to exploit the regularity and reduce the compilation time dramatically.\nThe price to pay is a bit of a hassle in separating all gates into regular ones and the remainder. My core function build_unitary accepts the regular layers as an argument layer_placements=[layer, number_of_repetitions] and the remainder gates are described by free_placements. Also, we need some way to access all parameters (angles) in our circuit. I chose the simplest approach here, to supply angles as a 1d array, but internally they play a bit different roles so there is also a function split_angles to separate a 1d array of all angles into several logical blocks.\nOK, so here is the code. Examples are found in the end of this section.\n\ndef split_angles(angles, num_qubits, layer_len, num_layers, free_placements_len):\n    \"\"\"Splits 1d array of all angles in a circuit into four groups.\n    \n    Args:\n        angles: all angles in a circuit as 1d array.\n        num_qubits: number of qubits in a circuit.\n        layer_len: length (depth) of a single layer in a circuit.\n        num_layers: number of repeated layers.\n        free_placements_len: number of entanglig blocks not in layers.\n        \n    Returns:  a tuple (surface_angles, layers_angles, free_block_angles) where\n        surface_angles: angles in initial single-qubit blocks.\n        block_angles: angles of all entangling blocks.\n        layers_angles: angles for entangling blocks that are parts of complete layers.\n        free_block_angles: angles of remaining entangling blocks.\n    \"\"\" \n    \n    surface_angles = angles[:3*num_qubits].reshape(num_qubits, 3)\n    block_angles = angles[3*num_qubits:].reshape(-1, 4)\n    layers_angles = block_angles[:layer_len*num_layers].reshape(num_layers, layer_len, 4)\n    free_block_angles = block_angles[layer_len*num_layers:]\n    \n    return surface_angles, block_angles, layers_angles, free_block_angles\n\ndef build_unitary(num_qubits, block_type, angles, layer_placements=((), 0), free_placements=()):\n    \"\"\"\n    Builds `JAX`-compatible unitary matrix of a quantum circuit.\n   \n    Arguments specify structure of the circuit and values of parameters.\n    \n    Args:\n        num_qubits: number of qubits.\n        block_type: type of entangling block to use. Currently only 'cx' and 'cz' are supported.\n        angles: 1d array of all angle parameters in the circuit.\n        layer_placements: a tuple (single_layer, n) where `single_layer` specifies \n            positions of several entangling blocks and `n` how many time to repeat each layer.\n        free_placements: Positions of entangling blocks that do no belong to layers.\n\n    Returns:\n        A `jax.numpy` unitary matrix of the quantum circuit.\n    \"\"\"\n    \n    layer, num_layers = layer_placements\n    \n    layer_depth = len(layer)    \n    num_blocks = len(layer)*num_layers+len(free_placements) # Count all entangling blocks.\n    \n    # Divides 1d array of all angles into three logically distinct groups.\n    \n    surface_angles, _, layers_angles, free_block_angles = split_angles(angles, num_qubits, \n                                                                    len(layer), num_layers, len(free_placements))\n    \n    # Initizlizes identity matrix of the proper size.\n    \n    u = jnp.identity(2**num_qubits).reshape([2]*num_qubits*2)\n    \n    # Unitary matrix is built in three steps. \n    # First, 3 single-qubit gates are applied to each qubit.\n    # Second, all entangling blocks that are parts of layers are applied.\n    # Finally, remainder blocks that a not parts any layer are applied.\n    \n    # Initial round of single-qubit gates\n    \n    for i, a in enumerate(surface_angles):\n        gate = rz_mat(a[2]) @ rx_mat(a[1]) @ rz_mat(a[0])\n        u = apply_gate_to_tensor(gate, u, [i])\n    \n    # Sequence of layers wrapped in `fori_loop`.\n    # Using `fori_loop` instead of plain `for` loop reduces the compilation time significantly.\n    # To use `fori_loop` it is convenient to define a separate function that applies a whole layer of gates.\n    \n\n    def apply_layer(i, u, layer, layers_angles):\n        \"\"\"Apply several gates to a given quantum circuit.\n        \n        Supplying the totality of `layers_angles` makes \n        the function compatible with `fori_loop`.\n        \n        Args:\n            i: index of the layer.\n            u: matrix to apply gates to.\n            layer: positions of all gates to be applied.\n            layers_angles: angles of all layers.        \n        \"\"\"\n        \n        layer_angles = layers_angles[i]\n    \n        for block_angles, position in zip(layer_angles, layer):\n            gate = block(block_type, block_angles).unitary().reshape(2,2,2,2) \n            u = apply_gate_to_tensor(gate, u, position)\n            \n        return u\n\n    if num_layers&gt;0:\n        u = lax.fori_loop(0, num_layers, lambda i, u: apply_layer(i, u, layer, layers_angles), u)\n    \n    # Adds the remainding (free) entangling blocks.\n    \n    for angles, position in zip(free_block_angles, free_placements):\n        gate = block(block_type, angles).unitary().reshape(2,2,2,2)\n        u = apply_gate_to_tensor(gate, u, position)\n\n    return u.reshape(2**num_qubits, 2**num_qubits)"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#layers",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#layers",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Layers",
    "text": "Layers\nHere are a couple of simple functions to help define gate arrangements. The basic layer is sequ_layer which consists of entangling gates applied to each possible pair of two qubit gates enumerated by pairs \\((i,j)\\) with \\(i&lt;j\\).\n\ndef sequ_layer(num_qubits):\n    return [[i,j] for i in range(num_qubits) for j in range(i+1, num_qubits)]\n\ndef fill_layers(layer, depth):\n    num_complete_layers = depth // len(layer)\n    complete_layers = [layer, num_complete_layers]\n    incomplete_layer = layer[:depth % len(layer)]\n    \n    return complete_layers, incomplete_layer\n\nFunction fill_layers allows to specify how much entangling gates we want in total and splits them into complete layers (to be used as layer_placements) and possible remainder gates (that become free_placements). For example, a sequ_layer on three qubits consists of three gates at positions\n\nsequ_layer(3)\n\n[[0, 1], [0, 2], [1, 2]]\n\n\nIf we want to have the sequ pattern and 10 entangling gates in total we can put three complete layers and a final single gate. fill_layers does just that\n\nlayer_placements, free_placements = fill_layers(sequ_layer(3), 10)\nprint(layer_placements)\nprint(free_placements)\n\n[[[0, 1], [0, 2], [1, 2]], 3]\n[[0, 1]]"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#packing-everything-together-ansatz-circuits",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#packing-everything-together-ansatz-circuits",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Packing everything together: ansatz circuits",
    "text": "Packing everything together: ansatz circuits\nNow that we have defined our building blocks and convenience functions to assemble them it is time to pack everything together and reap the harvest.\nI will define ansatz class that assembles our building blocks according to a predefined pattern. It’s circuit method gives a qiskit circuit which can be used for visualization and cross-checks. It’s unitary attribute returns fully jax-compatible matrix representation of the same circuit. Finally, its learn method uses our optimization routine to approximate a target unitary. First the code, then an example.\n\nclass Ansatz():\n    \"\"\"Parametric quantum circuit.\n\n    Ansatz/parametric circuit is defined by tupes of entangling blocks and their arrangement.\n    Concrete values of parameters are not considered part of the ansatz. Class provides access\n    to both `qiskit` version of the circuit and `jax.numpy` unitary matrix.\n\n    Attributes:\n        num_qubits: number of qubits\n        block_type: type of entangling blocks\n        num_angles: total number of angles (parameters) in the circuit.\n        unitary: `jax.numpy` unitary matrix of the circuit as function of angles.\n    \n    Methods:\n        circuit: `qiskit` version of the circuit.\n        learn: numerical approximation of the target unitary.\n    \"\"\"\n    \n    def __init__(self, num_qubits, block_type, layer_placements=[[], 0], free_placements=[]):\n        \n        self.num_qubits = num_qubits\n        self.block_type = block_type\n        \n        self.layer, self.num_layers = layer_placements\n        self.free_placements = free_placements\n        self.all_placements = self.layer*self.num_layers+free_placements\n        \n        self.num_angles = 3*num_qubits+4*len(self.all_placements)\n        \n        self.unitary = lambda angles: build_unitary(self.num_qubits, self.block_type, angles, \n                                                     layer_placements=[self.layer, self.num_layers], \n                                                     free_placements=self.free_placements)\n    def circuit(self, angles=None): \n        \"\"\"qiskit version circuit. If angles not specified a parametric circuit is constructed.\"\"\"\n        \n        if angles is None:\n            angles = np.array([Parameter('a{}'.format(i)) for i in range(self.num_angles)])\n            \n        surface_angles, block_angles, _, _ = split_angles(angles, self.num_qubits, \n                                                          len(self.layer), self.num_layers, \n                                                          len(self.free_placements))        \n        \n        qc = QuantumCircuit(self.num_qubits)\n        \n        # Initial round of single-qubit gates.\n        \n        for n, a in enumerate(surface_angles):\n            qc.rz(a[0], n)\n            qc.rx(a[1], n)\n            qc.rz(a[2], n)\n        \n        # Entangling gates accoring to placements\n        \n        for a, p in zip(block_angles, self.all_placements):\n            qc_block = block(self.block_type, a).circuit() \n            qc = qc.compose(qc_block, p)\n            \n        return qc\n        \n    def learn(self, u_target, **kwargs): \n        \"\"\"Use numerical optimization to approximate u_target.\"\"\"\n        \n        u_func = self.unitary\n        return unitary_learn(u_func, u_target, self.num_angles, **kwargs)\n\nHere is an example that should illustrate how all this can be used.\n\nn_qubits = 3\nblock_type = 'cx'\n\n# For technical reasons all entangling gates are divided into 'layers' and 'free' gates. \n\nsingle_layer = [[0, 1], [2, 1]] # We make single layer consisting of 'cx' block on qubits [0,1]\n                                # followed by reversed 'cx' block on qubits [1,2].\n    \nlayers = [single_layer, 3] # The layer is repeated 3 times.\n\nfree_placements = [[1, 0], [0, 1], [1, 2], [2, 1]] # Apeend remaining `free placements` a.\n\nanz = Ansatz(n_qubits, block_type, layer_placements=layers, free_placements=free_placements)\n\nHere is what resulting circuit looks like.\n\nanz.circuit().draw(output='mpl')\n\n\n\n\nJust to make sure let us check that the unitary matrix of this circuit extracted from qiskit agrees with our own implementation for a random set of angles.\n\nangles = random.uniform(random.PRNGKey(0), shape=(anz.num_angles,), minval=0,maxval=2*jnp.pi)\n\nqs_u = Operator(anz.circuit(angles).reverse_bits()).data # qiskit matrix representation\nour_u = anz.unitary(angles) # our matrix representation\nprint(jnp.allclose(qs_u, our_u, rtol=1e-6, atol=1e-7))\n\nTrue"
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#learning-2-qubit-random-unitary",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#learning-2-qubit-random-unitary",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Learning 2-qubit random unitary",
    "text": "Learning 2-qubit random unitary\nLet’s start by learning a random 2-qubits unitary. First, define one.\n\nu_target = unitary_group.rvs(4, random_state=0)\n\nHere is the parametrized circuit we are going to use. cz means that the entangling gate is controlled-Z while free_placements are just positions where to put these entangling gates. There isn’t much choice for 2 qubits as you could guess. I will explain why I call these free_placements a bit later.\n\nanz = Ansatz(2, 'cz', free_placements=[[0,1], [0,1], [0, 1]])\nanz.circuit().draw(output='mpl') # anz.circuit() is a fully-functional `qiskit` version of our ansatz.\n\n\n\n\nThe learning process is easy as pie:\n\n%%time\n\nangles_history, loss_history = anz.learn(u_target)\nplt.plot(loss_history)\nplt.yscale('log')\n\nCPU times: user 2.45 s, sys: 21.4 ms, total: 2.48 s\nWall time: 2.43 s\n\n\n\n\n\nThe graph shows that we achieve great fidelity in under 500 iterations.\nDon’t believe me? Is there a way to tell if this plot indeed reflects a successful compilation without looking under the hood? OK OK, since you’re asking, I will double-check using pure qiskit:\n\nangles = angles_history[-1] # Last(=best) angles in the optimization process.\nqc = anz.circuit(angles) # genuine qiskit circuit. \nu_qs = Operator(qc.reverse_bits()).data # qiskit API to extract the unitary matrix.\ndisc(u_qs, u_target) # OK, I guess here you have believe I've implemented the cost function properly.\n                     # If you want to compare the matrices component-wise, fine with me.\n\nDeviceArray(2.3841858e-07, dtype=float32)\n\n\nSimilar checks can be done in more complicated scenarios below.\nYou can move forward to other examples or try some experiments here. Some ideas: 1. Changing gate type from cz to cx (should not affect the result). 1. Decreasing the number of layers (fidelity won’t be nearly as good). 1. Increasing the number of layers (same fidelity with less iterations)."
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#learning-3-qubit-random-unitary",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#learning-3-qubit-random-unitary",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Learning 3-qubit random unitary",
    "text": "Learning 3-qubit random unitary\nI advertised in the introduction that with just 14 entangling gates any 3-qubit unitary can be nearly perfectly approximated. Let me back up this claim. Here is how we can construct the corresponding ansatz.\n\nnum_qubits = 3\nblock_type = 'cz'\ndepth = 14\nlayer_placemets, free_placements = fill_layers(sequ_layer(num_qubits), depth)\n\nanz = Ansatz(num_qubits, block_type, layer_placements=layer_placements, free_placements=free_placements)\nanz.circuit().draw(output='mpl')\n\n\n\n\nThe way gate placements are passes to Ansatz here require a bit of unpacking. This is an implementation detail I didn’t take enough care to hide. For technical reasons I explained in the numerical section optimization is much faster when gates are arranged in a regular pattern. The pattern we use here is called sequ_layer and for three qubits it is simply\n\nsequ_layer(num_qubits)\n\n[[0, 1], [0, 2], [1, 2]]\n\n\ni.e. it just lists all possible pairs of three qubits. However, since 14 % 3 = 2 the two last gates do not fit into the regular pattern and require a bit of a special treatment. This is what the function fill_layers does for us. Indeed\n\nlayer_placements, free_placements = fill_layers(sequ_layer(num_qubits), depth)\nprint('basic layer is repeated four times:', layer_placements)\nprint('remaining blocks reside at positions:', free_placements)\n\nbasic layer is repeated four times: [[[0, 1], [0, 2], [1, 2]], 4]\nremaining blocks reside at positions: [[0, 1], [0, 2]]\n\n\nI hope that did explain the way that gate positions are passed to the Ansatz. Instead of sequ_layer you can pass any arrangment of gates to be periodically repeated. We will do just that when considering a restricted topology.\nNow let’s run the optimization.\n\n%%time\n\nu_target = unitary_group.rvs(2**num_qubits, random_state=0)\nangles_history, loss_history = anz.learn(u_target)\n\nplt.plot(loss_history)\nplt.yscale('log')\n\nCPU times: user 9.75 s, sys: 177 ms, total: 9.92 s\nWall time: 9.18 s\n\n\n\n\n\nOK, I hope this does convince you that our ansatz was indeed good enough! Another interesting thing to do is to make a sweep to see how the fidelity increases (error drops) with the number of layers.\n\n%%time\n\nbest_loss = [[], []]\nfor depth in range(15): # TLB(3)=14\n    layer_placemets, free_placements = fill_layers(sequ_layer(n_qubits), depth)\n    for i, block_type in enumerate(['cx', 'cz']):\n        anz = Ansatz(num_qubits, block_type, layer_placements=layer_placemets, free_placements=free_placements)\n        angles, loss_history = anz.learn(u_target, target_disc=10e-4)\n        best_loss[i].append(min(loss_history))\n\nplt.plot(best_loss[0], label='cx loss')\nplt.plot(best_loss[1], label='cz loss')\nplt.ylabel('error')\nplt.xlabel('number of entangling gates')\nplt.legend()\n\nCPU times: user 3min 49s, sys: 6.68 s, total: 3min 55s\nWall time: 3min 35s\n\n\n&lt;matplotlib.legend.Legend at 0x7f39d2950a60&gt;\n\n\n\n\n\nOne lesson here is that both types of two-qubits gate perform similarly well at all depths. This is not surprising for because cx and cz gates can be related by single-qubit Hadamard transformations. It would be interesting to see if other two-qubit gates perform differently.\nAnother important observation is that the best fidelity is a monotonic function of the the amount of two-qubit gates. There is some work on variational algorithms testing various metrics that would adequately reflect expressivity of the ansatz. I think that plain number of \\(CNOT\\) gates should in fact be a fantastic and simple metric for this."
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#learning-6-qubit-random-unitary",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#learning-6-qubit-random-unitary",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Learning 6-qubit random unitary",
    "text": "Learning 6-qubit random unitary\nI do know that 3 is followed by 4, but shall we perhaps get more ambitious? Let’s try to compile a 6-qubit random unitary (you can try to go higher if your machine allows):\n\n%%time\n\nnum_qubits = 6\ndepth = TLB(num_qubits) # 1020 for 6 qubits\nlayer_placements, free_placements = fill_layers(sequ_layer(num_qubits), depth)\n\nu_target = unitary_group.rvs(2**num_qubits, random_state=0)\nanz = Ansatz(num_qubits, 'cz', layer_placements=layer_placements, free_placements=free_placements)\n\nangles_history, loss_history = anz.learn(u_target, num_iterations=5000)\n\nplt.title('number of qubits: {}'.format(num_qubits))\nplt.xlabel('number of iterations')\nplt.ylabel('error')\nplt.plot(loss_history)\nplt.yscale('log')\n\nCPU times: user 5min 33s, sys: 1min 1s, total: 6min 34s\nWall time: 6min 28s\n\n\n\n\n\nNote that depth of the theoretical lower bound for 6 qubits is \\(TLB(6)=1020\\) which implies that there are \\(\\approx 4000\\) parameters in our ansatz. On my modest laptop the training completes in about 10 minutes. Of course I would not claim this to be the cutting edge, but our JAX setup seems to be competitive at the scale (3-6 qubits) addressed in the literature so far."
  },
  {
    "objectID": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#restricted-topology",
    "href": "posts/machine_learning_compilation_experiments/2021-12-13-machine learning compilation of quantum circuits -- experiments.html#restricted-topology",
    "title": "Machine learning compilation of quantum circuits – experiments",
    "section": "Restricted topology",
    "text": "Restricted topology\nOne of the most remarkable features of this approach is that topology restrictions do not seem to bring any overhead to compilation of random unitaries. To make the point and illustrate this claim I will consider the least connected topology I can think of, the chain topology. The corresponding layer consists of all pairs of adjacent qubits.\n\ndef chain_layer(num_qubits):\n    return [(i,i+1) for i in range(num_qubits-1)]\n\nHere is a 6-qubit illustration.\n\nAnsatz(6, 'cx', layer_placements=[chain_layer(6), 1]).circuit().draw(output='mpl')\n\n\n\n\nHere I drew a single layer consisting of 5 blocks. To reach the theoretical lower bound requires to stack together 1020/5=204 layers. Let’s do that and see how the learning goes.\n\n%%time\n\nnum_qubits = 6\ndepth = TLB(num_qubits)\nlayer_placements, free_placements = fill_layers(chain_layer(num_qubits), depth)\n\nu_target = unitary_group.rvs(2**num_qubits, random_state=0)\nanz = Ansatz(num_qubits, 'cx', layer_placements=layer_placements, free_placements=free_placements)\n\nangles_history_chain, loss_history_chain = anz.learn(u_target)\n\nCPU times: user 5min 9s, sys: 1min 3s, total: 6min 13s\nWall time: 6min\n\n\nLet’s compare the results with the previously considered fully connected topology.\n\nplt.title('number of qubits: {}'.format(num_qubits))\nplt.xlabel('number of iterations')\nplt.ylabel('error')\nplt.plot(loss_history, label='fully connected')\nplt.plot(loss_history_chain, label='chain')\nplt.legend()\nplt.yscale('log')\n\n\n\n\nAs you can see, the chain topology performs only slightly worse than the fully connected topology which seems truly remarkable."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "",
    "text": "This post will be a bit different from others, in that I’ll try to give a high level review instead of diving in detail into a specific topic. Also, since this is my personal blog, I figured I will give some personal background on this one.\nI got involved in the field of quantum computing relatively recently, in 2021. Before that, while working in theoretical physics, I didn’t even know the field exists, let alone is taking off at the industrial scale (yes, I’m known for my breadth of knowledge). After the transition into quantum, I’ve tried to catch up on many levels – from understanding the fundamentals, to getting up to speed with state of the art in hardware and algorithms, to carrying out some original research. Although I made some progress in all directions, you can imagine how fragmented and incomplete my picture of the field is. While there is still a ton of topics I’d like to understand better, one particular question have been sitting on the back of my mind for quite a while.\nWhat quantum computers will actually be good for?\nChatGPT is probably a good way to summarize a folk knowledge. So, I asked it this exact question and got a pretty long list (click to unfold).\n\n\n\n\n\n\nSure, here’s a numbered list of some potential applications of quantum computers:\n\n\n\n\n\n\nCryptography: Quantum computers have the potential to break many of the currently used encryption methods and could be used to develop new, more secure encryption methods that are resistant to quantum attacks.\nOptimization: Quantum computers could be used to solve optimization problems exponentially faster than classical computers, potentially revolutionizing industries ranging from finance to transportation to healthcare.\nChemistry and materials science: Quantum computers can simulate the behavior of molecules and materials at the quantum level, which could help in the development of new drugs and materials.\nMachine learning and artificial intelligence: Quantum computers could potentially outperform classical computers in tasks such as image and speech recognition, which could have applications in areas such as autonomous vehicles and robotics.\nFinancial modeling: Quantum computers could be used to quickly analyze large amounts of financial data and predict stock prices or other market trends.\nSupply chain optimization: Quantum computers could be used to optimize supply chain operations, such as routing and scheduling, to improve efficiency and reduce costs.\nWeather forecasting: Quantum computers could be used to simulate complex weather patterns and improve the accuracy of weather forecasting.\nEnergy production: Quantum computers could be used to optimize energy production and distribution, potentially reducing energy costs and improving energy efficiency.\nParticle physics: Quantum computers could be used to analyze data from particle accelerators and help physicists understand the behavior of particles at the subatomic level.\nTransportation: Quantum computers could be used to optimize transportation networks, including traffic flow, route planning, and logistics.\n\n\n\n\nAnd indeed, all these topics, and many more, are frequently covered in popular discussions and technical papers on quantum computing. Probably for a non-expert, it would be nearly impossible to critically sort through these and resist the impression that quantum computers will be good for everything (and soon). However, my exposure to the field already made me skeptical about many of the usually proposed applications. And I felt a pressing need to sort this out for myself. I now feel like I’ve mostly done my homework on this one, and here is what I found.\nWe don’t really know yet what quantum computers will be good for.\nAnd among the zoo of proposed applications, most are rather speculative, while a single one seems to stand out as most promising. You can read on to see what I have to back up these claims, or jump directly to the very short summary section (Section 10).\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI will specifically focus on quantum computation, not sensing or communication. Moreover, I will look for practical, useful problems, not just any possible demonstration of quantum advantage.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs mentioned, my expertise in the field is limited. So take my assessment critically. I will cite many sources, but of course they are subject to my selection bias. Also, I’m more than happy to be proven wrong in this case. Feel free to leave the feedback.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI do not attempt to make the references comprehensive, e.g. I won’t cite original work of Grover or Shor. This is a blog post, come on. On the contrary, I will try to limit citations to those directly relevant to my arguments."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#disclaimers",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#disclaimers",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "",
    "text": "Note\n\n\n\nI will specifically focus on quantum computation, not sensing or communication. Moreover, I will look for practical, useful problems, not just any possible demonstration of quantum advantage.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs mentioned, my expertise in the field is limited. So take my assessment critically. I will cite many sources, but of course they are subject to my selection bias. Also, I’m more than happy to be proven wrong in this case. Feel free to leave the feedback.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI do not attempt to make the references comprehensive, e.g. I won’t cite original work of Grover or Shor. This is a blog post, come on. On the contrary, I will try to limit citations to those directly relevant to my arguments."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#variational-quantum-algorithms",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#variational-quantum-algorithms",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Variational quantum algorithms",
    "text": "Variational quantum algorithms\nThe main bundle of NISQ algorithms are variational quantum algorithms (VQA)  [11]. The two most studied examples are Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE). QAOA mostly focuses on ground state preparation for classical Ising Hamiltonians, which in facts covers a huge range of problems related to combinatorial optimization. VQE typically addresses Hamiltonians that arise from physics or chemistry, but largely does the same thing. I think the line between different types of VQA is quite blurry.\n\n\n\n\n\n\nVariational quantum algorithms\n\n\n\n\n\nQuite generally, variational quantum algorithms aim to find a low energy state of some Hamiltonian \\(H\\) encoding the problem of interest. They start with a trivial quantum state \\(|0\\rangle\\) and apply a parameterized quantum circuit to it \\(U(\\theta)|0\\rangle\\). The resulting energy \\[E(\\theta)=\\langle0|U^\\dagger(\\theta)HU(\\theta)|0\\rangle\\] is minimized by adjusting parameters \\(\\theta\\) classically.\nParameterized quantum circuits \\(U(\\theta)\\) can be informed by the problem as in QAOA, which seeks to approximate the adiabatic evolution, or completely problem-agnostic as in Hardware-Efficient ansatze.\n\n\n\nHere is my simple-minded and a bit cynic take on the idea behind variational quantum “algorithms”, which I think would be better called heuristics. Real quantum algorithms (without quotes) typically require circuits that are very deep. The current generation of quantum devices is pretty inaccurate, errors in two-qubit gates are of the order of \\(0.1-1\\%\\). If you apply many gates, there will be nothing but noise at the output, and the computation is not useful. VQA approach the problem as follows. Alright, we do not know algorithms with shallow circuits, but let’s try to build some. We’ll prepare a quantum circuit that is sufficiently shallow to have a non-zero signal-to-noise ratio, and introduce parameters in there. While we do not know if any values of these parameters correspond to a useful computation, let’s try to adjust them (classical optimization loop) so that the circuit does something useful.\nI mean, it is not a bad idea, and in many respects similar to how classical machine learning works. The problem seems to be, the current hardware only allows circuits so shallow, that you may optimize them all you want, no interesting results will follow. Another practical problem is that evaluation of the energy function \\(E(\\theta)\\) requires taking a ton of samples, which is slow and expensive. On the theoretical side, the loss landscape of most VQA appears to be pretty terrible, featuring barren plateaus and many bad local minimums. So even with a perfect hardware, there are no guarantees for good results."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-machine-learning",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-machine-learning",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Quantum machine learning",
    "text": "Quantum machine learning\n\n\n\n\n\n\nSupervised quantum machine learning models\n\n\n\n\n\nA typical quantum model for a supervised learning task looks very similar to VQA instances described above \\[E(x,\\theta)=\\langle0|U^\\dagger(x,\\theta) H U(x,\\theta)|0\\rangle \\ .\\] Only here, part of the parameters \\(x\\) are now not model “weights” to be adjusted, but instead encode the training data. The rest of the parameters \\(\\theta\\) are to be optimized to yield a better loss function \\(E(x,\\theta)\\).\n\n\n\nQuantum machine learning (QML)  [12] sounds quite fancy, but it shares much of the problems with VQA. Additional questions you might ask about QML models is whether they have and edge over classical in data encoding, expressivity, generalize better etc. To the best of my understanding, all claims that some QML models are somehow better than classical counterparts are heuristic, inconclusive, or only work for extremely artificial datasets. Moreover, let me quote a recent perspective  [13] by Schuld and Killoran titled “Is quantum advantage the right goal for quantum machine learning?”\n\nContrary to commercial expectations – machine learning may turn out to be one of the hardest applications to show a practical quantum advantage for.\n\nWhy? By all means take a look at the paper if you are interested, but a short answer is that\n\nQuantum machine learning research is trying to beat large, high-performing algorithms for problems that are conceptually hard to study.\n\nIn other words, classical machine learning is so efficient it sets a very high bar; it’s hard to theoretically analyze how it works, let alone prove quantum advantage; and we can’t collect any meaningful empirical data on QML because we only have toy hardware yet."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#noisy-summary",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#noisy-summary",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Noisy summary",
    "text": "Noisy summary\nThere are many more versions of NISQ algorithms beyond VQA and QML. However, they all come with significant challenges. I quote an extensive recent review  [10]\n\nAt the moment of documenting this review, there is no known demonstration of industrially relevant quantum advantage.\n\nOn top of that, to the best of my knowledge, there are also no theoretical guarantees that NISQ algorithms can lead to quantum advantage at all. So, NISQ algorithms seemed like a low-hanging fruit, but despite all the work of the past years, useful applications have not been demonstrated. Even the industry now seems to become less optimistic about NISQ, and focus more on the fault-tolerant algorithms, and so will we."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-dynamics",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-dynamics",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Quantum dynamics",
    "text": "Quantum dynamics\nQuantum simulation is the application that is often cited as having kick-started the field. At the same time, it is still widely believed to have the best shot at useful quantum advantage.\nIt is a very natural application, since no convoluted procedure to fold and squeeze a classical problem into a quantum domain is required. Instead, it looks at the task that is obviously quantum in origin, and proposes an efficient way to solve it with a quantum computer.\n\n\n\n\n\n\nQuantum Simulation\n\n\n\n\n\nQuantum simulation is designed to take an initial quantum state \\(|\\psi_0\\rangle\\) and carry out its evolution under some Hamiltonian \\(H\\), i.e. to find\n\\[|\\psi(t)\\rangle=e^{-iHt}|\\psi_0\\rangle \\ .\\]\nFor a Hamiltonian which is sparse, e.g. consists of not too many local terms \\(H=\\sum_k{H_k}\\), one can use the Trotter-Suzuki approximation \\(e^{(A+B)\\Delta t}=e^{A\\Delta t}e^{B\\Delta t}+O(\\Delta t^2)\\) to reduce the simulation of the full Hamiltonian evolution over some small time period \\(\\Delta t\\) to a simulation of separate local terms, which is in principle straightforward\n\\[e^{-i H \\Delta t}=\\prod_k e^{-i H_k \\Delta t}+O(\\Delta t^2) \\ .\\]\nEvolution over a finite time period \\(t\\) can then be produced by a sequence of short evolutions \\(e^{-iHt}=\\left(e^{-iH\\Delta t}\\right)^{\\frac{t}{\\Delta t}}\\). The error coming from “Trotterization” of each small time step can be reduced by making \\(\\Delta t\\) smaller, at the cost of increasing the circuit depth polynomially.\n\n\n\nWhile numerous classical methods for approximate simulation of quantum systems have been developed, with great success in many cases, they are not sufficient in general. This is another important point about the simulation problem – the difficulty of the classical approach is well appreciated, so quantum computer is really expected to make the difference here.\nIs quantum simulation useful? I mean, it obviously is, but how exactly? There are definitely implications for fundamental science such as probing complicated quantum dynamics, new phases of matter, quantum chaos and so on. But what about designing a high-temperature superconductor or a new battery? Unfortunately, I am not aware of a rigorous connection between the ability to do quantum simulation and producing practically useful outcomes. So far it seems to be more about exploring the physics/chemistry with the new tools and beyond the regimes the current techniques allow. For this reason, it’s not clear that analog quantum simulators, which will be ultimately limited in their accuracy, will have applications beyond basic science  [23]."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-phase-estimation",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-phase-estimation",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Quantum phase estimation",
    "text": "Quantum phase estimation\nThere is another flavor of quantum simulation that leads to more deterministic results. The archetypical algorithm here is the quantum phase estimation (QPE).\n\n\n\n\n\n\nQuantum phase estimation\n\n\n\n\n\nQuantum phase estimation (QPE) can be thought of as an efficient quantum circuit to perform a projective energy measurement.\nIt allows finding eigenvalues and preparing eigenstates of a Hamiltonian \\(H\\), provided one can efficiently implement controlled evolution operators \\(e^{-iHt}\\). Usually, QPE is formulated as an algorithm for finding eigenvalues of a unitary operator \\(U\\) given its eigenstate \\(|\\lambda\\rangle\\) with an unknown eigenvalue. QPE proceeds by applying powers of \\(U\\) (\\(U, U^2, U^4,\\dots\\)) to state \\(|\\lambda\\rangle\\), each controlled by its own auxiliary qubit. The state of the auxiliary qubits then contains a lot of information about \\(\\lambda\\), roughly one bit of accuracy per qubit, and this information can be efficiently revealed after performing a quantum Fourier transform on the auxiliary qubits.\nIf the original state \\(|\\psi\\rangle\\) is not an eigenstate of \\(U\\), QPE performs a projective energy measurement. It will output some energy \\(E\\) and prepare the corresponding state \\(|\\lambda_E\\rangle\\) with probability proportional to the overlap of \\(|\\langle\\psi|\\lambda_E\\rangle|^2\\).\n\n\n\nQuantum phase estimation can yield high-accuracy information about the eigenstates of a Hamiltonian. In many practical questions of quantum chemistry, this exactly what you want to know. There is a caveat, though. Most interesting in practice are ground and low-energy states, and for QPE to reveal information about them, the initial state for the algorithm needs to have sufficiently high overlap with low-energy states. In general this problem is QMA-hard (not expected to be efficiently solvable even on a quantum computer), and whether it is solvable in practical scenarios is still not settled conclusively  [24]."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#nothing-ive-said-here-is-new",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#nothing-ive-said-here-is-new",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Nothing I’ve said here is new",
    "text": "Nothing I’ve said here is new\nIf you think that the perspective taken in this blog post is in any way novel or radical, it’s not. While the points I made here are not often voiced or put in writing, many experts have been saying similar things for years. Here are some references.\n\nI highly recommend this talk  [18] given by Matthias Troyer way back in 2014. Or a more recent one  [27] from 2021. Interestingly, they are pretty similar in content, and in particular Troyer seems to entirely ignore the variational algorithms, and maybe for a good reason. There is also a recent short write-up by him and collaborators  [16].\nIn a 2021 talk Ryan Babbush  [28] (in conclusions part) says that the community still needs to figure out, with clarity, what will quantum computers be useful for. He says this in the context of early fault-tolerant computation, but I think the point applies more broadly.\nHere is a piece by a renowned condensed matter physicist Sankar Das Sarma  [29], arguing that potential applications of NISQ are highly overstated.\nHere are two presentations by Owen Lockwood  [30],  [31] critically assessing the state of NISQ algorithms and NISQ QML. Owen might not have the weight of other people I reference here, but I found his take on things original and informative.\nHere is a pretty critical LinkedIn post by Victor Galitski  [32]. It is again a mostly a critique of NISQ, with focus mainly on socio-economic rather than algorithmic side of things, but still worth a read.\nFinally, I’ll mention this popular interview with John Preskill  [33], where he mentions (section ‘simulation’) that quantum simulation is still probably our best grounded expectation for practical quantum advantage."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#i-may-have-a-bias-problem",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#i-may-have-a-bias-problem",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "I may have a bias problem",
    "text": "I may have a bias problem\nAlright, you might have noticed that even this list gets increasingly less rigorous. My investigation, which started as a noble search for truth, quickly turned into a confirmation bias exercise. Indeed, I quite quickly started to err on the side that ‘we still don’t really know what quantum computers will be good for’, and enjoyed finding support for this view. While this may not be a great journalistic work, I still think this point of view is seriously underrepresented and worth voicing. At the same time, I’m really open to changing my mind, as I have all the reasons to want the field to succeed, and the sooner, the better."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-computing-is-gonna-be-a-rock-star-one-day",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#quantum-computing-is-gonna-be-a-rock-star-one-day",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "Quantum computing is gonna be a rock star one day",
    "text": "Quantum computing is gonna be a rock star one day\nI must also say that in the long run, a radical impact of quantum computing looks inevitable to me. This is a fundamentally new way of information processing, and this must make a difference. As Scott Aaronson have argued, if for fundamental reasons large scale quantum computers can never be built, it would be a new and revolutionary law of physics. I’d say that similarly, if we could ‘prove’ that quantum computers can not be useful, this would be a new remarkable law of nature worth discovering. From what we know now, it looks extremely unlikely. However, use cases for truly novel technologies are hard to forecast."
  },
  {
    "objectID": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#what-looks-the-most-promising-at-the-moment",
    "href": "posts/what_to_do_with_a_quantum_computer/what_to_do_with_a_quantum_computer.html#what-looks-the-most-promising-at-the-moment",
    "title": "What will a quantum computer be good for, exactly?",
    "section": "What looks the most promising at the moment?",
    "text": "What looks the most promising at the moment?\nIn searching for practical quantum advantage, several requirements need to be met.\n\nThere must be a problem that a quantum computer can solve efficiently.\nEvidence that a classical computer can’t.\nLast but not least, the problem must be useful.\n\nIf you think about it, this list is as much about the problem we want to solve as it is about the power of quantum algorithms. And finding the right problems, although possible in theory, turns out to be very challenging in practice.\nIt is exciting to try applying quantum algorithms to problems that appear to have no direct relation to the quantum world whatsoever. Basically, we start with (1) and then try to comply to (2) and (3). Say, we have an idea about how to solve certain large linear systems of equations and then try to find a subset of those that are useful and intractable classically. And while there may be gems on this path, a lot of evidence now shows that finding the right problems of this kind is really tricky. One early impressive success is Shor’s algorithm, but It may still be the only well-established example.\nOn the other hand, one can tackle obviously quantum-inspired problems. Quite recently, people started to look at cases when the input data is quantum rather than classical, and there the quantum advantage is already established, but no wide uses have been suggested yet. I’d say that the simulation of quantum systems for physics and chemistry appears to be our most grounded proposal for where to look for a practical quantum advantage. It addresses an obviously important problem known to be classically hard by decades of intensive research. So (2) and (3) are covered, and (1) also comes naturally. Indeed, people also often describe quantum simulation as a native task for a quantum computer. I quote an elegant passage from  [23]\n\nThis is the ‘native’ and most natural application of quantum computers, where we aim to use a quantum computer to mimic the rules that describe physical microscopic quantum systems. These problems are computationally challenging for the same underpinning reason that quantum computers can be powerful.\n\nAlright, I’ll leave it at that. As usual, any feedback is welcome."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "",
    "text": "Code\ntry:\n  import qiskit\nexcept ImportError:\n  !pip install qiskit\ntry:\n  import pylatexenc\nexcept ImportError:\n  !pip install pylatexenc\n  \nfrom qiskit import transpile, QuantumCircuit, QuantumRegister\nfrom qiskit.circuit.library import *\nfrom qiskit.quantum_info import OneQubitEulerDecomposer, random_clifford, Operator, Statevector\nfrom qiskit.extensions import UnitaryGate\nfrom qiskit.circuit import Instruction\nfrom typing import Union\nimport numpy as np"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#cnot",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#cnot",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "CNOT",
    "text": "CNOT\nThe simplest “Toffoli” gate is CNOT gate, also known as controlled NOT or controlled X (CX).\n\n\nCode\nqc = QuantumCircuit(2)\nqc.cx(0,1)\n\n# Note, if you draw this interactively and have an error here (happens in Colab), redefine the global variable below.\n# output = 'mpl'\n\nqc.draw(output=output)\n\n\n\n\n\nIt flips the value (applies X gate) of the second qubit if the first qubit is in state \\(|1\\rangle\\) and does nothing if the first qubit is in state \\(|0\\rangle\\). The first qubit is called the control qubit, the last is called the target. If the target qubit starts in state \\(|0\\rangle\\), than CNOT effectively copies that value of the control qubit to the target."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#q-toffoli",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#q-toffoli",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "3q Toffoli",
    "text": "3q Toffoli\nThe regular Toffoli gates is defined on three qubits.\n\n\nCode\nqc = QuantumCircuit(3)\nqc.ccx(0, 1, 2)\nqc.draw(output=output)\n\n\n\n\n\nIt has two controls and one target. This gate flips the value of the target qubit if and only if both of the control qubits are in state \\(|1\\rangle\\), and does nothing otherwise. If the target qubit starts in state \\(|0\\rangle\\) the Toffoli gate effectively stores the logical AND of the control qubits \\(q_0 \\land q_1\\) in the target register."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#n-qubit-toffoli",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#n-qubit-toffoli",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "n-qubit Toffoli",
    "text": "n-qubit Toffoli\nThis construction has a plain generalization to multiple control qubits, e.g. here is the the Toffoli gate on 5 qubits (with 4 control qubits)\n\n\nCode\nqc = QuantumCircuit(5)\nqc.mct([0, 1, 2, 3], 4)\nqc.draw(output=output)\n\n\n\n\n\n\\(n-\\)qubit Toffoli gates are very important primitives in quantum computing. For elaboration on this claim I recommend the introduction section in Maslov’s paper, which starts with\n“Multiple control Toffoli gates are the staple of quantum arithmetic and reversible circuits. They are employed widely within quantum algorithms, including inreversible transformations, such as arithmetic circuits and all sorts of Boolean operations over quantum registers, as well as subroutines within other specialized quantum transforms.”"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#decompositions",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#decompositions",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Decompositions",
    "text": "Decompositions\nAlthough there are intriguing recent suggestions to implement multiple-control Toffoli gates directly (see e.g. here) most hardware platforms currently require them to be decomposed into 1q+2q gates. This is the same what the Classiq problem asked for, to decompose a multiple-controlled Toffoli gate into CNOT gates and arbitrary 1q gates.\nI will denote Toffoli gates on \\(n\\) qubits (with \\(n-1\\) controls) by \\(T^n\\) (another frequent notation is \\(C^{n-1}X\\)). \\(T^2\\) gate, which is just CNOT, need no further decomposition. \\(T^3\\) gate, the regular Toffoli gate, can be decomposed into CNOT+1q gates as follows.\n\n\nCode\nqc = QuantumCircuit(3)\nqc.ccx(0, 1, 2)\nqc.decompose().draw(output=output)\n\n\n\n\n\nThis decomposition has 6 CNOT gates as well as depth 6 with respect to CNOT gates. Total depth is 11. We will be looking for similar decompositions of \\(T^{n}\\) with \\(n\\ge3\\). The goal of the Classiq challenge was to find decompositions with the smallest total depth. Other optimization metrics such as CNOT count, CNOT depth or T count and T depth may be preferred for some applications. Here T without a superscript refers to the magic T gates, which are important for fault-tolerant circuits. Fortunately, these metrics are not that much decoupled, and a circuit efficient in one respect is also often very efficient in others, see again Maslov’s paper for examples."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#ancilla-qubits",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#ancilla-qubits",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Ancilla qubits",
    "text": "Ancilla qubits\nAs CNOT+1q gates set is universal, it can be used to decompose an arbitrary \\(n-\\)qubit gate, including \\(T^n\\). The efficiency of this decomposition can be greatly enhanced if ancilla qubits are provided. Here is an example using a clean ancilla qubit to decompose \\(T^4\\) into three regular Toffoli gates.\n\nThe first \\(T^3\\) stores \\(q_0 \\land q_1\\) in the ancilla qubit. The second \\(T^3\\) combines this with an additional control and applies to the target. The last step is called uncomputation, it is only to restore the state of the ancilla bit to \\(|0\\rangle\\) (\\(T^3\\) applied twice is the identity). For a great introduction covering Toffoli gates, their decompositions, various type of ancilla qubits and more see this blog series by Craig Gidney.\nAs constructed, the circuit for \\(T^4\\) with one clean ancilla uses three \\(T^3\\) and has CNOT count \\(3\\times6=18\\). I will now show how this can be compressed using relative phase Toffoli gates.\n\n(The wiggly lines are to emphasize the wave nature of quantum mechanics. Seriously though, the words ‘relative phase’ remind me of the phase shifts in signal processing, and I had to come up with some pic for this post anyway.)"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#definition",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#definition",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Definition",
    "text": "Definition\nRelative phase Toffoli gates \\(RT^n\\) are defined as standard Toffoli gates \\(T^n\\) followed by a diagonal gate \\(D^n\\). I will draw them as follows.\n\nSo, the boxed Toffoli gate stands for the relative phase Toffoli gate. The boxed controlled Z gate (last gate on the right circuit) will be my notation for the diagonal gate itself. Note that this representation is not standard (which I found difficult to typeset), my apologies."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#diagonal-gates",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#diagonal-gates",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Diagonal gates",
    "text": "Diagonal gates\nDiagonal gate is just what it sounds, a gate with only diagonal entries. For instance, a general 2q diagonal gate gate be written as \\(D=e^{\\phi_0}|00\\rangle\\langle00|+e^{\\phi_1}|01\\rangle\\langle01|+e^{\\phi_2}|10\\rangle\\langle10|+e^{\\phi_3}|11\\rangle\\langle11|\\). The relative phase Toffoli gate is called that way because its elements only differ from the original Toffoli gate by a phase. Crucially, different elements may differ by different phases (so this is not the irrelevant global phase). Diagonal gates themselves are non-trivial objects. For example, up to a conjugation by two Hadamard gates \\(n-\\)qubit Toffoli gate is equivalent to \\(n-\\)qubit controlled Z gate, which is diagonal."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#relative-gates-are-shorter",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#relative-gates-are-shorter",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Relative gates are shorter",
    "text": "Relative gates are shorter\nWhy care about the relative phase gates? First, they can have shorter representations compared to the original gates. This might look unreasonable at first, because we are adding an extra diagonal gate. However, the extra gate can cancel part of the original circuit. So, by appending a suitable diagonal gate one can make the relative phase Toffoli gate shorter than the Toffoli gate itself. Here is an example of \\(RT^3\\) gate.\n\n\nCode\n# Build circuit\nqc = QuantumCircuit(3)\nqc.h(2)\nqc.t(2)\nqc.cx(1, 2)\nqc.tdg(2)\nqc.cx(0, 2)\nqc.t(2)\nqc.cx(1, 2)\nqc.tdg(2)\nqc.h(2)\n\n# Store to gate\nRT3 = qc.to_instruction(label='RT3')\nRT3dg = RT3.inverse()\nRT3dg.label = 'RT3dg'\n\n# Draw\nqc.draw(output=output)\n\n\n\n\n\nNote that it only has 3 CNOT gates, while \\(T^3\\) requires 6 gates. Here is a numeric way to check that this gate is indeed \\(RT^3\\).\n\ndef is_relative(U, V):\n    D = V.conj().T @ U\n    # If and only if D is unitary and diagonal the check is True.\n    check = np.allclose(np.abs(D), np.eye(U.shape[0])) \n    return check\n\n# Check that it is indeed the relative phase Toffoli 3 gate\nu_RT3 = Operator(qc).data\nqc_T3 = QuantumCircuit(3)\nqc_T3.ccx(0, 1, 2)\nu_T3 = Operator(qc_T3).data # Regular Toffoli gate.\nprint('is RT3:', is_relative(u_RT3, u_T3))\n\nis RT3: True"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#diagonal-gates-commute-with-controls",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#diagonal-gates-commute-with-controls",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Diagonal gates commute with controls",
    "text": "Diagonal gates commute with controls\nSecond, the relative phase Toffoli gates can be used instead of the standard Toffoli gates in compute-uncompute pairs. For example, \\(T^4\\) with one clean ancilla can be decomposed in the following way.\n\nHere the second relative gate should be \\((RT^3)^\\dagger\\) but I leave it implicit in the notation. OK, but why can we replace \\(T^3\\) by \\(RT^3\\)? Because a diagonal gate commutes with a control.\n\nWires with backslash on them denote a group of several qubits. \\(U\\) can be any operator on \\(n\\) qubits. The diagonal gate can also act on any number of qubits \\(k+m\\). And they can intersect along any number of controls \\(m\\). To show this in full generality would be a bit of a notational mess. Here is the idea though. General diagonal gate can be represented as \\(D=\\sum_n P_n e^{i\\phi_n}\\) where \\(P_n=|n\\rangle\\langle n|\\) is the projector on \\(n-\\)th basis state. General controlled unitary is \\(CU = P_0\\otimes I+P_1\\otimes U\\). The matrix \\(U\\) is only applied to the qubits that are not touched by the diagonal gate, therfore \\(D\\) and \\(CU\\) only intersect along wires containing projectors. Since projectors commute, the whole operators also commute. Here is an illustration for the 2q D and \\(n\\)-qubit U\n\\[\\Big[\\Big(e^{\\phi_0}|00\\rangle\\langle00|+e^{\\phi_1}|01\\rangle\\langle01|+e^{\\phi_2}|10\\rangle\\langle10|+e^{\\phi_3}|11\\rangle\\langle11|\\Big)\\otimes I_{n+1}, I_1\\otimes |0\\rangle\\langle0|\\otimes I_{n}+I_1\\otimes |0\\rangle\\langle0|\\otimes U_n\\Big]=0\\]\nSince \\(RT^3\\) contains 3 CNOT gates the new construction for \\(T^4\\) now has only 12 CNOT gates, significantly improving on the preceding 18 CNOT decomposition."
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#special-relative-phase-gates",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#special-relative-phase-gates",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Special relative phase gates",
    "text": "Special relative phase gates\nHere is another version of \\(RT^3\\), known as a special relative phase Toffoli gate \\(SRT^3\\).\n\n\nCode\n# Similar to the privious circuit but with an additional CZ gate\nqc = QuantumCircuit(3)\nqc.h(2)\nqc.cx(0, 2)\nqc.h(2)\nqc.append(RT3, [0, 1, 2])\nqc = qc.decompose('RT3')\nqc = transpile(qc, basis_gates=['cx', 't', 'tdg', 'h'], optimization_level=3)\nqc.draw(output='mpl')\n\n# Store to gate\nSRT3 = qc.to_instruction(label='SRT3')\nSRT3dg = qc.inverse().to_instruction(label='SRT3dg')\n\n# Check that it is indeed the relative phase Toffoli 3 gate\nu_SRT3 = Operator(qc).data\nprint('is relative:', is_relative(u_SRT3, u_T3))\n\n# Draw\nqc.draw(output=output)\n\n\nis relative: True\n\n\n\n\n\nThis circuit has one more CNOT gate than \\(RT^3\\) defined above. A special feature of this gate (and the reason it is called special) is that the diagonal gate has a restricted form – it only acts on the first two qubits, graphically"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#rt4",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#rt4",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "\\(RT^4\\)",
    "text": "\\(RT^4\\)\nAs suggested by Maslov, this gate can be used to construct \\(RT^4\\) out of \\(RT^3\\). Indeed, take a look at the following circuit.\n\nIt is obtained from \\(RT^3\\) by adding an extra qubit and replacing the middle CNOT with \\(SRT^3\\). Why does this work? The new circuit must (1) do what the 3q circuit did when the state of \\(q_0\\) is \\(|1\\rangle\\) and (2) do nothing if the state of \\(q_0\\) is \\(|0\\rangle\\). When the new qubit is in state \\(|1\\rangle\\), the \\(RT^3\\) gate behaves just like a CNOT between \\(q_1\\) and \\(q_3\\) and we are back to the 3q circuit behavior. When the new qubit is \\(|0\\rangle\\), the \\(RT^3\\) gate becomes 3q identity, and all gates around it cancel in pairs, so the whole circuit is 4q identity. Good. Finally, the additional diagonal gate acting on \\(q_0, q_1\\) can be moved to the right and combined with the diagonal gate defining \\(RT^3\\). Note that if we inserted \\(RT^3\\) instead of \\(SRT^3\\) in the middle, this wouldn’t work because 3q diagonal gate would not commute with gates on qubit \\(q_3\\). This implementation of \\(RT^4\\) has 6 CNOT gates, which is apparently the optimal result.\nHere is an explicit implementation.\n\n\nCode\n# Build circuit\nqc = QuantumCircuit(4)\nqc.h(3)\nqc.t(3)\nqc.cx(2, 3)\nqc.tdg(3)\nqc.append(SRT3, [0, 1, 3])\nqc.t(3)\nqc.cx(2, 3)\nqc.tdg(3)\nqc.h(3)\nqc = qc.decompose('SRT3')\n\n# Store to gate\nRT4 = qc.to_instruction(label='RT4')\nRT4dg = RT4.inverse()\nRT4dg.label = 'RT4dg'\n\n# Check that it is indeed the relative phase Toffoli 3 gate\nu_RT4 = Operator(qc).data\nqc_T4 = QuantumCircuit(4)\nqc_T4.mct([0, 1, 2], 3)\nu_T4 = Operator(qc_T4).data\nprint('is relative:', is_relative(u_RT4, u_T4))\n\n# Draw\nqc.draw(output=output)\n\n\nis relative: True"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#rt5",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#rt5",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "\\(RT^5\\)",
    "text": "\\(RT^5\\)\nThe final ingredient I will need is \\(RT^5\\). It is not present in Maslov’s paper and I haven’t found it with a brief search in other literature, but we can use the same idea to build \\(RT^5\\) out of \\(RT^4\\) and \\(SRT^3\\). Namely, in the diagram above we replace both CNOTs acting between \\(q_0\\) and \\(q_3\\) by \\(SRT^3\\). This leads to the following circuit.\n\n\nCode\n# Build circuit\nqc = QuantumCircuit(5)\nqc.h(4)\nqc.t(4)\nqc.cx(3, 4)\nqc.tdg(4)\nqc.h(4)\nqc.barrier()\nqc.append(SRT3dg, [0, 1, 4])\nqc.barrier()\nqc.t(4)\nqc.cx(2, 4)\nqc.tdg(4)\nqc.barrier()\nqc.append(SRT3, [0, 1, 4])\nqc.barrier()\nqc.t(4)\nqc.cx(2, 4)\nqc.tdg(4)\nqc.h(4)\nqc.t(4)\nqc.cx(3, 4)\nqc.tdg(4)\nqc.h(4)\n\nqc = qc.decompose('RT3')\nqc = transpile(qc, basis_gates=['cx', 't', 'tdg', 'h'], optimization_level=3)\n\n# Check that it is indeed the relative phase Toffoli 3 gate\nu_RT5 = Operator(qc).data\nqc_T5 = QuantumCircuit(5)\nqc_T5.mct([0, 1, 2, 3], 4)\nu_T5 = Operator(qc_T5).data\nprint('is relative:', is_relative(u_RT5, u_T5))\n\n# Draw\nqc.draw(output=output)\n\n\nis relative: True\n\n\n\n\n\nSubcircuits between barriers are the inserted \\(SRT^3\\) gates. This circuit has 12 CNOT gates. Apparently, by looking at it carefully enough one can find a pair of CNOT gates that cancel each other. I stumbled upon this fact by accident. I tried to substitute shorter \\(RT^3\\) instead of \\(SRT^3\\) here and was sure it would fail, but it worked. The resulting decomposition of \\(RT^5\\) has only 10 CNOT gates. Here is the circuit.\n\n\nCode\n# Build circuit\nqc = QuantumCircuit(5)\nqc.h(4)\nqc.t(4)\nqc.cx(3, 4)\nqc.tdg(4)\nqc.h(4)\nqc.append(RT3, [0, 1, 4])\nqc.t(4)\nqc.cx(2, 4)\nqc.tdg(4)\nqc.append(RT3, [0, 1, 4])\nqc.t(4)\nqc.cx(2, 4)\nqc.tdg(4)\nqc.h(4)\nqc.t(4)\nqc.cx(3, 4)\nqc.tdg(4)\nqc.h(4)\n\nqc = qc.decompose('RT3')\nqc = transpile(qc, basis_gates=['cx', 't', 'tdg', 'h'], optimization_level=3)\n\n# Store to gate\nRT5 = qc.to_instruction(label='RT5')\nRT5dg = RT5.inverse()\nRT5dg.label = 'RT5dg'\n\n\n# Check that it is indeed the relative phase Toffoli 3 gate\nu_RT5 = Operator(qc).data\nqc_T5 = QuantumCircuit(5)\nqc_T5.mct([0, 1, 2, 3], 4)\nu_T5 = Operator(qc_T5).data\nprint('is relative:', is_relative(u_RT5, u_T5))\n\n# Draw\nqc.draw(output=output)\n\n\nis relative: True"
  },
  {
    "objectID": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#generalization-of-relative-phase-gates",
    "href": "posts/classiq_toffoli/2022-06-21-classiq toffoli.html#generalization-of-relative-phase-gates",
    "title": "Classiq coding competition – Toffoli gate decomposition",
    "section": "Generalization of relative phase gates",
    "text": "Generalization of relative phase gates\nIt is possible to further generalize relative phase gates by allowing more general operators which cancel in compute-uncompute pairs. See Maslov’s paper for the use of more general gates with dirty ancilla qubits."
  },
  {
    "objectID": "posts/entanglement/2021-07-12-entanglement.html",
    "href": "posts/entanglement/2021-07-12-entanglement.html",
    "title": "What is entanglement?",
    "section": "",
    "text": "Introduction\nI’ve known the formal definition of entanglement for years, but I am only now appreciating many of its profound implications. In this post I would like to share two aspects that put entangled states into sharp contrast with unentangled (separable pure) states and classical random variables. Instead of proofs I provide references and simple experiments in qiskit.\n\n\n\n\n\nEntanglement is the failure of states to factorize\nSo what is entanglement? Entanglement is what entangled states have. What are those? Take two spins. The state\n\\[\\begin{align}\n\\frac1{\\sqrt{2}}\\Big(|\\uparrow\\uparrow\\rangle+|\\downarrow\\downarrow\\rangle\\Big) \\label{bell}\n\\end{align}\\] is your canonical example of an entangled stated. In contrast, all the states below are unentangled \\[\\begin{align}\n|\\uparrow\\uparrow\\rangle,\\qquad |\\downarrow\\uparrow\\rangle,\\qquad \\frac1{\\sqrt{2}}|\\uparrow\\rangle\\Big(|\\uparrow\\rangle-|\\downarrow\\rangle\\Big),\\qquad \\frac1{\\sqrt{2}}\\Big(|\\uparrow\\rangle-|\\downarrow\\rangle\\Big)\\Big(|\\uparrow\\rangle+|\\downarrow\\rangle\\Big)  \\label{unen}\n\\end{align}\\]\nThe difference between \\(\\eqref{bell}\\) and \\(\\eqref{unen}\\) is the following. All latter states are actually products of the form \\(|\\psi_1\\rangle |\\psi_2\\rangle\\) where \\(|\\psi_1\\rangle\\) is the state of the first system and \\(|\\psi_2\\rangle\\) of the second. In contrast, state \\(\\eqref{bell}\\) can not be represented in as a product. It is instead a linear combination of factorized states which is not reducible to a single product. You can define entangled states by this property of not being factorizible into states of consistuent spins.\nNow that we know what entangled states are it is perfectly reasonable to ask: “so what?”. Why are entangled states special? I am going to give two angles on this questions, out of many possible.\n\nNote for the sake of concreteness and simplicity I talk about “spins”. In the context of discrete-variable quantum computation “spin”\\(\\equiv\\)“qubit”, but I prefer spins, because they come with a useful geometrical intuition. The abstract Bloch sphere associated to a qubit describes an actual orientation of a spin in \\(3d\\) space.\n\n\nEntangled spin behaves very differently from unentangled\nA spin which is not entangled can always be described by a direction \\(\\bf n\\) along which it is pointing \\(|\\uparrow_{\\bf n}\\rangle\\). If one measures the component of the spin along this direction, the result is always \\(\\frac12\\). Such a measurement corresponds to a projector \\(P({\\bf n})={\\bf n}\\cdot {\\bf \\sigma}=n_x \\sigma_x+n_y\\sigma_y+n_z\\sigma_z\\). If state \\(|\\uparrow_{\\bf n}\\rangle\\) is measured along a different axis \\(\\bf n'\\) the result depends on the angle \\(\\theta\\) between \\(\\bf n\\) and \\(\\bf n'\\). With probability \\(\\cos^2\\frac\\theta2\\) one gets projection \\(+\\frac12\\) and with probability \\(\\sin^2\\frac\\theta2\\) one gets \\(-\\frac12\\). However, for any state of the spin \\(|\\psi\\rangle\\) there is an axis \\(\\bf n\\), such that measuring the spin along this axis gives \\(\\frac12\\) with probability one.\nThis is also true for any of the unentangled states \\(\\eqref{unen}\\). For example, measuring the projection of the first spin in the state \\(|\\uparrow\\uparrow\\rangle\\equiv |\\uparrow_{\\bf z}\\uparrow_{\\bf z}\\rangle\\) along \\(\\bf z\\) always gives \\(+\\frac12\\). As another example, since \\[\\begin{align}\n|\\downarrow_{\\bf x}\\rangle=\\frac12\\Big(|\\uparrow_{\\bf z}\\rangle-|\\downarrow_{\\bf z}\\rangle\\Big) \\label{xdown}\n\\end{align}\\] the state \\(\\frac1{\\sqrt{2}}\\Big(|\\uparrow\\rangle-|\\downarrow\\rangle\\Big)\\Big(\\uparrow\\rangle+|\\downarrow\\rangle\\Big)\\) always registers \\(-\\frac12\\) when the projection of the first spin along \\(\\bf x\\) axis is measured.\nIn contrast, for the maximally entangled state \\(\\eqref{bell}\\) the axis with a definite projection of the first spin does not exist. In fact, for all intents and purposes, if you only look at observables associated with the first qubit, state \\(\\eqref{bell}\\) behaves as a statistical ensemble of states \\(|\\uparrow\\rangle\\) and \\(|\\downarrow\\rangle\\), i.e.\n\\[\\begin{align}\n\\frac1{\\sqrt{2}}\\Big(|\\uparrow\\uparrow\\rangle+|\\downarrow\\downarrow\\rangle\\Big)\\approx \\cases{|\\uparrow\\rangle \\text{ with probability $\\frac12$}\\\\ |\\downarrow\\rangle \\text{ with probability $\\frac12$}} \\label{bellapprox}\n\\end{align}\\]\nThis means, for example, that projection onto \\(\\bf z\\) axis of the first spin is completely random: with probability \\(\\frac12\\) it behaves as \\(|\\uparrow\\rangle\\) and gives projection \\(+\\frac12\\), with probability \\(\\frac12\\) it behaves as \\(|\\downarrow\\rangle\\) and gives projection \\(-\\frac12\\). This is different from a coherent superposition of the up and down states, such as \\(\\eqref{xdown}\\). Although state \\(\\eqref{xdown}\\) gives random results when measured along \\(\\bf z\\), it gives certain results when measured along \\(\\bf x\\). There is no such axis for state \\(\\eqref{bellapprox}\\). In fact, the spin projection along any axis is completely random.\nTo prove this fact I would need to go into some details of how one does construct an ensemble from an entangled state. This is not at all difficult but I won’t do it here. I encourage an interested reader to consult John Preskill’s notes (chapter 2.3).\nInstead, let me do a quick experimental check using qiskit. A Hadamard gate followed by a CNOT creates our state \\(\\eqref{bell}\\):\n\nfrom qiskit import QuantumCircuit, BasicAer, execute\nfrom qiskit.visualization import plot_histogram\n\nqc = QuantumCircuit(2, 1)\nqc.h(0)\nqc.cx(0, 1)\nqc.draw(output='mpl')\n\n\n\n\nTo my knowledge, one can only measure in the computational basis in qiskit, i.e. only along \\(\\bf z\\) axis in our terminology. To measure a spin along some axis \\(\\bf n\\) we can instead rotate the spin itself, and then measure along \\(\\bf z\\) axis. Mathematically, if \\({\\bf n} = R^{-1} {\\bf z}\\) for some rotation \\(R\\) then \\(\\langle \\uparrow_{\\bf z}|P({\\bf n})| \\uparrow_{\\bf z}\\rangle=\\langle \\uparrow_{R{\\bf z}}|P({\\bf z})| \\uparrow_{R{\\bf z}}\\rangle\\).\n\n# These parameters define an axis along which we will measure.\n# Feel free to change them and see if the outcome distribution changes.\ntheta, pi, lam = 0.13, 0.89, 0.37 \n\nqc.u(theta, pi, lam, 0) # Rotate the qubit.\nqc.measure(0, 0)\n\n# Execute on a simulator and plot a histogram of the result.\nbackend = BasicAer.get_backend('qasm_simulator')\nresult = execute(qc, backend, shots=1000).result()\ncounts  = result.get_counts(qc)\nplot_histogram(counts)\n\n\n\n\nThe result looks like a fair sample from the uniform probability distribution. This means that projection on the axis we have specified is indeed random. You can try to change the axis and see if you can get a biased distribution (spoiler: you can not).\n\n\nEntanglement correlations are stronger than classical\nFirst let me note that although we talked about the first spin before, the state \\(\\eqref{bell}\\) is symmetric and everything equally applies to the second spin. Although the behavior of each of these spins is completely random, there are strong correlations between the them. If we can make local measurements on both spins the state \\(\\eqref{bell}\\) behaves as\n\\[\\begin{align}\n\\frac1{\\sqrt{2}}\\Big(|\\uparrow\\uparrow\\rangle+|\\downarrow\\downarrow\\rangle\\Big)\\approx \\cases{|\\uparrow\\uparrow\\rangle \\text{ with probability $\\frac12$}\\\\ |\\downarrow\\downarrow\\rangle \\text{ with probability $\\frac12$}} \\label{bellapprox2}\n\\end{align}\\]\nSo for example projections onto \\(\\bf z\\) axis of both spins are always the same, although random. Again, this in fact holds for any axis. Here is an experimental verification.\n\n# Building Bell's state.\nqc = QuantumCircuit(2, 2)\nqc.h(0)\nqc.cx(0, 1)\n\n# Rotation of each qubit to simulate measurement along arbitary axis.\ntheta, pi, lam = 0.13, 0.89, 0.37 \n\nqc.u(theta, pi, lam, 0)\nqc.u(theta, pi, lam, 1)\nqc.measure([0, 1], [0, 1])\n\n# Simulate and plot results.\nbackend = BasicAer.get_backend('qasm_simulator')\nresult = execute(qc, backend, shots=2000).result()\ncounts  = result.get_counts(qc)\nplot_histogram(counts)\n\n\n\n\nThe result I get is almost certainly a uniform distribution of over \\(00=|\\uparrow_{\\bf n}\\uparrow_{\\bf n}\\rangle\\) and \\(11=|\\downarrow_{\\bf n}\\downarrow_{\\bf n}\\rangle\\) (you can change \\(\\bf n\\) by changing angles in the code), however I also get a tiny number of spurious counts for \\(01\\) and \\(10\\), which is probably a bug, hm.\nWhen seeing this for the first time there is definitely something to contemplate, like say an EPR paradox. Spoiler: it is not possible to use these correlations for superluminal transmission of information, but they are still a valuable resource. I will discuss just one manifestation of these quantum correlations which has a very concrete operational interpretation – it allows a quantum team to play a certain probabilistic game better than any classical team could! Note that this is also basically Bell’s theorem in disguise.\nSo here is the setup. Alice and Bob are playing together against Charlie. Charlie sends random uncorrelated bits \\(x\\) to Alice and \\(y\\) to Bob. Admittedly, Charlie’s job is not very creative and nothing in his strategy can be changed. Now, in response to the obtained bits Alice produces her output bit \\(a\\) and Bob his \\(b\\). Team A&B wins if \\(a\\oplus b=x\\land y\\) where \\(\\oplus\\) is XOR (sum modulo 2) and \\(\\land\\) is the logical AND. Explicitly, if \\(x\\land y=1\\) both Alice and Bob got \\(x=y=1\\) (which happens one quarter of the time) and they win iff they respond \\(a=0, b=1\\) or \\(a=1, b=0\\) so that \\(a\\oplus b=1\\). For all other inputs from Charlie, i.e. when \\((x,y)\\) is equal to \\((0,0), (1,0)\\) or \\((0,1)\\) the logical sum \\(x\\land y=0\\) and Alice and Bob win iff \\(a=0,b=0\\) or \\(a=1, b=1\\) so that \\(a\\oplus b=0\\).\nNow, although in the same team, Alice and Bob are not allowed to communicate during the game. But they can discuss their strategy in advance. The best that a classical team can do is to win \\(75\\%\\) of the time. To achieve this winning rate it is sufficient to simply output \\(a=0, b=0\\) irrespective of Charlie’s bits \\(x,y\\). This strategy only loses when \\(x=y=1\\), i.e. one quarter of the time.\nNow comes the interesting part. If Alice and Bob each have a spin, and these spins are entangled as in state \\(\\eqref{bell}\\), they can achieve the winning probability \\[\\begin{align}\nP_{win}= \\frac12+\\frac1{2\\sqrt{2}}\\approx 0.85! \\label{pwin}\n\\end{align}\\] So, what should they do?\nDefine four axes \\(\\bf n_1,n_2,n_3,n_4\\) in the \\(\\bf xz\\) plane (of course this is just one of the possibilities). Take \\({\\bf n_1}= (1,0)\\), then \\({\\bf n_2}=(\\frac1{\\sqrt{2}},\\frac1{\\sqrt{2}})\\) is counter-clockwise rotated by \\(\\pi/4\\) wrt to \\(\\bf n_1\\); \\({\\bf n_3}=(0,1)\\) is rotated by \\(\\pi/2\\); and finally \\({\\bf n_4}=(-\\frac1{\\sqrt{2}},\\frac1{\\sqrt{2}})\\) is rotated by \\(3\\pi/4\\).\n\n\n\nNow here is the strategy that Alice and Bob follow \\[\\begin{align}\na(x)=\\cases{P_{\\bf n_3}, \\qquad x=0 \\\\P_{\\bf n_1}, \\qquad x=1}\\qquad\\qquad b(y)=\\cases{P_{\\bf n_2}, \\qquad y=0 \\\\P_{\\bf n_4}, \\qquad y=1} \\label{abcases}\n\\end{align}\\]\nWhere \\(P_{\\bf n}=+1\\) if Alice’s (or Bob’s) spin gave projection \\(+\\frac12\\) when measured along \\(\\bf n\\) and \\(P_{\\bf n}=0\\) if the projection was \\(-\\frac12\\). An example: if Alice recieves \\(x=0\\) and Bob \\(y=1\\) Alice measures her spin along \\(n_3=\\bf z\\) axis and sends back the result, while Bob measures his spin along \\(\\bf{n_4}\\) (which is \\(3\\pi/4\\) rotated \\(\\bf x\\) axis) and sends his result.\nNow, shall we check that this strategy indeed achieves the advertised winning probability \\(\\eqref{pwin}\\)? Sure, I also thought so!\n\nimport numpy as np\n\n# Define rotation axes by their angles.\ntheta1 = 0\ntheta2 = np.pi/4\ntheta3 = np.pi/2\ntheta4 = 3*np.pi/4\n\ndef charlie():\n    # Charlies job is to generate two random bits.\n    return np.random.randint(0,1+1, size=(2))\n\ndef alice(x):\n    # Alice decides on the measurement axis according to her strategy.\n    if x==0:\n        return theta3\n    if x==1:\n        return theta1\n\ndef bob(x):\n    # Bob does his part of the protocol.\n    if x==0:\n        return theta2\n    if x==1:\n        return theta4\n    \ndef one_round():\n    \n    # First we prepare an entangled state.\n    qc = QuantumCircuit(2, 2)\n    qc.h(0)\n    qc.cx(0, 1)\n    \n    # Now Charlie generates his bits.\n    x, y = charlie()\n    \n    # A&B team makes their move.\n    a_angle = alice(x)\n    b_angle = bob(y)\n    \n    # Again, we can not measure directly along the desired axes, \n    # but must rotate the qubits instead. Rotation in the xz plane is made by `ry` gate.\n    qc.ry(a_angle, 0) # Alice rotates her qubit.\n    qc.ry(b_angle, 1) # Bob his.\n    \n    # Now we add measurments and actually run the circuit.\n    qc.measure([0, 1], [0, 1])\n    backend = BasicAer.get_backend('qasm_simulator')\n    result = execute(qc, backend, shots=1).result()\n    counts  = result.get_counts(qc)\n    \n    # Output of counts is a dict like `{'01': 1}`. This extracts the measurment results:\n    a, b = [int(c) for c in list(counts.keys())[0]]\n    \n    # And now we check, team A&B gogogo!\n    return (a + b) % 2 == x * y\n\nAlright, now let us collect the statistics:\n\nnum_rounds = 2000\n\nwins = 0\nfor _ in range(num_rounds):\n    wins += one_round()\nprint (\"Win probability:{}\".format(wins/num_rounds))\n\nWin probability:0.847\n\n\nSo that’s pretty close to the theoretical value \\(\\eqref{pwin}\\). Note that for each round of the game a new entangled pair is needed.\nNow that we have seen that the strategy works let us briefly discuss why. I will only give a sketch and refer for details to Preskill’s lectures chap 4.3.\nOne thing Alice and Bob could do is to always measure along the same axes. Then, their results would be perfectly correlated (i.e. they always output \\(a=b=0\\) or \\(a=b=1\\)) which gives 0.75 winning probability, the same as the best deterministic strategy. Now, in one quarter of cases (when \\(x=y=1\\)) they are better off outputting anticorrelated results. If we revisit the figure above equation \\(\\eqref{abcases}\\) we see that the angle between \\(a(1)\\) and \\(b(1)\\) is \\(3\\pi/4\\) which indeed gives a negative correlation in this case \\(\\Big(\\cos \\frac{3\\pi}{4}=-\\frac{1}{\\sqrt{2}}\\Big)\\). The price to pay is that angles between \\(\\Big(a(0),b(0)\\Big)\\), \\(\\Big(a(0),b(1)\\Big)\\) and \\(\\Big(a(1),b(0)\\Big)\\) are now non-zero (and hence correlations are less than 1) which makes this strategy lose in some cases when the deterministic strategy wins. However, as we have seen experimentally the trade-off is still in our favor. It is also possible to prove that our choice of axes gives the maximum possible win probability. This is ultimately bound by Tsirelson’s bound, see below.\nNow you might ask – what if there exists a clever randomized classical strategy which would perform better than deterministic 0.75 using a similar trick? Turns out this is not possible. The proof is based on the following inequality \\[\\begin{align}\n\\Big|\\langle a_0 b_0\\rangle+\\langle a_0 b_1\\rangle+\\langle a_1 b_0\\rangle-\\langle a_1 b_1\\rangle\\Big|\\leq 2\n\\end{align}\\] which holds for any random variables \\(a_0, a_1, b_0, b_1\\) taking values \\(\\pm1\\) and described by a joint probability distribution. This is known as CHSH inequality and a technical proof is trivial. Why quantum correlations do not have to obey the bound? Well, the reason is somewhat deep and quantum and ultimately related to Bohr’s complementarity – non-commuting observables can not be simultaneously assigned values. That this statement has quantitative consequences is illustrated by Bell’s theorem or our game.\nTehcnically quantum correlations obey the Tsirelson’s bound \\[\\begin{align}\n\\Big|\\langle a_0 b_0\\rangle+\\langle a_0 b_1\\rangle+\\langle a_1 b_0\\rangle-\\langle a_1 b_1\\rangle\\Big|\\leq 2\\sqrt{2}\n\\end{align}\\] which, as you see, is weaker by a factor \\(\\sqrt{2}\\), so the correlations themselves can be stronger, although still bounded.\n\n\nFinal remarks\nQuantum entanglement is indeed very unusual and consequential. There are many more wonders that it entails, please consult your favorite lecture notes for a non-exhaustive list. My current favorite are John Preskill’s lecture notes. For a non-mathematical although technically very accurate discussion of entanglement see this artice by Frank Wilczek entanglement made simple.\nAny questions and suggestions are welcome, as this is my first blog demo.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/machine_learning_quantum_ecc/2022-06-16-machine learning error correction codes.html",
    "href": "posts/machine_learning_quantum_ecc/2022-06-16-machine learning error correction codes.html",
    "title": "Machine learning error correction codes",
    "section": "",
    "text": "Code\ntry:\n   import optax\nexcept ImportError:\n  !pip install optax\n\ntry:\n    from mynimize import *\nexcept ImportError:\n    !git clone https://github.com/idnm/mynimize\n    !git reset --hard e11daa3396ef7682fccf744ce3dce0262cbbfac2\n    from mynimize.main import *\n\nfrom collections import namedtuple\nfrom functools import reduce\n\nfrom jax.scipy.linalg import expm\nfrom jax import random\nfrom scipy.stats import unitary_group"
  },
  {
    "objectID": "posts/machine_learning_quantum_ecc/2022-06-16-machine learning error correction codes.html#projecting-the-final-state",
    "href": "posts/machine_learning_quantum_ecc/2022-06-16-machine learning error correction codes.html#projecting-the-final-state",
    "title": "Machine learning error correction codes",
    "section": "Projecting the final state",
    "text": "Projecting the final state\nThere are still details to be filled in. One is to specify how to get a single-qubit logical state from the final state of the physical qubits. Similarly to the embedding step, I will assume that the relevant information is contained exclusively in the first physical qubit. Then, successful error correction implies that the first physical qubit is unentangled with the others after the decoding step and has the same state it had before the encoding.\n\\[|\\psi\\rangle\\otimes |0\\rangle^{n-1} \\to \\text{Encoding+Error+Decoding} \\to |\\psi\\rangle\\otimes |e\\rangle_{n-1}\\]\nNote that the rest of the physical qubits will end up in different states \\(|e\\rangle_{n-1}\\) depending on the error that have been corrected. Requiring that the final state is \\(|\\psi\\rangle\\otimes |0\\rangle^{n-1}\\) regardless of the error is too strong and can not be satisfied for any interesting set of errors."
  },
  {
    "objectID": "posts/machine_learning_quantum_ecc/2022-06-16-machine learning error correction codes.html#dealing-with-continuum",
    "href": "posts/machine_learning_quantum_ecc/2022-06-16-machine learning error correction codes.html#dealing-with-continuum",
    "title": "Machine learning error correction codes",
    "section": "Dealing with continuum",
    "text": "Dealing with continuum\nNext, how do we deal with the continuum of states and errors? I guess that a truly black-box approach would be to generate a large set of initial states and single-qubit errors and train the model using all this data. If successful, check on the test data to exclude overfitting. I’m sure that would work, but here I will take a shortcut and exploit the linearity of the whole construction. Denote by \\(U(E)\\) the full unitary of the encoding+error+correction process, for some error \\(E\\)\n\\[U(E)=U_{decoding}\\,\\, U_{error}(E) \\,\\, U_{encoding} \\ .\\]\nFor a given initial state \\(|\\psi\\rangle=\\alpha |0\\rangle+\\beta |1\\rangle\\) and a fixed error \\(E\\), the final state can be reconstructed from the action on \\(|0\\rangle\\) and \\(|1\\rangle\\) states\n\\[|\\psi\\rangle\\otimes |0\\rangle^{n-1}=\\alpha\\,\\, U(E) |0\\rangle\\otimes|0\\rangle^{n-1}+\\beta\\,\\, U(E)|1\\rangle\\otimes|0\\rangle^{n-1} \\ .\\]\nSimilarly, if we can correct errors corresponding to \\(X, Y\\) and \\(Z\\) unitaries on a given qubit, we will be able to correct an arbitrary linear combination of them, which is unitary. Indeed, say we can correct both \\(X\\) and \\(Y\\) errors \\[\\begin{align*}\n|\\psi\\rangle\\otimes |0\\rangle^{n-1}\\to U(X) \\to |\\psi\\rangle\\otimes |x\\rangle_{n-1} \\ ,\\\\\n|\\psi\\rangle\\otimes |0\\rangle^{n-1}\\to U(Y) \\to |\\psi\\rangle\\otimes |y\\rangle_{n-1} \\ .\n\\end{align*}\\] Then their unitary linear combination will also be corrected in a sence that the state of the first physical qubit is the original encoded state \\[\\begin{align*}\n|\\psi\\rangle\\otimes |0\\rangle^{n-1}\\to U(aX+bY) \\to |\\psi\\rangle\\otimes\\left(a|x\\rangle_{n-1}+b|y\\rangle_{n-1}\\right) \\ .\n\\end{align*}\\] In fact, correcting \\(X,Y\\) and \\(Z\\) errors on any of the qubits is sufficient to correct their arbitrary linear combination, including non-unitary ones and those acting on different qubits. More on that later."
  },
  {
    "objectID": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html",
    "href": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html",
    "title": "How was this blog set up?",
    "section": "",
    "text": "This blog was setup in 2021 using fastpages. Two years later my site’s layout broke and I couldn’t fix it by reverting to previous commits. Then I went to check if I can update utils running under the hood and discovered that fastpages are deprecated in favor on another notebook-based publishing tool quarto. After a deep sigh, I decided to migrate the blog to quatro instead of trying to fix the fastpages version. The majority of the work is taken care automatically, see this this migration guide. I’m putting down a list of things that didn’t work immediately, or that I had to spent some time looking for, just in case.\n\nIt’s really hard to type quatro instead of quarto or quadro when you are web searching.\nTo have a custom icon on top of your blog’s webpage include the following file in _quarto.yml\n---\nwebsite:\n  favicon: /path/to/image.png\n---\nIf paths to your images do not work, try /path/image.png instead of path/image.png or vice-versa. Or see here ?.\nYou can specify the last time the post was modified by including date-modified: 'xxxx-xx-xx' in the metadata.\nAgain, I had a problem with numbering and referencing equations. And again, I found a github comment that solved the issue. Including\n---\nformat:\n  html:\n    html-math-method: mathjax\n    include-in-header:\n      - text: |\n          &lt;script&gt;\n          window.MathJax = {\n            tex: {\n              tags: 'ams'\n            }\n          };\n          &lt;/script&gt;\n ---\nin the blog’s notebook header solves the issue without any need to modify the standard latex labeling conventions. However, apparently this solution might break in the future.\nTo exclude posts from your ‘about’ page put listing: false in the about.qmd.\n\nProblems not solved.\n\nI could not figure out how to render preview images for blog posts properly. They typically have wrong size. It’s possible to configure height of preview images in _quarto.yml\n---\nlisting:\n    image-height: 150px\n---\nHowever, this will crop, not resize the image. Heuristically I found that when the images have certain proportion (approximately the a4 paper size wide side down) they render well enough. So I had to went back and resize canvas of all my preview images. Hope there is a good solution that I’ve missed.\nfastpages provided a really nice automatic badge for opening the notebook in colab. I think there is no support for this yet in quatro, but you can generate the badge yourself.\n\nI must say that so far I really like the preview tool of quarto. Just call\nquarto preview\nin the shell and you quickly get a local version of your blog that is instantly updated as you change something in the posts or settings, much better than the preview I had with fastpages. Apparently, you things can get even better than that by using a proper IDE to work with quarto, but here I can offer little advice.\nI also recommend checking out this guide on how to setup a tweak a quatro blog.\n\n\n\nAfter deciding to start a scientific blog I was looking for an appropriate technical solution. My main requirements were - Ease of set up. - Ease of writing posts. - Decent support of \\(\\LaTeX\\). - Support of code snippets.\nAfter some search I decided to try out fastpages. I have a very limited understanding of the stack that fastpages use, so I treat it as a magic box. The magic box was easy for me to install while other bullet points are addressed all at once since fastpages allows to generate a post from a jupyter notebook. Although jupyter notebook is not exactly my favorite \\(\\LaTeX\\) editor it still much better than many other options and a good overall compromise. So essentially with fastpages you can write your posts in jupyter notebook, then commit to your github repository and the content will automatically be hosted at your domain on github pages."
  },
  {
    "objectID": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#update-quatro",
    "href": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#update-quatro",
    "title": "How was this blog set up?",
    "section": "",
    "text": "This blog was setup in 2021 using fastpages. Two years later my site’s layout broke and I couldn’t fix it by reverting to previous commits. Then I went to check if I can update utils running under the hood and discovered that fastpages are deprecated in favor on another notebook-based publishing tool quarto. After a deep sigh, I decided to migrate the blog to quatro instead of trying to fix the fastpages version. The majority of the work is taken care automatically, see this this migration guide. I’m putting down a list of things that didn’t work immediately, or that I had to spent some time looking for, just in case.\n\nIt’s really hard to type quatro instead of quarto or quadro when you are web searching.\nTo have a custom icon on top of your blog’s webpage include the following file in _quarto.yml\n---\nwebsite:\n  favicon: /path/to/image.png\n---\nIf paths to your images do not work, try /path/image.png instead of path/image.png or vice-versa. Or see here ?.\nYou can specify the last time the post was modified by including date-modified: 'xxxx-xx-xx' in the metadata.\nAgain, I had a problem with numbering and referencing equations. And again, I found a github comment that solved the issue. Including\n---\nformat:\n  html:\n    html-math-method: mathjax\n    include-in-header:\n      - text: |\n          &lt;script&gt;\n          window.MathJax = {\n            tex: {\n              tags: 'ams'\n            }\n          };\n          &lt;/script&gt;\n ---\nin the blog’s notebook header solves the issue without any need to modify the standard latex labeling conventions. However, apparently this solution might break in the future.\nTo exclude posts from your ‘about’ page put listing: false in the about.qmd.\n\nProblems not solved.\n\nI could not figure out how to render preview images for blog posts properly. They typically have wrong size. It’s possible to configure height of preview images in _quarto.yml\n---\nlisting:\n    image-height: 150px\n---\nHowever, this will crop, not resize the image. Heuristically I found that when the images have certain proportion (approximately the a4 paper size wide side down) they render well enough. So I had to went back and resize canvas of all my preview images. Hope there is a good solution that I’ve missed.\nfastpages provided a really nice automatic badge for opening the notebook in colab. I think there is no support for this yet in quatro, but you can generate the badge yourself.\n\nI must say that so far I really like the preview tool of quarto. Just call\nquarto preview\nin the shell and you quickly get a local version of your blog that is instantly updated as you change something in the posts or settings, much better than the preview I had with fastpages. Apparently, you things can get even better than that by using a proper IDE to work with quarto, but here I can offer little advice.\nI also recommend checking out this guide on how to setup a tweak a quatro blog."
  },
  {
    "objectID": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#fastpages",
    "href": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#fastpages",
    "title": "How was this blog set up?",
    "section": "",
    "text": "After deciding to start a scientific blog I was looking for an appropriate technical solution. My main requirements were - Ease of set up. - Ease of writing posts. - Decent support of \\(\\LaTeX\\). - Support of code snippets.\nAfter some search I decided to try out fastpages. I have a very limited understanding of the stack that fastpages use, so I treat it as a magic box. The magic box was easy for me to install while other bullet points are addressed all at once since fastpages allows to generate a post from a jupyter notebook. Although jupyter notebook is not exactly my favorite \\(\\LaTeX\\) editor it still much better than many other options and a good overall compromise. So essentially with fastpages you can write your posts in jupyter notebook, then commit to your github repository and the content will automatically be hosted at your domain on github pages."
  },
  {
    "objectID": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#solved",
    "href": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#solved",
    "title": "How was this blog set up?",
    "section": "Solved",
    "text": "Solved\n\nI wanted to use numbered \\(\\LaTeX\\) equations with hyperlinks, which are not easily supported. This comment solved my problem!\nYou need to edit _pages/about.md to customize the way your “about” page is displayed.\nTo customize the front page you need to edit index.html. This is literally written on the front page of your blog, but I have not noticed it for a while.\nInitially a lot of troubleshooting is needed to get the appearance of the blog I wanted. Commiting and waiting for the online web page to set up is super-slow. Here is an official guide on how to setup a live preview of your blog locally. One minor point that was a problem for me is that the default local server for blog preview https://127.0.0.1:4000 was not correct. After running sudo make server one of the outputs that jekyll produces is Server address: http://0.0.0.0:4000/blog/ which was the correct address for the live preview of my blog.\nYou need to do some work to make your site appear in google search results. This manual is very helpful, but a bit outdated: some of the things like generating sitemap.xml are now automated and do not require additional work as described in that post."
  },
  {
    "objectID": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#not-solved",
    "href": "posts/how_was_this_blog_setup/2021-07-11-how this blog was set up.html#not-solved",
    "title": "How was this blog set up?",
    "section": "Not solved",
    "text": "Not solved\n\nOn the web page the display equations of \\(\\LaTeX\\) have fluctuations in size which does not look good."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html",
    "title": "Machine learning compilation of quantum circuits",
    "section": "",
    "text": "I am going to review a recent preprint by Liam Madden and Andrea Simonetto that uses techniques from machine learning to tackle the problem of quantum circuits compilation. I find the approach suggested in the paper very interesting and the preliminary results quite promising.\n\n\n\nNote that a variety of terms are floating around the literature and used more or less interchangibly. Among those are synthesis, compilation, transpilation and decomposition of quantum circuits. I will not make a distinction and try to stick to compilation.\n\nBut first things first, what is a compilation of a quantum circuit? The best motivation and illustration for the problem is the following. Say you need to run a textbook quantum circuit on a real hardware. The real hardware usually allows only for a few basic one and two qubit gates. In contrast, your typical textbook quantum circuit may feature (1) complex many-qubit gates, for example multi-controlled gates and (2) one and two qubit gates which are not supported by the hardware. As a simple example take this 3-qubit Grover’s circuit (from qiskit textbook):\n\n\nCode\n#initialization\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# importing Qiskit\nfrom qiskit import IBMQ, Aer, assemble, transpile\nfrom qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\nfrom qiskit.providers.ibmq import least_busy\n\n# import basic plot tools\nfrom qiskit.visualization import plot_histogram\n\ndef initialize_s(qc, qubits):\n    \"\"\"Apply a H-gate to 'qubits' in qc\"\"\"\n    for q in qubits:\n        qc.h(q)\n    return qc\n\ndef diffuser(nqubits):\n    qc = QuantumCircuit(nqubits)\n    # Apply transformation |s&gt; -&gt; |00..0&gt; (H-gates)\n    for qubit in range(nqubits):\n        qc.h(qubit)\n    # Apply transformation |00..0&gt; -&gt; |11..1&gt; (X-gates)\n    for qubit in range(nqubits):\n        qc.x(qubit)\n    # Do multi-controlled-Z gate\n    qc.h(nqubits-1)\n    qc.mct(list(range(nqubits-1)), nqubits-1)  # multi-controlled-toffoli\n    qc.h(nqubits-1)\n    # Apply transformation |11..1&gt; -&gt; |00..0&gt;\n    for qubit in range(nqubits):\n        qc.x(qubit)\n    # Apply transformation |00..0&gt; -&gt; |s&gt;\n    for qubit in range(nqubits):\n        qc.h(qubit)\n    # We will return the diffuser as a gate\n    U_s = qc.to_gate()\n    U_s.name = \"U$_s$\"\n    return U_s\n\nqc = QuantumCircuit(3)\nqc.cz(0, 2)\nqc.cz(1, 2)\noracle_ex3 = qc.to_gate()\noracle_ex3.name = \"U$_\\omega$\"\n\nn = 3\ngrover_circuit = QuantumCircuit(n)\ngrover_circuit = initialize_s(grover_circuit, [0,1,2])\ngrover_circuit.append(oracle_ex3, [0,1,2])\ngrover_circuit.append(diffuser(n), [0,1,2])\ngrover_circuit = grover_circuit.decompose()\ngrover_circuit.draw(output='mpl')\n\n\n\n\n\nThe three qubit gates like Toffoli are not generally available on a hardware and one and two qubit gates my be different from those in the textbook algorithm. For example ion quantum computers are good with Mølmer–Sørensen gates and may need several native one qubit gates to implement the Hadamard gate.\nAdditional important problem is to take into account qubit connectivity. Usually textbook algorithms assume full connectivity, meaning that two-qubit gates can act on any pair of qubits. On most hardware platforms however a qubit can only interact with its neighbors. Assuming that one and two qubits gates available on the hardware can implement a SWAP gate between adjacent qubits, to solve the connectivity problem one can insert as many SWAPs as necessary to connect topologically disjoint qubits. Using SWAPs however leads to a huge overhead in the number of total gates in the compiled circuit, and it is of much importance use them as economically as possible. In fact, the problem of optimal SWAPping alone in generic situation is NP-complete.\n\n\n\nWhen compiling a quantum circuit one has to decide which resulting circuits are considered to be efficient. Ideally, one should optimize for the total fidelity of the circuit. Let us imagine running the algorithm on a real device. Probably my theorist’s image of a real device is still way too platonic, but I will try my best. Many details need to be taken into account. For example, gates acting on different qubits or pairs of qubits may have different fidelities. Decoherence of qubits with time can make circuits where many operations can be executed in parallel more favorable. Cross-talk (unwanted interactions) between neighboring qubits may lead to exotic patterns for optimal circuits. A simple proxy for the resulting fidelity that is often adopted is the number of two-qubit gates (which are generically much less accurate than a single-qubit gates). So the problem that is often studied, and that is addressed in the preprint we are going to discuss, is the problem of optimal compilation into a gate set consisting of arbitrary single-qubit gates and CNOTs, the only two qubits gate. The compiled circuit must\n\nRespect hardware connectivity.\nHave as few CNOTs as possible.\nExceed a given fidelity threshold.\n\nLast item here means that we also allow for an approximate compilation. By increasing the number of CNOTs one can always achieve an exact compilation, but since in reality each additional CNOT comes with its own fidelity cost this might not be a good trade-off. Note also that a specific choice for two-qubit gate is made, a CNOT gate. Any two-qubit gate can be decomposed into at most 3 CNOTs see e.g. here, so in terms of computational complexity this is of course inconsequential. However in the following discussion we will care a lot about constant factors and may wish to revisit this choice at the end.\n\n\n\nSince finding the exact optimal solution to the compilation problem is intractable, as with many things in life one needs to resort to heuristic methods. A combination of many heuristic methods, in fact. As an example one can check out the transpilation workflow in qiskit. Among others, there is a step that compiles &gt;2 qubit gates into one and two qubit gates; the one that tries to find a good initial placement of the logical qubits onto physical hardware; the one that ‘routes’ the desired circuit to match a given topology being as greedy on SWAPs as possible. Each of these steps can use several different heuristic optimization algorithms, which are continuously refined and extended (for example this recent preprint improves on the default rounting procedure in qiskit). In my opinion it would be waay better to have one unified heuristic for all steps of the process, especially taking into account that they are not completely independent. Although this might be too much to ask for, some advances are definitely possible and machine learning tools might prove very useful. The paper we are going to discuss is an excellent demonstration.\n\n\n\nThere is a couple of very nice theoretical results about the compilation problem that I need to mention. But first, let us agree that we will compile unitaries, not circuits. What is the difference? Of course, any quantum circuit (without measurements and neglecting losses) corresponds to a unitary matrix. However, to compute that unitary matrix for a large quantum circuit explicitly is generally an intractable problem, precisely for the same reasons that quantum computation is assumed to be more powerful than classical. Still, taking as the input a unitary matrix (which is in general hard to compute from the circuit) is very useful both theoretically and practically. I will discuss pros and cons of this approach later on.\nOK, now the fun fact. Generically, one needs at least this many CNOTs\n\\[\\begin{align}\nL:=\\# \\text{CNOTs} \\geq \\frac14\\left(4^n-3n-1\\right) \\label{TLB}\n\\end{align}\\]\nto exactly compile an \\(n\\)-qubit unitary. ‘Generically’ means that the set of \\(n\\)-qubit unitaries that can be compiled exactly with smaller amount of CNOTs has measure zero. Keep in mind though, that there are important unitaries in this class like multi-controlled gates or qubit permutations. We will discuss compilation of some gates from the ‘measure-zero’ later on.\nThe authors of the preprint (I hope you and me still remember that there is some actual results to discuss, not just my overly long introduction to read) refer to \\(\\eqref{TLB}\\) as the theoretical lower bound or TLB for short. The proof of this fact is actually rather simple and I will sketch it. A general \\(d\\times d\\) unitary has \\(d^2\\) real parameters. For \\(n\\) qubits \\(d=2^n\\). Single one-qubit gate has 3 real parameters. Any sequence of one-qubit gates applied to the same qubit can be reduced to a single one-qubit gate and hence can have no more than 3 parameters. That means, that without CNOTs we can only have 3n parameters in our circuit, 3 for each one-qubit gate. This is definitely not enough to describe an arbitrary unitary on \\(n\\) qubits which has \\(d^2=4^n\\) parameters.\nNow, adding a single CNOT allows to insert two more 1-qubit unitaries after it, like that\n\n\nCode\nfrom qiskit.circuit import Parameter\n\na1, a2, a3 = [Parameter(a) for a in ['a1', 'a2', 'a3']]\nb1, b2, b3 = [Parameter(b) for b in ['b1', 'b2', 'b3']]\n\nqc = QuantumCircuit(2)\nqc.cx(0, 1)\nqc.u(a1, a2, a3, 0)              \nqc.u(b1, b2, b3, 1)\n              \nqc.draw(output='mpl')\n\n\n\n\n\nAt the first glance this allows to add 6 more parameters. However, each single-qubit unitary can be represented via the Euler angles as a product of only \\(R_z\\) and \\(R_x\\) rotations either as \\(U=R_z R_x R_z\\) or \\(U=R_x R_y R_z\\) (I do not specify angles). Now, CNOT can be represented as \\(CNOT=|0\\rangle\\langle 0|\\otimes I+|1\\rangle\\langle 1|\\otimes X\\). It follows that \\(R_z\\) commutes with the control of CNOT and \\(R_x\\) commutes with the target of CNOT, hence they can be dragged to the left and joined with preceding one-qubit gates. So in fact each new CNOT gate allows to add only 4 real parameters:\n\n\nCode\na1, a2 = [Parameter(a) for a in ['a1', 'a2']]\nb1, b2 = [Parameter(b) for b in ['b1', 'b2']]\n\nqc = QuantumCircuit(2)\nqc.cx(0, 1)\nqc.rx(a1, 0)              \nqc.rz(a2, 0)\nqc.rz(b1, 1)\nqc.rx(b2, 1)\n              \nqc.draw(output='mpl')\n\n\n\n\n\nThat’s it, there are no more caveats. Thus, the total number of parameters we can get with \\(L\\) CNOTs is \\(3n+4L\\) and we need to describe a \\(d\\times d\\) unitary which has \\(4^n\\) parameters. In fact, the global phase of the unitary is irrelevant so we only need \\(3n+4L \\geq 4^n-1\\). Solving for \\(L\\) gives the TLB \\(\\eqref{TLB}\\). That’s pretty cool, isn’t it?\nNow there is an algorithm, called quantum Shannon decomposition (see ref), which gives an exact compilation of any unitary with the number of CNOTs twice as much as the TLB requires. In complexity-theoretic terms an overall factor of two is of course inessential, but for current NISQ devices we want to get as efficient as possible. Moreover, to my understanding the quantum Shannon decomposition is not easily extendable to restricted topology while inefficient generalizations lead to a much bigger overhead (roughly an order of magnitude)."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#what-is-compilation",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#what-is-compilation",
    "title": "Machine learning compilation of quantum circuits",
    "section": "",
    "text": "Note that a variety of terms are floating around the literature and used more or less interchangibly. Among those are synthesis, compilation, transpilation and decomposition of quantum circuits. I will not make a distinction and try to stick to compilation.\n\nBut first things first, what is a compilation of a quantum circuit? The best motivation and illustration for the problem is the following. Say you need to run a textbook quantum circuit on a real hardware. The real hardware usually allows only for a few basic one and two qubit gates. In contrast, your typical textbook quantum circuit may feature (1) complex many-qubit gates, for example multi-controlled gates and (2) one and two qubit gates which are not supported by the hardware. As a simple example take this 3-qubit Grover’s circuit (from qiskit textbook):\n\n\nCode\n#initialization\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# importing Qiskit\nfrom qiskit import IBMQ, Aer, assemble, transpile\nfrom qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\nfrom qiskit.providers.ibmq import least_busy\n\n# import basic plot tools\nfrom qiskit.visualization import plot_histogram\n\ndef initialize_s(qc, qubits):\n    \"\"\"Apply a H-gate to 'qubits' in qc\"\"\"\n    for q in qubits:\n        qc.h(q)\n    return qc\n\ndef diffuser(nqubits):\n    qc = QuantumCircuit(nqubits)\n    # Apply transformation |s&gt; -&gt; |00..0&gt; (H-gates)\n    for qubit in range(nqubits):\n        qc.h(qubit)\n    # Apply transformation |00..0&gt; -&gt; |11..1&gt; (X-gates)\n    for qubit in range(nqubits):\n        qc.x(qubit)\n    # Do multi-controlled-Z gate\n    qc.h(nqubits-1)\n    qc.mct(list(range(nqubits-1)), nqubits-1)  # multi-controlled-toffoli\n    qc.h(nqubits-1)\n    # Apply transformation |11..1&gt; -&gt; |00..0&gt;\n    for qubit in range(nqubits):\n        qc.x(qubit)\n    # Apply transformation |00..0&gt; -&gt; |s&gt;\n    for qubit in range(nqubits):\n        qc.h(qubit)\n    # We will return the diffuser as a gate\n    U_s = qc.to_gate()\n    U_s.name = \"U$_s$\"\n    return U_s\n\nqc = QuantumCircuit(3)\nqc.cz(0, 2)\nqc.cz(1, 2)\noracle_ex3 = qc.to_gate()\noracle_ex3.name = \"U$_\\omega$\"\n\nn = 3\ngrover_circuit = QuantumCircuit(n)\ngrover_circuit = initialize_s(grover_circuit, [0,1,2])\ngrover_circuit.append(oracle_ex3, [0,1,2])\ngrover_circuit.append(diffuser(n), [0,1,2])\ngrover_circuit = grover_circuit.decompose()\ngrover_circuit.draw(output='mpl')\n\n\n\n\n\nThe three qubit gates like Toffoli are not generally available on a hardware and one and two qubit gates my be different from those in the textbook algorithm. For example ion quantum computers are good with Mølmer–Sørensen gates and may need several native one qubit gates to implement the Hadamard gate.\nAdditional important problem is to take into account qubit connectivity. Usually textbook algorithms assume full connectivity, meaning that two-qubit gates can act on any pair of qubits. On most hardware platforms however a qubit can only interact with its neighbors. Assuming that one and two qubits gates available on the hardware can implement a SWAP gate between adjacent qubits, to solve the connectivity problem one can insert as many SWAPs as necessary to connect topologically disjoint qubits. Using SWAPs however leads to a huge overhead in the number of total gates in the compiled circuit, and it is of much importance use them as economically as possible. In fact, the problem of optimal SWAPping alone in generic situation is NP-complete."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#simplified-problem",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#simplified-problem",
    "title": "Machine learning compilation of quantum circuits",
    "section": "",
    "text": "When compiling a quantum circuit one has to decide which resulting circuits are considered to be efficient. Ideally, one should optimize for the total fidelity of the circuit. Let us imagine running the algorithm on a real device. Probably my theorist’s image of a real device is still way too platonic, but I will try my best. Many details need to be taken into account. For example, gates acting on different qubits or pairs of qubits may have different fidelities. Decoherence of qubits with time can make circuits where many operations can be executed in parallel more favorable. Cross-talk (unwanted interactions) between neighboring qubits may lead to exotic patterns for optimal circuits. A simple proxy for the resulting fidelity that is often adopted is the number of two-qubit gates (which are generically much less accurate than a single-qubit gates). So the problem that is often studied, and that is addressed in the preprint we are going to discuss, is the problem of optimal compilation into a gate set consisting of arbitrary single-qubit gates and CNOTs, the only two qubits gate. The compiled circuit must\n\nRespect hardware connectivity.\nHave as few CNOTs as possible.\nExceed a given fidelity threshold.\n\nLast item here means that we also allow for an approximate compilation. By increasing the number of CNOTs one can always achieve an exact compilation, but since in reality each additional CNOT comes with its own fidelity cost this might not be a good trade-off. Note also that a specific choice for two-qubit gate is made, a CNOT gate. Any two-qubit gate can be decomposed into at most 3 CNOTs see e.g. here, so in terms of computational complexity this is of course inconsequential. However in the following discussion we will care a lot about constant factors and may wish to revisit this choice at the end."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#existing-results",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#existing-results",
    "title": "Machine learning compilation of quantum circuits",
    "section": "",
    "text": "Since finding the exact optimal solution to the compilation problem is intractable, as with many things in life one needs to resort to heuristic methods. A combination of many heuristic methods, in fact. As an example one can check out the transpilation workflow in qiskit. Among others, there is a step that compiles &gt;2 qubit gates into one and two qubit gates; the one that tries to find a good initial placement of the logical qubits onto physical hardware; the one that ‘routes’ the desired circuit to match a given topology being as greedy on SWAPs as possible. Each of these steps can use several different heuristic optimization algorithms, which are continuously refined and extended (for example this recent preprint improves on the default rounting procedure in qiskit). In my opinion it would be waay better to have one unified heuristic for all steps of the process, especially taking into account that they are not completely independent. Although this might be too much to ask for, some advances are definitely possible and machine learning tools might prove very useful. The paper we are going to discuss is an excellent demonstration."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#theoretical-lower-bound-and-quantum-shannon-decomposition",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#theoretical-lower-bound-and-quantum-shannon-decomposition",
    "title": "Machine learning compilation of quantum circuits",
    "section": "",
    "text": "There is a couple of very nice theoretical results about the compilation problem that I need to mention. But first, let us agree that we will compile unitaries, not circuits. What is the difference? Of course, any quantum circuit (without measurements and neglecting losses) corresponds to a unitary matrix. However, to compute that unitary matrix for a large quantum circuit explicitly is generally an intractable problem, precisely for the same reasons that quantum computation is assumed to be more powerful than classical. Still, taking as the input a unitary matrix (which is in general hard to compute from the circuit) is very useful both theoretically and practically. I will discuss pros and cons of this approach later on.\nOK, now the fun fact. Generically, one needs at least this many CNOTs\n\\[\\begin{align}\nL:=\\# \\text{CNOTs} \\geq \\frac14\\left(4^n-3n-1\\right) \\label{TLB}\n\\end{align}\\]\nto exactly compile an \\(n\\)-qubit unitary. ‘Generically’ means that the set of \\(n\\)-qubit unitaries that can be compiled exactly with smaller amount of CNOTs has measure zero. Keep in mind though, that there are important unitaries in this class like multi-controlled gates or qubit permutations. We will discuss compilation of some gates from the ‘measure-zero’ later on.\nThe authors of the preprint (I hope you and me still remember that there is some actual results to discuss, not just my overly long introduction to read) refer to \\(\\eqref{TLB}\\) as the theoretical lower bound or TLB for short. The proof of this fact is actually rather simple and I will sketch it. A general \\(d\\times d\\) unitary has \\(d^2\\) real parameters. For \\(n\\) qubits \\(d=2^n\\). Single one-qubit gate has 3 real parameters. Any sequence of one-qubit gates applied to the same qubit can be reduced to a single one-qubit gate and hence can have no more than 3 parameters. That means, that without CNOTs we can only have 3n parameters in our circuit, 3 for each one-qubit gate. This is definitely not enough to describe an arbitrary unitary on \\(n\\) qubits which has \\(d^2=4^n\\) parameters.\nNow, adding a single CNOT allows to insert two more 1-qubit unitaries after it, like that\n\n\nCode\nfrom qiskit.circuit import Parameter\n\na1, a2, a3 = [Parameter(a) for a in ['a1', 'a2', 'a3']]\nb1, b2, b3 = [Parameter(b) for b in ['b1', 'b2', 'b3']]\n\nqc = QuantumCircuit(2)\nqc.cx(0, 1)\nqc.u(a1, a2, a3, 0)              \nqc.u(b1, b2, b3, 1)\n              \nqc.draw(output='mpl')\n\n\n\n\n\nAt the first glance this allows to add 6 more parameters. However, each single-qubit unitary can be represented via the Euler angles as a product of only \\(R_z\\) and \\(R_x\\) rotations either as \\(U=R_z R_x R_z\\) or \\(U=R_x R_y R_z\\) (I do not specify angles). Now, CNOT can be represented as \\(CNOT=|0\\rangle\\langle 0|\\otimes I+|1\\rangle\\langle 1|\\otimes X\\). It follows that \\(R_z\\) commutes with the control of CNOT and \\(R_x\\) commutes with the target of CNOT, hence they can be dragged to the left and joined with preceding one-qubit gates. So in fact each new CNOT gate allows to add only 4 real parameters:\n\n\nCode\na1, a2 = [Parameter(a) for a in ['a1', 'a2']]\nb1, b2 = [Parameter(b) for b in ['b1', 'b2']]\n\nqc = QuantumCircuit(2)\nqc.cx(0, 1)\nqc.rx(a1, 0)              \nqc.rz(a2, 0)\nqc.rz(b1, 1)\nqc.rx(b2, 1)\n              \nqc.draw(output='mpl')\n\n\n\n\n\nThat’s it, there are no more caveats. Thus, the total number of parameters we can get with \\(L\\) CNOTs is \\(3n+4L\\) and we need to describe a \\(d\\times d\\) unitary which has \\(4^n\\) parameters. In fact, the global phase of the unitary is irrelevant so we only need \\(3n+4L \\geq 4^n-1\\). Solving for \\(L\\) gives the TLB \\(\\eqref{TLB}\\). That’s pretty cool, isn’t it?\nNow there is an algorithm, called quantum Shannon decomposition (see ref), which gives an exact compilation of any unitary with the number of CNOTs twice as much as the TLB requires. In complexity-theoretic terms an overall factor of two is of course inessential, but for current NISQ devices we want to get as efficient as possible. Moreover, to my understanding the quantum Shannon decomposition is not easily extendable to restricted topology while inefficient generalizations lead to a much bigger overhead (roughly an order of magnitude)."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#templates",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#templates",
    "title": "Machine learning compilation of quantum circuits",
    "section": "Templates",
    "text": "Templates\nI’ve already wrote an introduction way longer than intended so from now on I will try to be brief and to the point. The authors of the preprint propose two templates inspired by the quantum Shannon decomposition. The building block for each template is a ‘CNOT unit’\n\n\nCode\na1, a2 = [Parameter(a) for a in ['a1', 'a2']]\nb1, b2 = [Parameter(b) for b in ['b1', 'b2']]\n\nqc = QuantumCircuit(2)\nqc.cx(0, 1)\nqc.ry(a1, 0)              \nqc.rz(a2, 0)\nqc.ry(b1, 1)\nqc.rx(b2, 1)\n              \nqc.draw(output='mpl')\n\n\n\n\n\nFirst template is called sequ in the paper and is obtained as follows. There are \\(n(n-1)/2\\) different CNOTs on \\(n\\)-qubit gates. We enumerate them somehow and simply stack sequentially. Here is a 3-qubut example with two layers (I use qiskit gates cz instead of our ‘CNOT units’ for the ease of graphical representation)\n\n\nCode\nqc = QuantumCircuit(3)\nfor _ in range(2):\n    qc.cz(0, 1)\n    qc.cz(0, 2)\n    qc.cz(1, 2)\n    qc.barrier()\nqc.draw(output='mpl')\n\n\n\n\n\nThe second template is called spin and for 4 qubits looks as follows\n\n\nCode\nqc = QuantumCircuit(4)\nfor _ in range(2):\n    qc.cz(0, 1)\n    qc.cz(1, 2)\n    qc.cz(2, 3)\n    qc.barrier()\nqc.draw(output='mpl')\n\n\n\n\n\nI’m sure you get the idea. That’s it! The templates fix the pattern of CNOTs while angles of single-qubit gates are adjustable parameters which are collectively denoted by \\(\\theta\\).\nThe idea now is simple. Try to optimize these parameters to achieve the highest possible fidelity for a given target unitary to compile. I am not at all an expert on the optimization methods, so I might miss many subtleties, but on the surface the problem looks rather straightforward. You can choose your favorite flavor of the gradient descent and hope for convergence. The problem appears to be non-convex but the gradient descent seems to work well in practice. One technical point that I do not fully understand is that the authors choose to work with fidelity defined by the Frobenius norm \\(||U-V||_F^2\\) which is sensitive to the global phase of each unitary. To my understanding they often find that local minima of this fidelity coincides with the global minimum up to a global phase. OK, so in the rest of the post I refer to the ‘gradient descent’ as the magic numerical method which does good job of finding physically sound minimums."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#results",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#results",
    "title": "Machine learning compilation of quantum circuits",
    "section": "Results",
    "text": "Results\n\nCompiling random unitaries\nOK, finally, for the surprising results. The authors find experimentally that both sequ and spin perform surprisingly well on random unitaries always coming very close to the TLB \\(\\eqref{TLB}\\) with good fidelity. More precisely, the tests proceed as follows. First, one generates a random unitary. Next, for each number \\(L\\) of CNOTs below the TLB one runs the gradient descent to see how much fidelity can be achieved with this amount of CNOTs. Finally, one plots the fidelity as a function of \\(L\\). Impressively, on the sample of hundred unitaries the fidelity always approaches 100% when the number of CNOTs reaches the TLB. For the \\(n=3\\) qubits TLB is \\(L=14\\), for \\(n=5\\) \\(L=252\\) (these are the two cases studied). So, in all cases studied, the gradient descent lead by the provided templates seems to always find the optimal compilation circuit! Recall that this is two times better than quantum Shannon decomposition. Please see the original paper for nice plots that I do not reproduce here.\n\n\nCompiling on restricted topology\nThese tests were performed on the fully connected circuits. The next remarkable discovery is that restricting the connectivity does not to seem to harm the performance of the compilation! More precisely, the authors considered two restricted topologies in the paper, ‘star’ where all qubits are connected to single central one and ‘line’ where well, they are connected by links on a line. The spin template can not be applied to star topology, but it can be applied to line topology. The sequ template can be generalized to any topology by simply omitting CNOTs that are not allowed. Again, as examining a hundred of random unitaries on \\(n=3\\) and \\(n=5\\) qubits shows, the fidelity nearing 100% can be achieved right at the TLB in all cases, which hints that topology restriction may not be a problem in this approach at all! To appreciate the achievement, imagine decomposing each unitary via the quantum Shannon decomposition and then routing on restricted topology with swarms of SWAPs, a terrifying picture indeed. It would be interesting to compare the results against the performance of qiskit transpiler which is unfortunately not done in the paper to my understanding.\n\n\nCompiling specific ‘measure zero’ gates\nSome important multi-qubit gates fall into the ‘measure zero’ set which can be compiled with a smaller amount of CNOTs than is implied by the TLB \\(\\eqref{TLB}\\). For example, 4-qubit Toffoli gate can be compiled with 14 CNOTs while the TLB requires 61 gates. Numerical tests show that the plain version of the algorithm presented above does not generically obtain the optimal compilation for special gates. However, with some tweaking and increasing the amount of attempts the authors were able to find optimal decompositions for a number of known gates such as 3- and 4-qubit Toffoli, 3-qubit Fredkin and 1-bit full adder on 4 qubits. The tweaking included randomly changing the orientation of some CNOTs (note that in both sequ and spin the control qubit is always at the top) and running many optimization cycles with random initial conditions. The best performing method appeared to be sequ with random flips of CNOTs. The whole strategy might look a bit fishy, but I would argue that it is not. My argument is simple: you only need to find a good compilation of the 4-qubit Toffoli once. After that you pat yourself on the back and use the result in all your algorithms. So it does not really matter how hard it was to find the compilation as long as you did not forget to write it down.\n\n\nCompressing the quantum Shannon decomposition\nFinally, as a new twist on the plot the authors propose a method to compress the standard quantum Shannon decomposition (which is twice the TLB, remember?). The idea seems simple and works surprisingly well. The algorithm works as follows. 1. Compile a unitary exactly using the quantum Shannon decomposition. 1. Promote parameters in single-qubit gates variables (they have fixed values in quantum Shannon decomposition). 2. Add [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)-type regularization term, which forces one-qubit gates to have small parameters, ideally zero (which makes the corresponding gates into identities). 3. Run a gradient descent on the regularized cost function (fidelity+LASSO term). Some one-qubit gates will become identity after that (one might need to tune the regularization parameter here). 4. After eliminating identity one-qubit gates one can end up in the situation where there is a bunch of CNOTs with no single-qubit gates in between. There are efficient algorithms for reducing the amount of CNOTs in this case.\n5. Recall that the fidelity was compromised by adding regularization terms. Run the gradient descent once more, this time without regularization, to squeeze out these last percents of fidelity.\nFrom the description of this algorithm it does not appear obvious that the required cancellations (elimination of single-qubit gates and cancellations in resulting CNOT clusters) is bound to happen, but the experimental tests show that they do. Again, from a bunch of random unitaries it seems that the \\(\\times 2\\) reduction to the TLB is almost sure to happen! Please see the preprint for plots."
  },
  {
    "objectID": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#weak-spots",
    "href": "posts/machine_learning_compilation_of_qunatum_circuits/2021-07-22-machine learning compilation of quantum circuits.html#weak-spots",
    "title": "Machine learning compilation of quantum circuits",
    "section": "Weak spots",
    "text": "Weak spots\nAlthough I find results of the paper largely impressive, a couple of weak spots deserve a mention. ### Limited scope of experiments The numerical experiments were only carried out for \\(n=3\\) and \\(n=5\\) qubits which of course is not much. To see if the method keeps working as the number of qubits is scaled is sure very important. There may be two promblems. First, the templates can fail to be expressive enough for larger circuits. The authors hope to attack this problem from the theoretical side and show that the templates do fill the space of unitaries. Well, best of luck with that! Another potential problem is that although the templates work fine for higher \\(n\\), the learning part might become way more challenging. Well, I guess we should wait and see. ### Unitary as the input As I discussed somewhere way above, for a realistic quantum computation we can not know the unitary matrix that we need to compile. If we did, there would no need in the quantum computer in the first place. I can make two objects here. First, we are still in the NISQ era and pushing the existing quantum computers to their edge is a very important task. Even if an algorithm can be simulated classically, running it on a real device might be invaluable. Second, even quantum circuits on 1000 qubits do not usually feature 100-qubit unitaries. So it could be possible to separate a realistic quantum circuit into pieces, each containing only a few qubits, and compile them separately."
  },
  {
    "objectID": "posts/some_analytic_facts_about_vqa/2022-05-17-some analytic facts about variational algorithms.html",
    "href": "posts/some_analytic_facts_about_vqa/2022-05-17-some analytic facts about variational algorithms.html",
    "title": "Some analytic facts about variational quantum algorithms",
    "section": "",
    "text": "Code\nfrom qiskit import QuantumCircuit\nfrom qiskit.circuit import Parameter\nfrom math import pi\n\n\n\nIntroduction\nVariational quantum algorithms (VQA) is a huge field by now with many prospective applications and a poll of advocates for their potential quantum advantage on the NISQ devices, see e.g. here for a recent review. I usually think of variational quantum algorithms as analogues of the neural networks with a parametrized quantum circuit playing the role of a trainable model. Here is an example of a parametrized quantum circuit:\n\nDespite certain similarities there are also crucial distinctions between the classical and quantum nets, manifesting both in their functional shapes and trainability properties. Here I will mostly talk about some functional properties of parametrized quantum circuits, and briefly touch on the trainability issues at the end. A whole slew of additional peculiarities arise when you want to run quantum circuits on real quantum hardware and have to deal with errors and stochastic nature of measurements. Here I will ignore these issues completely, effectively assuming that we can run a classical simulation of the quantum circuit.\n\n\nHi \\(\\cos\\), hi \\(\\sin\\)!\nFor a generic neural net the loss function is a highly non-linear function of each weight, due to non-linear activation functions that connect the layers. In contrast, dependence of the parametrized quantum circuits on each single parameter separately is extremely simple.\nI will denote the unitary matrix of a parametrized quantum circuit by \\(U(\\theta)\\), with \\(\\theta\\) standing for all the parameters collectively. A simple observation, and really the basis for all of the following discussion, is the following equation \\[\\begin{align}\nU(\\theta_i)=U_0 \\cos \\frac{\\theta_i}{2}+U_1\\sin\\frac{\\theta_i}{2} \\label{u cos sin} \\ .\n\\end{align}\\] Here \\(\\theta_i\\) stands for any parameter of the circuit, other are assumed to be fixed. Matrix coefficients \\(U_0\\) and \\(U_1\\) are given by \\(U_0=U(0)\\) and \\(U_1 = U(\\pi)\\). Relation \\(\\eqref{u cos sin}\\) follows from the simple fact that all gates typically considered in VQA are of the form \\[\\begin{align*}\nG(\\theta) = e^{-i\\theta \\Sigma /2}\n\\end{align*}\\] with a generator \\(\\Sigma\\) that satisfies \\(\\Sigma^2=1\\). Hence, by a generalization of Euler’s formula \\(e^{i\\phi}=\\cos \\phi+i\\sin\\phi\\) any gate can be alternatively written as \\[\\begin{align*}\nG(\\theta) = \\cos \\frac{\\theta}{2}-i\\Sigma\\sin\\frac{\\theta}{2} \\ .\n\\end{align*}\\] As an example one take any single-qubit Pauli rotation ref, say \\(R_X(\\theta)=e^{-i \\theta X/2}=\\cos \\frac{\\theta}{2}-iX\\sin\\frac{\\theta}{2}\\). Parametric two-qubit gates, e.g. \\(R_{ZX}\\) gate, usually conform to the same rule. Because of the trigonometric function here the parameters in VQA are often referred to as angles, the terminology which I will follow.\n\n\nA typical loss function\nA subset of the VQA are variational quantum eigensolvers (VQE). A typical loss function in VQE is quadratic in \\(U(\\theta)\\). For instance, one common goal in VQE is to prepare the ground state of some Hamiltonian \\(H\\) using an ansatz \\(|\\psi(\\theta)\\rangle = U(\\theta)|0\\rangle\\). The relevant cost function to be minimized is\n\\[\\begin{align}\nL(\\theta)=\\langle\\psi(\\theta)|H|\\psi(\\theta)\\rangle \\label{loss VQE} \\ .\n\\end{align}\\]\nIn the unitary synthesis problem, that I’ve been recently interested in, the goal is to make the circuit \\(U(\\theta)\\) equivalent to some target unitary \\(V\\). The relevant loss can be defined as\n\\[\\begin{align*}\nL(\\theta)=-|\\operatorname{Tr} V^\\dagger U(\\theta)|^2\n\\end{align*}\\]\nYou got the idea. Note that while the circuit \\(U(\\theta)\\) has only two terms \\(\\eqref{u cos sin}\\) as a function of any angle \\(\\theta_i\\), the quadratic loss function will have three terms (note also the period doubling)\n\\[\\begin{align*}\nL(\\theta_i)=A \\cos\\theta_i+B \\sin \\theta_i + C \\ .\n\\end{align*}\\]\nHere \\(A, B, C\\) are functions of all the other angles except for \\(\\theta_i\\). They are the only unknowns that specify dependence on any particular angle and can be found with just three evaluations of the loss function, e.g.\n\\[\\begin{align*}\nA+B = L(0),\\qquad A+C=L(\\pi/2),\\qquad A-C = L(-\\pi/2) \\Rightarrow\\\\\nA = \\frac{L(\\pi/2)+L(-\\pi/2)}{2},\\qquad C=\\frac{L(\\pi/2)-L(-\\pi/2)}{2},\\qquad B = L(0)-A \\ .\n\\end{align*}\\]\n\n\nParameter shift rule\nPerhaps the best known consequence of this property is the parameter shift rule for derivatives. First, a bit of a background. Let’s assume we need to estimate the derivative of some function \\(f(x)\\) that we only have numerical access to. Then, there is nothing much better one can do than to use the finite difference approximations. For example, using two function evaluations it is possible to compute the first derivative up to the second approximation order\n\\[\\begin{align*}\nf'(x)=\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}+O(\\epsilon^2)\n\\end{align*}\\] In general, adding one more evaluation point allows to improve the accuracy by one order. However, when you have additional knowledge about the function much more efficient strategy may exists. In particular, for VQE loss functions \\(\\eqref{loss VQE}\\), which are basically simple sinusoidals, an exact derivative computation is possible with just two function evaluations\n\\[\\begin{align*}\nL'(\\theta_i)=-A \\sin(\\theta_i)+B\\cos{\\theta_i}=\\frac{L(\\theta_i+\\pi/2)-L(\\theta_i-\\pi/2)}{2} \\ ,\n\\end{align*}\\] which follows from \\(\\sin(x+\\pi/2)=\\cos x,\\,\\,\\cos(x+\\pi/2)=-\\sin x\\). Having access to exact derivatives generally enhances the performance of the gradient-based optimizers.\n\n\nSequential optimization\nAn interesting extension of this idea, that is apparently much less known than the parameter-shift rule itself, was proposed by several group at roughly the same time ([1], [2], [3], thanks to Vijendran for additional refs). Instead of using structural properties of \\(\\eqref{loss VQE}\\) to just compute derivatives, one can find the exact minimum of \\(L(\\theta_i)\\) with respect to any angle \\(\\theta_i\\) (assuming other angles are fixed). I.e. instead of a partial derivative one can compute “the partial minimum”.\nIndeed, since just three three evaluations fix \\(L(\\theta_i)\\) completely, and the function itself is rather simple, there is no problem finding \\(\\operatorname{argmin}_{\\theta_i}L(\\theta_i)\\) exactly. The explicit formula could be more transparent, but it is a simple trigonometry in the end (double check if you a going to use it!)\n\\[\\begin{align*}\n\\theta^*=\\operatorname{argmin}_{\\theta}\\left(A \\cos\\theta+B\\sin\\theta+C\\right)=\\cases{\\arctan \\frac{B}{A}+\\pi,&A&gt;0\\\\\\arctan \\frac{B}{A},& A&lt;0}\n\\end{align*}\\]\nWith this trick one can bypass gradient-based optimization as follows. Starting from \\(L(\\theta_1,\\theta_2,\\dots)\\) first optimize with respect to the first angle \\(L(\\theta_1,\\theta_2,\\dots)\\to L(\\theta_1^*,\\theta_2,\\dots)\\). Then optimize with respect to the second \\(L(\\theta_1^*,\\theta_2,\\dots)\\to L(\\theta_1^*,\\theta_2^*,\\dots)\\). Note that after this step the first angle in general no longer is the best choice, because the second angle has changed. Still, one can continue this procedure further until all angles are updated and then start anew. Each step is guaranteed to decrease the value of the loss function. If the loss landscape is nice overall, this sequential gradient-free optimization may in fact even outperform gradient-based methods. Unfortunately, to my understanding the sequential optimization is unlikely to help with the most crucial problems in the VQE loss landscapes: barren plateaus and local minimums.\n\n\nAverage performance of the VQE\nHere comes the original contribution of this blog post, I will show how to compute (semi-efficiently) the average loss \\[\\begin{align}\n\\overline{L}=\\frac{1}{(2\\pi)^p}\\int \\prod_{i=1}^p d\\theta_i \\langle \\psi(\\theta)|H|\\psi(\\theta)\\rangle \\ .\n\\end{align}\\] where \\(p\\) is the total number of angles in the parametrized circuit. Why would one be interested in such a quantity? Honestly, I do not know, but hey, this is a blog post and not a paper, so I’ll take a recreational attitude. Seriously though, I’ll use this result in the following section, which however is not well justified either:)\nTo begin with, let’s make explicit dependence of the parametrized circuit on all of its angles \\[\\begin{align}\nU(\\theta)=\\sum_{I} U_{I}\\left(\\cos\\frac{\\theta}{2}\\right)^{1-I}\\left(\\sin\\frac{\\theta}{2}\\right)^I \\label{u exp} \\ .\n\\end{align}\\] Here \\(I\\) is a multi-index, a binary string of length \\(p\\), and \\(\\left(\\cos\\frac{\\theta}{2}\\right)^{I}\\) is an abbreviation for \\(\\prod_{i=1}^p\\left(\\cos \\frac{\\theta_i}{2}\\right)^{I_i}\\). For \\(p=1\\) this reduces to \\(\\eqref{u cos sin}\\). For \\(p=2\\) we have \\[\\begin{align*}\nU(\\theta)=U_{00}\\cos\\frac{\\theta_1}{2}\\cos\\frac{\\theta_2}{2}+U_{01}\\cos\\frac{\\theta_1}{2}\\sin\\frac{\\theta_2}{2}+U_{10}\\sin\\frac{\\theta_1}{2}\\cos\\frac{\\theta_2}{2}+U_{11}\\sin\\frac{\\theta_1}{2}\\sin\\frac{\\theta_2}{2} \\ ,\n\\end{align*}\\] I think you got the idea. There are exactly \\(2^p\\) terms in this sum. Now let us substitute this expression into the loss function \\(\\eqref{loss VQE}\\)\n\\[\\begin{align}\nL(\\theta)=\\sum_{I,J}\\left(\\cos\\frac{\\theta}{2}\\right)^{1-I}\\left(\\sin\\frac{\\theta}{2}\\right)^I\\left(\\cos\\frac{\\theta}{2}\\right)^{1-J}\\left(\\sin\\frac{\\theta}{2}\\right)^J\\langle 0|U_I^\\dagger  H U_J |0\\rangle \\label{loss exp} \\ .\n\\end{align}\\]\nWhen we average, all terms with \\(I\\neq J\\) vanish since \\(\\int_0^{2\\pi} d\\theta \\sin\\frac{\\theta}{2}\\cos\\frac{\\theta}{2}=0\\). At the same time, all terms with \\(I=J\\) give equal angle integrals \\(\\frac{1}{(2\\pi)^p}\\int \\prod_{i=1}^p d\\theta_i \\left(\\cos\\frac{\\theta_i}{2}\\right)^{2 I}\\left(\\sin\\frac{\\theta_i}{2}\\right)^{2-2 I}=\\frac{1}{2^p}\\) since \\(\\int d\\theta \\cos^2\\frac{\\theta}{2}=\\int d\\theta \\sin^2\\frac{\\theta}{2}=\\pi\\). The results is that \\[\\begin{align}\n\\overline{L}=\\frac1{2^p}\\sum_{I}\\langle 0|U_I^\\dagger H U_I |0\\rangle \\label{L average} \\ .\n\\end{align}\\] This expression looks simple, but it is a sum with \\(2^p\\) terms, so for any reasonable number of parameters its huuuge. A typical number of parameters is exponential in the number of qubits, so this is the double exponential, not good. I’ve spent multiple hours thinking about how to compute this average more efficiently, but for generic function of the type \\(\\eqref{u exp}\\) with arbitrary matrix coefficients \\(U_I\\) I didn’t find a way to compute the average loss in less than an exponential in \\(p\\) number of function calls. However, taking into account that \\(U_I\\) are not arbitrary for parametrized quantum circuits, a computation linear in \\(p\\) is possible. The reason is that among exponentially many \\(U_I\\) there is only polynomially many “independent ones”, in a sense that I will now make precise.\nFor concreteness consider the following toy circuit:\n\n\nCode\nqc = QuantumCircuit(2)\nqc.cz(0,1)\nqc.rx(Parameter('$ \\\\theta_1 $'), 0)\nqc.rx(Parameter('$ \\\\theta_2 $'), 1)\nqc.rz(Parameter('$ \\\\theta_3 $'), 0)\nqc.rz(Parameter('$ \\\\theta_4 $'), 1)\nqc.draw(output='mpl')\n\n\n\n\n\nHere the entangling gate is the Controlled-Z. This circuit has four parameters and \\(2^4=16\\) associated matrix coefficients \\(U_I\\). What are they, exactly? It is in fact rather simple to understand. If the binary index is \\(0\\) the rotation gate is replaced by the identity, if it is \\(1\\) we insert the generator instead. For example \\(U_{1110}\\) is\n\n\nCode\nqc = QuantumCircuit(2)\nqc.cz(0,1)\nqc.x(0)\nqc.x(1)\nqc.z(0)\nqc.global_phase=pi\nqc.draw(output='mpl')\n\n\n\n\n\nThe global phase arises because \\(U(\\theta=\\pi)=-i \\Sigma\\) for \\(U(\\theta)=e^{-i\\theta\\Sigma/2}\\). Next, consider a more realistic circuit like the one below\n\nAll coefficients \\(U_I\\) arise as \\(2^p\\) different versions of this circuit where each rotation gate is replaced either by an identity or by a generator, just as at the figure above. Although these circuits might all look different, in fact there is just a handful of independent ones. This is due to the following commutation rules, which are easy to check:\n\nThese commutation rules allow to move all the pauli matrices past CZ gates and to the beginning of the circuit. For example, the circuit \\(U_{1101}\\) from above can be alternatively be rewritten as\n\n\nCode\nqc = QuantumCircuit(2)\nqc.x(0)\nqc.z(1)\nqc.x(1)\nqc.cz(0,1)\nqc.global_phase=pi\nqc.draw(output='mpl')\n\n\n\n\n\nThen, any string of Pauli matrices is equal to \\(I, X, Z\\) or \\(Y\\simeq XZ\\) up to a global phase. So in the end, up to phase factors there are only \\(4^n\\) linearly independent matrices \\(U_I\\) where \\(n\\) is the number of qubits. The counting \\(4^n\\) follows because after all the generators have been placed at the beginning of the circuit there can be only 4 different operators at each qubit thread. Note also that \\(4^n\\) is precisely the dimension of the unitary group on \\(n\\) qubits. We thus see that all of \\(2^p\\) matrix coefficients can be divided into \\(4^n\\) distinct classes and within each class \\(U_I=e^{i\\phi}U_{I'}\\). This global phase makes no difference for the averages in \\(\\eqref{L average}\\) which can therefore be rewritten as\n\\[\\begin{align*}\n\\overline{L}=\\frac{1}{4^n}\\sum_{c} \\langle 0|U_c^\\dagger H U_c|0\\rangle \\ ,\n\\end{align*}\\] where now the sum is over representatives of distinct classes. So the sum is reduced from \\(2^p\\) to \\(4^n\\) terms. OK, so how does the number of parameters and the number of qubits compare? Is this really a reduction?\nYes it is! First, if you want your parametrized quantum circuit to cover any unitary transformation on \\(n\\) qubits you need at least \\(p=4^n\\) parameters, because this is the dimension of the unitary group. So in this case we have and exponential reduction from \\(2^{4^n}\\) to \\(4^n\\). But even if you only put two rotation gates on each qubit you already got yourself \\(4^n\\) parameters. Adding anything beyond that, as you definitely wish to do, makes the reduction from \\(2^p\\) to \\(4^n\\) essential. Note though that it is still exponential in the number of qubits and would be unfeasible to compute exactly for a large system.\n\n\nLoss landscape as charge density\nOK, here is a brief justification for why I was interested in the average loss in the first place. Generic hamiltonian-agnostic VQE algorithms have in fact lots of trainability issues. One is the presence of the barren plateaus in certain regimes, which means that large portions of the parameter space have vanishing gradients and are bad places for optimizer to be in. Another issue is the presence of local minimums which can be just as bad. So I was wondering if it is possible to somehow use the analytic properties of the VQE loss functions to help mitigate these problems. Here is an idea that probably does not work, but I think still is sort of fun.\nHere is an example of a bad loss landscape, sketched in black:\n\nIt has many local minimums and flat parts, and only a single narrow global minimum. If we are only allowed to probe this loss landscape one value at a time we will have really hard time reaching the global minimum. However, if we have additional information we might be able to do better.\nLet’s assume that we know \\(\\Delta^{-1} L(\\theta)\\) where \\(\\Delta\\) is the Laplace operator. Physics interpretation is the following. If we view the loss landscape as the charge density \\(\\Delta^{-1}L(\\theta)\\) is the corresponding electric potential, sketched in red. Minimizing the electric potential instead of the charge density might be a much nicer problem because the electric field (the gradient of the potential field) typically stretches far away from localized charges and can attract the probe. Extreme example is the charge density of the point particle, which is impossible to find unless you trip over it. However, if you can probe the electric field of this charge you have an easy way discovering where it comes from. Sounds good, right? Well, not so fast. First, we do not know \\(\\Delta^{-1}L(\\theta)\\) for a typical VQE loss. Second, my examples were specifically crafted to sell the idea. It is easy to imagine a loss landscape where this does not help.\nBut we are not boring nitpickers, are we? Of course not, we are imaginative and brave, so we are going to assume even more. Let’s pretend that each successive application of \\(\\Delta^{-1}\\) makes our loss landscape better, so we are really interested in \\[\\begin{align*}\n\\mathcal{L}(\\theta)=\\Delta^{-\\infty} L(\\theta) \\ .\n\\end{align*}\\] Turns out this limiting landscape is very simple and can be found in a similar way to the average considered in the previous section. Indeed, the loss function \\(\\eqref{loss exp}\\) can be represented in the following form \\[\\begin{align*}\nL(\\theta)=const+A_1\\cos(\\theta_1)+B_1\\sin(\\theta_1)+A_2\\cos(\\theta_2)+B_2\\sin(\\theta_2)+\\\\A_{12}\\cos(\\theta_1)\\cos(\\theta_2)+B_{12}\\cos(\\theta_1)\\sin(\\theta_2)+\\dots\n\\end{align*}\\] This is an example with two parameters and several cross-terms are omitted. Here is the key point – each cross-term gets smaller under application of \\(\\Delta^{-1}\\), e.g. \\(\\Delta^{-1} \\cos\\theta_1=\\cos\\theta_1\\), \\(\\Delta^{-1} \\cos\\theta_1\\cos\\theta_2=\\frac12 \\cos\\theta_1\\cos\\theta_2\\) etc. This means, that under the application of \\(\\Delta^{-\\infty}\\) only the single-variable terms will survive (we ignore the constant term) \\[\\begin{multline*}\n\\mathcal{L}(\\theta)=\\Delta^{-\\infty}L(\\theta)=\\sum_{i=1}^{p}\\left(A_i\\cos\\theta_i+B_i\\sin\\theta_i\\right)\\substack{p=2\\\\=}\\\\\\,\\,A_1\\cos(\\theta_1)+B_1\\sin(\\theta_1)+A_2\\cos(\\theta_2)+B_2\\sin(\\theta_2)\n\\end{multline*}\\]\nThese terms are easy to compute if we can compute averages. For example, averaging over all \\(\\theta_i\\) except theta \\(\\theta_1\\) will only leave the monomials with \\(\\theta_1\\) \\[\\begin{align*}\nA_1\\cos{\\theta_1}+B_1\\sin{\\theta_1}=\\frac{1}{(2\\pi)^{p-1}}\\int_0^{2\\pi} \\prod_{i=2}^pd\\theta_i\\,\\, L(\\theta) \\ .\n\\end{align*}\\] How to perform the average on the rhs was shown in the previous section. Since the limiting loss function \\(\\mathcal{L}(\\theta)\\) is a sum of single-variable terms it is extremely simple to optimize, and the minumimum is unique\n\\[\\begin{align*}\n\\theta^* = (\\theta_1^*, \\theta_2^*,\\dots) = \\operatorname{argmin}_\\theta \\mathcal{L}(\\theta) \\ .\n\\end{align*}\\] The derivation above also gives a different, less exotic, interpretation of \\(\\theta^*\\). Each angle \\(\\theta_i^*\\) is the angle that minimizes the average loss function, where average is taken with respect to all other angles. Knowing how to find parameter value that optimizes an average performance does not seem like a completely useless information, does it? Could it help to alleviate the problems with barren plateaus or local minimums? I do not know, but I’m planning on making some numerical experiments along these lines.\n\n\nBeyond simple trigonometry\nI was really impressed with a recent paper Beyond Barren Plateaus: Quantum Variational Algorithms Are Swamped With Traps. Adopting some techniques from the study of neural networks the authors provide a random matrix theory description of a generic loss landscape for Hamiltonian-agnostic VQE. They show that for underparametrized circuits (when the number of parameters \\(p\\) is smaller than the dimension of the unitary group \\(4^n\\), which is basically the only reasonable setup) the loss landscape is really bad, with exponentially many local minimums being located far away (energy-wise) from the global minimum. They even provide an analytic distribution for the expected number of local minimums, which seems to match my numerical experiments rather well: \nThis is yet another piece of evidence showing that complexity of the classical optimization loop in variational algorithms can not be ignored, as if the problems posed by getting a real quantum device to work were not enough. One possible way to alleviate the issues is to use specifically designed parametrized circuits, which are aware of the symmetries or additional properties of the Hamiltonian. I have a feeling though, that even performance of a generic Hamiltonian-agnostic VQE could be improved by exploiting some structural properties of the loss landscapes. In this blog post I speculated about what could such an approach look like.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html",
    "href": "posts/quantum_singular_value_transformation_intro/index.html",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "",
    "text": "Code\n# Uncomment when in Colab\n\n#try: \n#  import pyqsp\n#except ImportError:\n#  !pip install pyqsp==0.1.6\n\nimport numpy as np\nimport scipy.stats\nimport scipy.linalg\n\nimport matplotlib.pyplot as plt\n\nfrom functools import reduce\n  \nimport pyqsp\nimport pyqsp.angle_sequence\nimport pyqsp.response\n\nfrom pyqsp.angle_sequence import QuantumSignalProcessingPhases\n\nimport contextlib\nimport io\n\ndef quiet(func):\n    def quiet_func(*args, **kwargs):    \n        f = io.StringIO()\n        with contextlib.redirect_stdout(f):\n            res = func(*args, **kwargs)\n        return res\n    return quiet_func\n\nplt.rcParams[\"figure.figsize\"] = (6, 4)"
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#a-high-level-overview",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#a-high-level-overview",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "1.1 A high-level overview",
    "text": "1.1 A high-level overview\nSo, how does QSVT work? Constructing a quantum algorithm using QSVT consists of the following main steps. Don’t worry if this does not immediately make sense, the goal of this post is to elaborate and give examples.\n\n\n\n\n\n\nQSVT in a nutshell\n\n\n\n\n\nExpress your problem of interest as applying a function \\(f(x)\\) to (the singular values of) a matrix \\(A\\to f(A)\\).\nFind a unitary circuit \\(U\\) that block encodes your matrix, i.e. in a suitable basis \\[U=\\begin{pmatrix}A&*\\\\ *&*\\end{pmatrix} \\ .\\]\nFind a good enough polynomial approximation to the target function \\(p(x)\\approx f(x)\\).\nUsing quantum signal processing construct a circuit \\(U_\\phi\\), that block encodes \\(p(A)\\), i.e. \\[U_\\phi=\\begin{pmatrix}p(A)&*\\\\ *&*\\end{pmatrix} \\ .\\]\n\n\n\n\nLet’s begin unpacking this by clarifying the first step, i.e. how to frame your quantum problem as a singular value transformation. Here are three examples.\n\nGrover’s search. There is some marked state \\(|m\\rangle\\) of \\(n\\) qubits that we need to find. In this case, we can take \\(A\\) to be \\(1\\times 1\\) matrix containing a single matrix element \\(a=\\langle m|H^{\\otimes n}|0\\rangle\\). If we can apply the sign-function to it \\(x\\to \\operatorname{sign}(x)\\), we will map an arbitrary amplitude \\(a&gt;0\\) to 1, and hence build the circuit that is guaranteed to take the initial state \\(|0\\rangle\\) to the marked state \\(|m\\rangle\\).\nQuantum simulation. Here given a Hamiltonian \\(H\\) we seek to construct a unitary \\(e^{-iHt}\\). Quite literally, this is the problem of applying \\(f(x)=e^{-ixt}\\) to the eigenvalues of \\(H\\). Up to technicalities, eigenvalues can be transformed in the same way as singular values.\nSolving linear equations. Given a matrix \\(A\\) and a vector \\(b\\) solve \\(Ax=b\\) for \\(x\\), i.e. compute \\(A^+b\\), where \\(A^+\\) is the Moore-Penrose pseudo-inverse of \\(A\\). But pseudo-inverse \\(A^+\\) is just equal to the usual hermitian conjugate \\(A^\\dagger\\) with singular values inverted, so here \\(f(x)\\propto \\frac1x\\).\n\nMany other applications of QSVT exist, but in this blog post I will focus on these three. Hopefully step (1) is starting to make sense now. How do we perform step (2) then, how to find the block encoding? To my understanding, this is typically the most challenging part, and has to be addressed case-by-case. I will give some intuition and examples of block encodings below in Sec. 4. Next, QSVT allows performing only polynomial transformations, but most problems of practical interest require applying non-polynomial functions. This is why we need step (3), which is finding the right polynomial approximation. Importantly, the degree of the approximating polynomial determines the complexity of the final quantum circuit, so this is an essential step as well. Finally, after you decided on the best approximating polynomial, you can implement it using techniques from the quantum signal processing, which is step (4). This should be straightforward in theory, while in practice there still may be issues with speed and numerical stability for large instances."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#what-i-will-and-will-not-discuss",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#what-i-will-and-will-not-discuss",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "1.2 What I will and will not discuss",
    "text": "1.2 What I will and will not discuss\nHere are some of the topics that will be covered.\n\nBasic quantum signal processing.\nBasic concepts behind the singular value decomposition.\nBasic concepts and examples of block encodings, including the linear combination of unitaries.\nSome intuition behind approximating polynomials and expected scaling.\nHow to construct QSVT circuits and why do they work.\nMany code samples, including explicit implementations of several quantum algorithms.\nExamples of how to use pyqsp  [6] package for signal processing.\n\nHere are some of the important omissions.\n\nHow to find QSP angles from a polynomial.\nHow exactly to construct an approximating polynomial.\nHow to do block encoding in general.\nOnly consider a limited number of examples.\nI will not implement algorithms as quantum circuits, but merely as matrix multiplications. This allows to cut some corners, but is less pedagogic.\n\nIn the end, this turned out to be a behemoth-sized post. Although I tried to break it up into small digestible pieces, do not expect a light read. Also, I tried my best to be pedagogical but also precise in technical detail. I welcome feedback on the presentation and suggestions on possible errors.\nAlright, with all disclaimers out of the way, we are ready to begin!"
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#ingredients",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#ingredients",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "2.1 Ingredients",
    "text": "2.1 Ingredients\nSo, suppose you have a two-level system and can perform a simple diagonal transformation on it \\[\\begin{align}\nS(\\phi)=\\begin{pmatrix}e^{i\\phi}&0 \\\\ 0& e^{-i\\phi}\\end{pmatrix} \\ . \\label{S}\n\\end{align}\\] Here \\(\\phi\\) is an angle you can vary. You can also perform a fixed non-diagonal operation \\[\\begin{align}\nR(a)=\\begin{pmatrix}a&\\sqrt{1-a^2}\\\\\\sqrt{1-a^2} & -a\\end{pmatrix} \\ . \\label{R}\n\\end{align}\\] Here, \\(a\\in \\mathbb{R}\\) is fixed. Note that \\(R(a=1)\\) is a reflection, hence the variable name."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#circuit",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#circuit",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "2.2 Circuit",
    "text": "2.2 Circuit\nThe most general quantum circuit you can build from these two ingredients is the following alternating sequence\n\n\n\n\n\n\nQSP circuit\n\n\n\n\n\\[\\begin{align}\nU_\\phi =S(\\phi_0)\\prod_{i=1}^d R(a) S(\\phi_i) = S(\\phi_0)R(a) S(\\phi_1) R(a)\\dots S(\\phi_{d-1}) R(a) S(\\phi_d) \\label{QSP} \\ .\n\\end{align}\\]\n\n\n\nNow, if you actually compute \\(U_\\phi\\), you’ll find it is of the following form (up to a global phase) \\[U_\\phi= \\begin{pmatrix} P(a) & i Q(a)\\sqrt{1-a^2} \\\\ i Q(a)^*\\sqrt{1-a^2} & P(a)^*\\end{pmatrix} \\ . \\] The key fact here is that both \\(P(a)\\) and \\(Q(a)\\) are polynomials.\n\n\n\n\n\n\nW and R signal conventions\n\n\n\n\n\nOne can choose different signal- and signal-processing operators. Our choice is the \\(R\\)-convention. It will be most convenient for generalizing to QSVT. Another standard one is \\(W\\)-convention, where the signal operator is \\[W(a)=\\begin{pmatrix}a & i\\sqrt{1-a^2}\\\\i\\sqrt{1-a^2} & a\\end{pmatrix} \\ .\\] The two signal operators are related by \\(R(a)=-i S(\\pi/4) W(a) S(\\pi/4)\\). For \\(W\\)-signal operator relation \\(\\eqref{QSP}\\) is exact, while for \\(R(a)\\) there is an additional global phase \\(i^d\\). If we have an angle sequence \\(\\phi_i^W\\) implementing the desired polynomial transformation with \\(W\\)-signal, the angles for \\(R\\)-signal are \\[\\phi_0^R=\\phi_0^W+(2d-1)\\frac{\\pi}{4}, \\quad \\phi_d^R=\\phi_d^W-\\frac{\\pi}{4}, \\quad \\phi_i^R=\\phi_i^W-\\frac{\\pi}{2} (0 &lt; i&lt; d)\\,\\, \\ .\\]"
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#polynomials-from-qsp",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#polynomials-from-qsp",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "2.3 Polynomials from QSP",
    "text": "2.3 Polynomials from QSP\nWhy are \\(P(a)\\) and \\(Q(a)\\) polynomials? Well, you can check this directly/prove by induction. For instance, \\[\\begin{pmatrix} P(a) & i Q(a)\\sqrt{1-a^2} \\\\ i Q(a)^*\\sqrt{1-a^2} & P(a)^*\\end{pmatrix} \\begin{pmatrix} a & i \\sqrt{1-a^2} \\\\ i\\sqrt{1-a^2} & a\\end{pmatrix}=\\begin{pmatrix} P'(a) & i Q'(a)\\sqrt{1-a^2} \\\\ i Q'(a)^*\\sqrt{1-a^2} & P'(a)^*\\end{pmatrix}\\] with (primes are not derivatives!) \\[P'(a)=aP(a)-Q(a)(1-a^2),\\qquad Q'(a)=P(a)+aQ(a) \\ .\\]\nThe polynomials \\(P,Q\\) obtained in this way are not arbitrary. They always satisfy the following three properties.\n\n\\(\\operatorname{deg}(P)\\le d\\), \\(\\operatorname{deg}(Q)\\le d-1\\). In words, the degree of \\(P\\) is no more than the number of times \\(R(a)\\) is applied, and degree of \\(Q\\) is one less.\n\\(P(-a)=(-1)^{d+1} P(a), Q(-a)=(-1)^{d} Q(a)\\). In words, both \\(P\\) and \\(Q\\) have definite and opposite parity. For \\(d\\) even \\(P\\) is odd and \\(Q\\) is even, for \\(d\\) odd \\(P\\) is even and \\(Q\\) is odd.\n\\(|P(a)|+(1-a^2)|Q(a)|^2=1\\). This follows from \\(U_\\phi\\) being a unitary. In particular, this implies that \\(P(a)\\) is bounded \\(|P(a)|\\le 1\\) for \\(-1\\le a\\le 1\\).\n\nAn important fact of QSP is that the reverse statement is also true - as long as a degree \\(d\\) polynomial \\(P\\) satisfies these conditions, there are \\(d+1\\) angles \\(\\phi_i\\) that produce \\(P\\) via the QSP \\(\\eqref{QSP}\\). While the proof is not trivial, the statement looks reasonable. Indeed, a complex polynomial \\(P\\) of degree \\(d\\) and definite parity has about \\(d\\) independent real coefficients, and this is exactly the number of angles we can tweak in the QSP. Since dimensions of the parameter spaces agree, it is plausible that they can map both ways."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-general_poly",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-general_poly",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "2.4 Generalizing QSP polynomials",
    "text": "2.4 Generalizing QSP polynomials\nWhile many polynomials \\(P\\) can be obtained in this way, there are important limitations. In particular, by property (3) \\(|P(1)|=1\\), which is often undesirable (say, you need to approximate \\(f(x)\\) such that \\(f(1)\\neq1\\)). The trick is to look at the real part \\(\\operatorname{Re} P(a)\\). While still bounded, it does not have to satisfy \\(\\operatorname{Re} P(1)=1\\) and is general enough for many applications. Then the problem is how to access \\(\\operatorname{Re}P\\). One way is to measure the QSP operator in \\(|\\pm\\rangle\\) basis. Indeed, you can check that\n\\[\n\\langle +|U_\\phi|+\\rangle=\\operatorname{Re}P(a)+i\\operatorname{Re}Q(a)\\sqrt{1-a^2} \\ .\n\\]\nA QSP sequence can be found so that \\(\\operatorname{Re}P(a)\\) gives the desired polynomial, while \\(\\operatorname{Re}Q(a)\\) is approximately zero.\nAnother way to single out the real part of \\(P(a)\\) is by introducing an auxiliary qubit and applying the following circuit\n\n\n\nFig. 1: Block encoding real polynomial\n\n\nThis trick more readily generalizes to QSVT. In effect, this circuit performs a block encoding of \\(\\frac12\\left(U_\\phi+U_{-\\phi}\\right)\\). Since \\(U_{-\\phi}=U_{\\phi}^*\\), it block encodes \\(\\frac12\\left(P(a)+P(a)^*\\right)=\\operatorname{Re}P(a)\\). If the circuit above is not clear, don’t worry, I will discuss block encodings in Sec. 4. The same trick can be repeated (using one more ancilla qubit) to combine even and odd polynomials into a general one. In effect\n\n\n\n\n\n\n\nCombining several QSP circuits, we can block encode any real polynomial \\(P(x)\\) satisfying \\(|P(x)|\\le1\\) for \\(x\\in [-1,1]\\). The number of signal operators required is proportional to \\(\\operatorname{deg}P(x)\\).\n\n\n\n\nThis will be sufficient for our QSVT applications."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#implementation",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#implementation",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "2.5 Implementation",
    "text": "2.5 Implementation\nLet’s now implement the QSP and see how it works in practice. First we define the \\(R\\)-signal and signal-processing operators. To get QSP sequences we will use pyqsp python package  [6]. Since it gives anges in the \\(W\\)-convention, we will also need to convert them to our \\(R\\)-convention. Finally, we define a function that assembles the QSP circuit.\n\ndef R(a):\n    \"\"\"Singnal operator in the R-convention.\"\"\"\n    \n    b = np.sqrt(1-a**2)\n    R = np.array([\n        [a, b],\n        [b, -a]])\n\n    return R\n\ndef S(phi):\n    \"\"\"Diagonal signal-processing operator.\"\"\"\n    \n    return np.diag([np.exp(1j*phi), np.exp(-1j*phi)])\n\ndef angles_from_W_to_R(phis):\n    \"\"\"Converts QSP angles from W-convention to R-convention.\"\"\"\n    \n    assert len(phis)&gt;0, 'At least one angle must be provided.'\n    \n    phis = \\\n    [phis[0]+(2*len(phis)-3)*np.pi/4] \\\n    + [phi-np.pi/2 for phi in phis[1:-1]] \\\n    + [phis[-1]-np.pi/4]\n    \n    return phis \n\ndef apply_QSP(R, phis):\n    \"\"\"Computes S(phi_0) @ R @ S(phi_1) @ R ... @ S(phi_d-1) @ R @ S(phi_d). \"\"\"\n        \n    res = S(phis[-1])\n    for phi in phis[:-1][::-1]:        \n        res = S(phi) @ R @ res\n    \n    return res\n\nNow let’s take some arbitrary polynomial \\(P\\), determine the QSP angles \\(P\\to \\phi\\), and check that the real part of the QSP amplitude \\(U_\\phi[0,0]\\) reproduces the chosen polynomial.\n\n# Note that the polynomial must have definite parity and be bounded.\np = np.polynomial.Polynomial([0., 0.1, 0., -0.4, 0., 0.4]) \n\n# QSP angles from polynomial coefficients.\nphis = pyqsp.angle_sequence.QuantumSignalProcessingPhases(p.coef, signal_operator='Wx')\nphis = angles_from_W_to_R(phis)\n\n# Range to scan over.\nx = np.linspace(-1, 1, 50)\n\n# Compute matrix elements of the QSP sequence\nunitaries = [apply_QSP(R(xi), phis) for xi in x]\nmatrix_elements = [np.real(u[0,0]) for u in unitaries]\n\n# Compare\nplt.plot(x, p(x));\nplt.plot(x, matrix_elements, '*');\nplt.title('Your polynomial implemented by QSP');\n\n\n\n\nIf you are running this as an interactive notebook, try changing the polynomial and see what happens. This exercise ends our introduction to QSP.\nYou may think of QSVT as a generalization of QSP, where instead of applying a polynomial transformation to a single matrix element, you apply it to the singular values of a block encoded matrix. Before explaining how exactly does that work, we should cover some technical background."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#what-is-it",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#what-is-it",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "3.1 What is it?",
    "text": "3.1 What is it?\nProbably it’s worth making a brief digression to discuss what are those singular values that we are going to transform. If, like me, you are more familiar with hermitian matrices and eigenvalues, think of the singular values as a generalization.\nSo, the key theorem known as the singular value decomposition (SVD), says that any (and I mean any: real or complex, hermitian or non-hermitian, square or rectangular) matrix \\(A\\) can be decomposed as follows \\[A=V\\Sigma W^\\dagger \\ .\\]\nIf \\(A\\) is \\(n\\times m\\) matrix, then \\(V\\) is a unitary of dimension \\(n\\times n\\), \\(W\\) is a unitary of dimension \\(m\\times m\\), and \\(\\Sigma\\) is something like a diagonal matrix, except it has dimension \\(n\\times m\\) and can be non-square. The ‘extra’ non-square part consists of zeros. If you remove it, \\(\\Sigma\\) is just a diagonal matrix with positive entries \\(\\sigma_i\\). Another way of writing SVD is using the bra-ket notation \\[A=\\sum_i \\sigma_i|v_i\\rangle \\langle w_i| \\ .\\]"
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#relation-to-eigenvalues",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#relation-to-eigenvalues",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "3.2 Relation to eigenvalues",
    "text": "3.2 Relation to eigenvalues\nFor Hermitian matrices the eigenvalue decomposition is almost the same as SVD with \\(V=W\\). A subtlety is that by definition \\(\\sigma_i\\ge0\\) while the eigenvalues can be negative. Negative signs of the eigenvalues can be absorbed into \\(V\\) or \\(W\\), but this will make them different.\nAlso, you can show that non-zero eigenvalues of hermitian operators \\(A^\\dagger A\\) and \\(AA^\\dagger\\) are equal to \\(\\sigma_i^2\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-operator_norm",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-operator_norm",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "3.3 Relation to operator norm",
    "text": "3.3 Relation to operator norm\nSVD has many applications, and I’ll mention two of them that will be relevant. Define the operator norm of \\(A\\) by \\[||A||=\\sup_v \\frac{|Av|}{|v|} \\ . \\]\nIn words, the linear transformation \\(A\\) in general changes the length of the input vector \\(v\\), and the operator norm quantifies the largest such change. It is easy to see that \\(||A||=\\max_i \\sigma_i\\), i.e. that operator norm is equal to the largest singular value.\nOperator norm is a natural measure in quantum mechanics, and in particular in QSVT. Assume we aimed to implement some operator \\(A_0\\) but only managed to implement \\(A\\). For an arbitrary state, we can write \\(A|\\psi\\rangle=A_0|\\psi\\rangle+(A-A_0)|\\psi\\rangle\\). The error term can be bounded as \\(|(A-A_0)|\\psi\\rangle|\\le ||A-A_0||\\). Hence, if \\(A\\) is close to \\(A_0\\) in the operator norm, its action on any state will have a large overlap with the target state. In quantum mechanics, this means that for all practical purposes \\(A\\) is a good approximation to \\(A_0\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-pseudo_inverse",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-pseudo_inverse",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "3.4 Moore-Penrose pseudo-inverse",
    "text": "3.4 Moore-Penrose pseudo-inverse\nAnother application of SVD is to linear systems of equations \\(Ax=b\\). Depending on \\(A\\) and \\(b\\), the system may have a single solution, many solutions or none at all. A related problem is to minimize \\(|Ax-b|^2\\), a problem which always has a solution. It can be written as \\(x=A^+b\\) with \\(A^+\\) being the Moore-Penrose pseudo-inverse. It is most easily defined in terms of the SVD \\[\\begin{align}\nA^+=\\sum_{\\sigma_i\\neq0}\\sigma_i^{-1} |w_i\\rangle \\langle v_i| \\ . \\label{pseudo-inverse}\n\\end{align}\\] For invertible matrices, \\(A^+\\) coincides with the usual inverse \\(A^{-1}\\). For non-invertible ones, this is in a sense the closest you can get. Note that \\(A^+\\) is equal to \\(A^\\dagger\\) with singular values inverted."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-matrix_poly",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-matrix_poly",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "3.5 How to define polynomial of an arbitrary matrix",
    "text": "3.5 How to define polynomial of an arbitrary matrix\nIt is straightforward to define a polynomial of a hermitian matrix, e.g. for \\(p(x)=x-x^2+x^4\\) we set \\(p(H)=H-H^2+H^4\\). Alternatively, we can say that the polynomial applies to eigenvalues of \\(H\\), i.e. if \\(H=\\sum_i \\lambda_i |v_i\\rangle\\langle v_i|\\) then \\(p(H)=\\sum_i p(\\lambda_i) |v_i\\rangle\\langle v_i|\\).\nFor a general matrix \\(A\\) expressions like \\(A^2\\) may not make sense, because of incompatible dimensions. More abstractly, for a generic operator \\(A\\) the input space need not be the same as the output space, so applying \\(A\\) twice is simply not defined. In principle, we could still define \\(p(A)\\) by applying \\(p\\) to the singular values of \\(A\\), similarly to the hermitian case. The more natural and useful definition is a bit different.\n\n\n\n\n\n\nPolynomial of a general matrix\n\n\n\n\nFor \\(A=\\sum_i \\sigma_i|v_i\\rangle\\langle w_i|\\) define \\[\\begin{align}\np(A)=\\begin{cases} \\sum_i p(\\sigma_i)|v_i\\rangle \\langle w_i|,\\qquad \\text{$p$ is odd}\\\\ \\sum_i p(\\sigma_i)|w_i\\rangle \\langle w_i|,\\qquad \\text{$p$ is even} \\end{cases}\n\\end{align}\\]\n\n\n\nFor example, for \\(p(x)=1-x^2+x^4\\) we get \\(p(A)=\\mathbb{1}-A^\\dagger A+(A^\\dagger A)^2\\), for \\(p(x)=x-x^3\\) we get \\(p(A)=A-AA^\\dagger A\\). The pattern should be clear. By alternating \\(A\\) and \\(A^\\dagger\\) we make sure that the input space of a new operator is the output space of a preceding one. If the \\(p(x)\\) is odd, \\(p(A)\\) maps between the same space as \\(A\\). If \\(p(x)\\) is even, \\(p(A)\\) maps the input space of \\(A\\) back to itself.\nWe will see that this distinction between the even and odd polynomials spills over into QSVT."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#block-encoding-in-the-computational-basis",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#block-encoding-in-the-computational-basis",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.1 Block encoding in the computational basis",
    "text": "4.1 Block encoding in the computational basis\nThe simplest case of the block encoding is when your matrix of interest \\(A\\) occupies the top-left corner of a unitary\n\\[U=\\begin{pmatrix}A& *\\\\ * & *\\end{pmatrix} \\ .\\]\nHere and below, the wildcard asterisk \\(*\\) means that we don’t care what is contained in other blocks as long as \\(U\\) is unitary. \\(A\\) and \\(*\\) blocks can be of any size compatible with one another. \\(U\\) must be square, of course. That’s it, that’s block encoding.\nThe tricky part is to actually find an efficient quantum circuit corresponding to \\(U\\). In general, this is very problem-specific. I will mostly assume that block encodings are given from above (we have an oracular access to them). I will however discuss block encoding of a linear combination of unitaries in Sec. 4.5."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#can-any-matrix-be-block-encoded",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#can-any-matrix-be-block-encoded",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.2 Can any matrix be block encoded?",
    "text": "4.2 Can any matrix be block encoded?\nThe block encoded matrix \\(A\\) can be almost arbitrary. The only restriction is that its operator norm is less than one \\(||A||\\le1\\) (otherwise \\(U\\) can not be unitary). If this is not the case, we can often encode \\(A/\\alpha\\) with sufficiently large \\(\\alpha\\) instead. Again, this is problem-specific."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#single-qubit-block-encoding",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#single-qubit-block-encoding",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.3 Single-qubit block encoding",
    "text": "4.3 Single-qubit block encoding\nOften, the operator \\(A\\) to be block encoded naturally acts on some \\(n\\)-qubit system, perhaps \\(A\\) is a Hamiltonian. In that case, assuming \\(||A||\\le 1\\), we only need one additional qubit to block-encode \\(A\\). The result may look something like\n\\[U=\\begin{pmatrix} A & \\sqrt{1-AA^\\dagger} \\\\ \\sqrt{1-A^\\dagger A} & -A^\\dagger \\end{pmatrix}\\ .\\]\n\n\n\n\n\n\nExercise\n\n\n\n\n\nShow that \\(U\\) is unitary. You will need to show that \\(A\\sqrt{1-A^\\dagger A}=\\sqrt{1-AA^\\dagger}A\\) which can be done via SVD.\n\n\n\nWe can express the fact that \\(U\\) block encodes \\(A\\) by writing \\(U=|0\\rangle\\langle0|\\otimes A+\\dots\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#controlled-unitary-is-a-block-encoding",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#controlled-unitary-is-a-block-encoding",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.4 Controlled unitary is a block encoding",
    "text": "4.4 Controlled unitary is a block encoding\nFor intuition, I will give two examples of how to implement block encodings. One familiar case is a controlled unitary operation. Indeed, \\[C(U)=|0\\rangle \\langle 0| \\otimes U+|1\\rangle \\langle 1| \\otimes U=\\begin{pmatrix} U & 0 \\\\ 0 & \\mathbb{1}\\end{pmatrix} \\ .\\] Note that this is \\(|0\\rangle\\)-controlled unitary, the standard \\(|1\\rangle\\)-controlled has \\(U\\) and the identity blocks swapped.\nMore generally, let \\(U\\) be controlled by a computational \\(n\\)-qubit state \\(|m\\rangle\\), i.e. \\(C_{|m\\rangle\\langle m|}U=|m\\rangle\\langle m| \\otimes U+(\\mathbb{1}-|m\\rangle\\langle m|)\\otimes\\mathbb{1}\\). Then, in matrix form\n\\[C_{|m\\rangle\\langle m|}U=\\begin{pmatrix}\\mathbb{1} &&&&\\\\ &\\ddots&&& \\\\ && U && \\\\ &&&\\ddots& \\\\ &&&&\\mathbb{1}\\end{pmatrix} \\ ,\\]\nwhere \\(U\\) occupies \\(m\\)-th block."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-LCU",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-LCU",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.5 Linear combination of unitaries",
    "text": "4.5 Linear combination of unitaries\nLinear combination of unitaries (LCU) is a powerful technique for block encoding. Using controlled versions of unitary operators \\(U_1,\\dots, U_N\\) it block encodes their linear combination \\(\\alpha_1 U_1+\\dots\\alpha_N U_N\\)\nIt works as follows. Assume \\(N=2^n\\) and you can implement unitaries \\(U_m\\) controlled by a state of \\(n\\) qubits, \\(C_{|m\\rangle\\langle m|}U_m\\). Taking the product of all these controlled unitaries we get what is known as the \\(\\text{SELECT}\\) operator\n\\[\\text{SELECT} = \\prod_m C_{|m\\rangle\\langle m|}U_m = \\begin{pmatrix}U_1 &&&&\\\\ &\\ddots&&& \\\\ && U_m && \\\\ &&&\\ddots& \\\\ &&&&U_{2^n}\\end{pmatrix}\\]\nIn other words, \\(\\text{SELECT} = \\sum_m |m\\rangle\\langle m| \\otimes U_m\\), and it applies (selects) a different unitary based on the value of the control state. Now also assume you have a \\(\\text{PREPARE}\\) operator acting on the control qubits as follows \\(\\text{PREPARE} |0\\rangle=\\sum_m \\sqrt{\\alpha_m} |m\\rangle\\) (assuming \\(\\sum_i\\alpha_i=1\\)). Then\n\\[\\text{PREPARE}^\\dagger\\cdot\\text{SELECT}\\cdot\\text{PREPARE} = \\begin{pmatrix}\\alpha_1 U_1+\\dots \\alpha_NU_N& *\\\\ *&* \\end{pmatrix}\\]\ni.e. it block encodes the desired linear combination.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nProve this statement. Show also that Fig. 1 is a particular case, and that it indeed block encodes \\(\\frac12\\left(U_{\\phi}+U_{-\\phi}\\right)\\) as claimed.\n\n\n\nNote that we need logarithmically less ancilla qubits than the number of unitaries, so the computations above are efficient. One of the use cases for this protocol is to block encode a local Hamiltonian, i.e. a Hamiltonian that consists of not too many Pauli strings of bounded weight. Implementing the controlled version of each Pauli string is straightforward, and the procedure goes through."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#applying-block-encoded-operator-to-a-state",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#applying-block-encoded-operator-to-a-state",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.6 Applying block encoded operator to a state",
    "text": "4.6 Applying block encoded operator to a state\nWe can apply a block-encoded operator \\(A\\) to a quantum state \\(|\\psi\\rangle\\) as follows. Assume \\(U=|0\\rangle\\langle0|\\otimes A+\\dots\\) block-encodes \\(A\\) and we prepared the \\(n+1\\) qubit system in the state \\(|\\Psi\\rangle=|0\\rangle \\otimes|\\psi\\rangle\\). Then\n\\[U|\\Psi\\rangle = \\begin{pmatrix} A & *\\\\ * & *\\end{pmatrix}\\begin{pmatrix} |\\psi\\rangle \\\\ 0\\end{pmatrix}=\\begin{pmatrix}A|\\psi\\rangle \\\\ * \\end{pmatrix}=|0\\rangle\\otimes A|\\psi\\rangle+|1\\rangle\\otimes|*\\rangle \\ .\\]\nTherefore, if we measure the ancillary qubit after applying \\(U\\) to \\(|\\Psi\\rangle\\) and find it in state \\(|0\\rangle\\), the state of the remaining qubits is \\[\\frac{A|\\psi\\rangle}{\\langle\\psi|A^\\dagger A|\\psi\\rangle} \\ .\\] I.e., up to a normalization, it is the state \\(A|\\psi\\rangle\\) we aimed to find. On the other hand, if we find the ancialla qubit in state \\(|1\\rangle\\) we don’t get a useful result. Indeed, a general non-hermitian operator can not be implemented on a quantum computer deterministically, we need to post-select on the state of the ancillary qubit. The success probability \\(P\\) of this protocol is given by \\(P=\\langle\\psi|A^\\dagger A|\\psi\\rangle\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#block-encoding-in-general-bases-and-projection-operators",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#block-encoding-in-general-bases-and-projection-operators",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.7 Block encoding in general bases and projection operators",
    "text": "4.7 Block encoding in general bases and projection operators\nIn general, we do not need the block encoding to be in the computational basis, i.e. that \\(A\\) be the top-left block of a unitary \\(U\\). We can assume this holds only in some specially chosen input and output bases\n\\[U = B_L \\begin{pmatrix} A & * \\\\ * & *\\end{pmatrix} B_R^\\dagger \\ .\\]\nHere, \\(B_L\\) and \\(B_R\\) are unitary matrices that perform the necessary bases change.\nThis statement is a bit empty because there is too much freedom: using properly chosen \\(B_R\\) and \\(B_L\\) we can say that any \\(U\\) is a block encoding of any \\(A\\). What we really need in QSVT is the ability to perform controlled projectors based on the block encoding bases. Here is what I mean. Define \\[\\Pi_n = \\begin{pmatrix} \\mathbb{1}_{n\\times n} & 0 \\\\ 0 & 0\\end{pmatrix} \\ .\\] This is a projector on the first \\(n\\) computation basis vectors. Now define left and right projectors \\[\\Pi_L = B_L P_n B_L^\\dagger, \\qquad \\Pi_R = B_R P_m B_R^\\dagger \\ .\\] If \\(A\\) is square \\(n=m\\), but for general block encodings this need not hold. The key property of the projectors is\n\\[\\Pi_L U \\Pi_R= B_L\\begin{pmatrix} A & 0 \\\\ 0 & 0\\end{pmatrix} B_R^\\dagger \\ .\\]\nQSVT does not explicitly use \\(B_L\\) or \\(B_R\\). We only need to have access to operators \\[\\Pi_L(\\phi)=e^{(2\\Pi_L-1)\\phi},\\quad \\Pi_R(\\phi)=e^{(2\\Pi_R-1)\\phi} \\ .\\] These are analogs of the signal-processing operator \\(S(\\phi)\\) we defined in QSP \\(\\eqref{S}\\). Note that \\[\\Pi_L(\\phi)=B_L\\begin{pmatrix}e^{i\\phi} &0 \\\\ 0 & e^{-i\\phi}\\end{pmatrix}B_L^\\dagger \\] (and similarly for \\(\\Pi_R(\\phi)\\)). So, in the right-basis, \\(\\Pi(\\phi)\\) is indeed a block version of the QSP signal-processing operator."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-projector_rotations",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-projector_rotations",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "4.8 Projector rotations from controlled projectors",
    "text": "4.8 Projector rotations from controlled projectors\nProjector rotations \\(\\Pi(\\phi)\\) are easy to implement provided access to the projector-controlled NOT operation \\(C_{\\Pi}NOT\\), which is defined as \\[C_{\\Pi}NOT=X\\otimes\\Pi+\\mathbb{1}\\otimes (1-\\Pi)\\] or, graphically,\n\n\n\n\n\nSlashed line means the operator can act on an arbitrary number of qubits. The rotation \\(\\Pi(\\phi)\\) can be implemented as follows\n\n\n\n\n\nThus, with an additional ancilla qubit and the ability to perform \\(C_\\Pi NOT\\) we get to implement \\(\\Pi(\\phi)\\). In what follows, I will not worry about this implementation detail and assume direct access to \\(\\Pi(\\phi)\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#recap-of-the-problem",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#recap-of-the-problem",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "5.1 Recap of the problem",
    "text": "5.1 Recap of the problem\nOur original intention was to apply some function \\(f(x)\\) to the singular values. However, in QSP/QSVT we only know how to apply polynomials \\(p(x)\\), so we will need a polynomial that approximates the target function well. There are also restrictions on polynomials that we can implement.\n\nThe polynomial must be bounded: \\(|p(x)|\\le1\\) for \\(-1\\le x \\le 1\\).\nWe can only implement directly even or odd \\(p(x)\\). If the target function \\(f(x)\\) is of indefinite parity, we approximate its even and odd parts separately, then combine.\n\nIn a typical application we only need to approximate \\(f(x)\\) on a subset \\([a,b]\\subset [-1, 1]\\). So the approximation task is to find \\(p(x)\\) such that\n\n\\(|p(x)-f(x)|\\le \\epsilon\\) for \\(x\\in (a,b)\\).\n\\(|p(x)|\\le 1\\) for \\(x\\in(-1,1)\\).\n\\(p(x)\\) has the smallest degree possible.\n\nHere is an illustration of what that should look like.\n\n\n\nApproximation\n\n\nNote that \\(|f(x)|\\) need not be bounded by 1 on \\([-1,1]\\), but only on \\([a,b]\\), where we wish to approximate it. Similarly, \\(|p(x)|\\) only needs to be bounded by 1 in \\([-1,1]\\). In fact, because \\(p(x)\\) is a polynomial, it will necessarily blow up for sufficiently large \\(x\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#chebyshev-polynomials",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#chebyshev-polynomials",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "5.2 Chebyshev polynomials",
    "text": "5.2 Chebyshev polynomials\nOne of the most popular polynomial bases for approximations are given by Chebyshev polynomials, which can be defined as\n\\[T_n(x)=\\cos(n\\arccos x)=\\frac12\\left(\\left(x+\\sqrt{1-x^2}\\right)^n+\\left(x-\\sqrt{1-x^2}\\right)^n\\right) \\ .\\]\nAlright, this does not look very intuitive, what is so special about them? Actually, I don’t know. Let’s maybe ask a simpler question first, why not use the good old Taylor expansion? This is probably the first thing that comes to mind when talking about a polynomial approximation. The problem is, the Taylor series is designed to capture the behavior of a function near a particular point, and the quality of approximation degrades quickly as we step outside the close vicinity. Even if the Taylor series converges there, the convergence may be way too slow.\nTo capture the behavior over a certain range of values some other functional basis may be better. Probably a Fourier expansion will work well, except it is not polynomial. Now, let’s actually take a look at the Chebyshev polynomials.\n\n\nCode\nplt.subplot(1, 2, 1)\nx = np.linspace(-1, 1, 100)\nfor n in range(5):\n    plt.plot(x, plt.np.polynomial.Chebyshev.basis(n)(x), label=f'$T_n$={n}')\nplt.title('Chebyshev polynomials in [-1, 1])')\nplt.legend();\nplt.subplot(1, 2, 2)\nx = np.linspace(-2, 2, 100)\nfor n in range(5):\n    plt.plot(x, plt.np.polynomial.Chebyshev.basis(n)(x), label=f'$T_n$={n}')\nplt.title('Chebyshev polynomials in [-2, 2]')\nplt.legend();\nplt.tight_layout()\n\n\n\n\n\nIs it just me, or the Chebyshev polynomials on \\([-1,1]\\) (left fig) actually look quite a lot like trigonometric functions? Note that they are also bounded there, and that they quickly blow up outside \\([-1, 1]\\) (right fig). For further intuition about the Chebyshev polynomials, you may wish to check out tis blog post by Jason Sachs  [7].\n\n\n\n\n\n\nWarning\n\n\n\nThe Chebyshev polynomials are most interesting for \\(x\\in [-1,1]\\), but in QSP/QSVT applications we will linearly map this domain to \\([a,b]\\subset[-1,1]\\), and so we will in fact be interested in how the Chebyshev polynomials behave outside \\([-1,1]\\) as well."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#chebyshev-series-and-bernstein-ellipse",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#chebyshev-series-and-bernstein-ellipse",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "5.3 Chebyshev series and Bernstein ellipse",
    "text": "5.3 Chebyshev series and Bernstein ellipse\nNow, as a respectable functional basis the Chebyshev polynomials have the property that an arbitrary reasonable function \\(f(x)\\) can be expanded into the Chebyshev series \\[f(x)=\\sum_{n\\ge0} a_nT_n(x), \\qquad x\\in [-1, 1] \\ .\\] Coefficients \\(a_n\\) decay and can bound as follows  [2]\n\\[\\begin{align}\na_n\\le 2M\\rho^{-n} \\ . \\label{an}\n\\end{align}\\]\nParameters \\(M\\) and \\(\\rho\\) need to be explained. Define the Bernstein ellipse \\(E_{\\rho}\\) as the subset of the complex plane inside the ellipse \\(\\frac{z+z^{-1}}{2}, |z|=\\rho\\), or alternatively \\(\\frac{\\rho e^{i\\phi}+\\rho^{-1} e^{-i\\phi}}{2},\\phi\\in (0,2\\pi)\\). Without loss of generality, we assume \\(\\rho\\ge1\\). The interval \\([-1,1]\\) is contained in \\(E_\\rho\\). Here is a sketch.\n\n\n\n\n\nNow, assume that \\(f(x)\\) can be analytically continued from \\([-1,1]\\) to the interior of \\(E_\\rho\\). Then, we denote by \\(M\\) an upper bound on the value of \\(f(z)\\), i.e. \\(|f(z)|\\le M, z\\in E_\\rho\\).\nIn a typical application, we have a family of functions \\(f_\\lambda(x)\\) and want to look at the asymptotic of \\(\\lambda\\to 0\\). As \\(\\lambda\\) decreases, \\(M\\) tends to increase. To counteract this and keep \\(M\\) constant, we will need to shrink the Bernstein ellipse.\nAlright, so for any fixed \\(M\\) and \\(\\rho\\) the Chebyshev coefficients of \\(f(x)\\) decay exponentially \\(\\eqref{an}\\). Given this fact and that all Chebyshev polynomials are bounded by 1 on \\(x\\in [-1,1]\\) we find that the truncated Chebyshev series satisfies \\[|f(x)-\\sum_{m=0}^n a_m T_m(x)|_{[-1,1]}\\le \\frac{2M\\rho^{-n-1}}{\\rho-1} \\ . \\]\nHowever, this approximation result is in general not sufficient for QSVT applications. The interval \\([-1,1]\\) here should be linearly mapped to a subset \\([a,b]\\) from the previous section, i.e. the subset where the target function needs to be approximated. This means that along with finding a good approximation on \\([-1,1]\\) we also need to ensure that the resulting polynomial remains bounded in some region outside \\([-1,1]\\), which is an additional challenge."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-polynomial_how_to",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-polynomial_how_to",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "5.4 Bounded Chebyshev approximations",
    "text": "5.4 Bounded Chebyshev approximations\nA general approach to this problem is proposed in  [2]. The idea is to take an approximation that behaves well on \\([-1,1]\\) but blows up outside, and multiply it by a function that is close to 1 at \\([-1, 1]\\) and rapidly decays outside. Then the product can be re-expanded in the Chebyshev basis and yields the desired result. Here is what we get (Theorem 19 of  [2]).\n\n\n\n\n\n\nBounded approximation by Chebyshev polynomials\n\n\n\n\nLet \\(f(x)\\) be analytic on \\([-1, 1]\\) and analytically continuanable to \\(E_\\rho\\) where it is bounded by \\(M\\). Denote \\(\\rho=1+\\alpha, \\alpha&gt;0\\). For any \\(\\epsilon\\) there is a polynomial \\(p(x)\\) such that\n\n\\(|f(x)-p(x)|_{[-1,1]}\\le M\\epsilon\\).\n\\(|p(x)|_{[-1-\\delta, 1+\\delta]}\\le M\\).\n\\(|p(x)|_{[-b,-1-\\delta]\\cup [1+\\delta, b]}\\le M\\epsilon\\).\n\n\n\n\nIn words, \\(p(x)\\) approximates \\(f(x)\\) to relative precision \\(\\epsilon\\) on \\([-1,1]\\). Furthermore, \\(p(x)\\) is bounded by a constant in some \\(\\delta\\)-neighborhood outside \\([-1, 1]\\). Finally, \\(p(x)\\) is \\(\\epsilon\\)-close to zero outside this \\(\\delta\\)-neighborhood and up to a larger \\(b\\)-neighborhood. Here is a sketch.\n\n\n\n\n\nThe condition that \\(p(x)\\) is zero ouside some \\(\\delta\\)-neighborhood is useful for approximating piece-wise smooth functions. Now, I didn’t yet tell what are \\(\\delta\\) and \\(b\\) and, most importantly, what is the degree of the polynomial? According to  [2], \\(\\delta\\) can be anywhere in \\((0, \\frac{min(1,\\alpha^2)}{C})\\) for a sufficiently large but constant \\(C\\). And you get to choose \\(b&gt;1+\\delta\\) freely. Then, the desired polynomial has degree \\[\\begin{align}\n\\operatorname{deg}p(x)=O\\left(\\frac{b}{\\delta}\\log \\frac{b}{\\delta \\epsilon}\\right) \\label{pdeg} \\ .\n\\end{align}\\]\nSome important features of this result.\n\nIf you insist on a good approximation at the point of discontinuity, you pay a linear price. I.e. the degree of the polynomial scales linearly (modulo the log factor) with \\(\\delta^{-1}\\).\nIf you want your polynomial approximation to be bounded far enough away from \\([-1,1]\\) you pay a linear price, i.e. the degree scales linearly (modulo the log factor) with \\(b\\).\nNeatly, dependence on the error \\(\\epsilon\\) is only logarithmic."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-theta_approx",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-theta_approx",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "5.5 Illustration: approximating the sign function",
    "text": "5.5 Illustration: approximating the sign function\nIn many applications of QSVT it is actually very helpful to approximate the sign function \\(\\operatorname{sign}(x)\\). This will also help us illustrate the approximation results described above.\nNote that \\(\\operatorname{sign}(x)=\\theta(x)-\\theta(-x)\\), where \\(\\theta(x)\\) is the step function. We can approximate each \\(\\theta(\\pm x)\\) separately, then combine. We will require that our approximation is \\(\\epsilon\\)-close to \\(\\operatorname{sign}(x)\\) outside a small region \\((-a,a)\\) around the discontinuity.\nTake for concreteness \\(\\theta(x)\\). For \\(x&gt;0\\) it is smooth and in fact constant, so \\(M=1\\) for arbitrary \\(\\rho\\). The only thing we need to take care of is that the discontinuity region, where our approximation fails, is small enough \\(\\delta\\le a\\). This implies that there is a polynomial of degree \\(O(\\frac1a\\log\\frac1{a\\epsilon})\\) that does the job.\nIn this and some other cases, the approximation bound stated in the previous section does not give the tightest result. For instance, approximating the sign function is possible with a polynomial of degree \\(O(\\frac1a\\log\\frac1\\epsilon)\\), i.e. the additional \\(\\log\\frac1{a}\\) factor is not necessary. However, the overhead we get from applying the procedure above is often modest, a may be a good price to pay for the universality of the approach."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#recap",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#recap",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "6.1 Recap",
    "text": "6.1 Recap\nLet’s recap the setup.\n\nThe goal is to perform a functional transformation on the singular values of some operator \\(A\\to f(A)\\).\nWe are provided with a unitary \\(U\\) that block encodes \\(A\\), i.e. \\(U=B_L\\begin{pmatrix} A&*\\\\ *&*\\end{pmatrix}B_R^\\dagger\\).\nWe are provided with projectors \\(\\Pi_L,\\Pi_R\\) that describe this block encoding, i.e. \\(\\Pi_L U \\Pi_R=B_L\\begin{pmatrix} A&0\\\\0&0\\end{pmatrix}B_R^\\dagger\\) and can perform projector rotations \\(\\Pi_L(\\phi), \\Pi_R(\\phi)\\).\nWe figured out what is a good polynomial approximation to our target function \\(p(x)\\approx f(x)\\).\n\nFrom these ingredients, the QSVT builds a unitary \\(U_\\phi\\) that block encodes the desired polynomial transformation."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#how-qsvt-works",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#how-qsvt-works",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "6.2 How QSVT works",
    "text": "6.2 How QSVT works\nAlright, how to build \\(U_\\phi\\)? A bit awkwardly, we will have to separately describe two cases, when \\(p(x)\\) is even and when it is odd. So, for \\(p(x)\\) even (the number of QSP phases is odd)\n\n\n\n\n\n\nQSVT circuit (even)\n\n\n\n\n\\[\\begin{align}\nU_\\phi = \\left(\\prod_{i=0}^{d/2-1}\\Pi_R(\\phi_{2i})U^\\dagger\\Pi_L(\\phi_{2i+1})U \\right) \\Pi_R(\\phi_d),\\qquad U_\\phi=B_R\\begin{pmatrix}p(A) & *\\\\ * & *\\end{pmatrix} B_R^\\dagger\\ . \\label{QSVT even}\n\\end{align}\\]\n\n\n\nFor \\(p(x)\\) odd (the number of QSP phases even)\n\n\n\n\n\n\nQSVT circuit (odd)\n\n\n\n\n\\[\\begin{align}\nU_{\\phi}=\\Pi_L(\\phi_{0})U\\prod_{i=1}^{(d-1)/2}\\Pi_R(\\phi_{2i-1})U^\\dagger\\Pi_L(\\phi_{2i})U \\Pi_R(\\phi_d),\\qquad U_\\phi=B_L\\begin{pmatrix}p(A) & *\\\\ * & *\\end{pmatrix} B_R^\\dagger \\ . \\label{QSVT odd}\n\\end{align}\\]\n\n\n\nRecall that we defined the polynomial of a matrix \\(p(A)\\) in Sec. 3.5, and that it also had a personality split into even/odd cases. The angles \\(\\phi\\) are the same angles that we would use to apply \\(p(x)\\) in QSP with \\(R\\)-convention. Overall, this is pretty similar to how QSP circuits work.\n\n\n\n\n\n\nNote\n\n\n\nOne thing I tripped over. In QSP, we have different signal conventions and need to use a different angle sequence for each. Now I just told you that for QSVT we need to borrow QSP angles in the \\(R\\)-convention. Don’t we need to specify how exactly is our operator \\(A\\) block encoded in \\(U\\) then? No, we need not. The difference between QSP and QSVT is that in QSP we apply the same signal operator throughout, while in QSVT we alternate between \\(U\\) and \\(U^\\dagger\\). Because different block encodings are related by unitary transformations, these differences cancel in the QSVT sequence. To see why the R-convention of QSP most naturally maps to QSVT, take a look at section Sec. 6.5."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#qsvt-circuit",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#qsvt-circuit",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "6.3 QSVT circuit",
    "text": "6.3 QSVT circuit\nLet’s take a moment to visualize the QSVT as a quantum circuit. In the simplest case, which often holds in practice, \\(B_L=B_R=\\mathbb{1}\\) and \\(\\Pi=\\Pi_R=\\Pi_L=|0\\rangle^{\\otimes k}\\langle0|^{\\otimes k}\\), i.e. \\(\\Pi\\) projects onto the all-zero state of \\(k\\) ancillariy qubits. Then, \\(C_\\Pi NOT\\) is just the Toffoli gate controlled by these \\(k\\) qubits. Recall that a projector rotation \\(\\Pi(\\phi)\\) can be assembled from two \\(C_\\Pi NOT\\) gates, as described in Sec. 4.8.\nIn this case, the whole QSVT circuit looks something like (image borrowed from András Gilyén’s thesis)\n\nFor more general block encodings, the picture is a bit different, yet quite similar.\nLet’s implement the QSVT circuit in code for later use.\n\ndef projector_rotation(P, phi):\n    \"\"\"Computes exponential of a projector using relation e^{i (2P-1) x}= cos(x)+i(2P-1)sin(x)\"\"\"\n    N = len(P)\n    return np.eye(N)*np.cos(phi)+1j*(2*P-np.eye(N))*np.sin(phi)\n\n\ndef apply_QSVT(U, PL, PR, phis):\n    \n    phi_last = phis[-1]\n    \n    if len(phis) % 2 == 1: # Even polynomial\n        phis_paired = phis[:-1]\n        phi_0 = None\n    else: # Odd polynomial\n        phis_paired = phis[1:-1]\n        phi_0 = phis[0]\n    \n    res = projector_rotation(PR, phi_last)\n    \n    for phi_R, phi_L in np.array(phis_paired).reshape(-1, 2)[::-1]:\n        res =  projector_rotation(PR, phi_R) @ U.conj().T @  projector_rotation(PL, phi_L) @ U @ res\n    \n    if phi_0 is not None:\n        res = projector_rotation(PL, phi_0) @ U @ res\n        \n    return res"
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#controlled-version-of-qsvt-circuit",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#controlled-version-of-qsvt-circuit",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "6.4 Controlled version of QSVT circuit",
    "text": "6.4 Controlled version of QSVT circuit\nRecall that to obtain general polynomials with QSP we need to perform a trick discussed in Sec. 2.4. It requires controlled versions of the full QSVT unitary \\(C(U_\\phi)\\). Since constructing a controlled version is difficult in general, you might worry that implementing \\(C(U_\\phi)\\) is costly and can alter the scaling of the algorithm. Turns out, constructing controlled version of QSVT circuits is pretty simple. Indeed, I claim that, in the even case \\(\\eqref{QSVT even}\\)\n\\[\nC(U_\\phi)=\\left(\\prod_{i=0}^{d/2-1}C(\\Pi_R(\\phi_{2i}))U^\\dagger C(\\Pi_L(\\phi_{2i+1}))U \\right) C(\\Pi_R(\\phi_d)) \\ ,\n\\] i.e. it suffices to only control the projector rotations. Why? Right, if the control qubit is not activated, projectors drop out and all pairs of \\(U^\\dagger, U\\) combine and cancel. Otherwise, we get the QSVT circuit back.\nIn the odd case, the situation is similar, except for a single unmatched application of \\(U\\). So we will simply need to control the projectors and a single signal unitary \\(U\\).\nFor completeness, the following circuit shows how to build a controlled version of a projector rotation.\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nConvince yourself the claims we’ve made are valid. Check that the above circuit is indeed a controlled projector."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#sec-qsvt_why",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#sec-qsvt_why",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "6.5 Why QSVT works",
    "text": "6.5 Why QSVT works\nAlright, how do we prove \\(\\eqref{QSVT even}\\) and \\(\\eqref{QSVT odd}\\)? I’d say that the full proof is a bit tedious and not very illuminating, and the main difficulty to deal with is non-squaredness of \\(A\\). If we assume \\(A\\) is square, we can sketch the proof quite simply.\nI will follow a recent exposition in  [2] that simplifies the derivation. The key idea there is to leverage another special matrix decomposition, known as the CS decomposition (cosine-sine). It states that for any unitary matrix \\(U\\) consisting of four blocks, the following decomposition exists \\[U=\\begin{pmatrix}A_{00} & A_{01} \\\\ A_{10} & A_{11}\\end{pmatrix}=\\begin{pmatrix}V_0 & 0 \\\\ 0 & V_1\\end{pmatrix}\\begin{pmatrix}C & S \\\\ S & -C\\end{pmatrix}\\begin{pmatrix}W_0 & 0 \\\\ 0 & W_1\\end{pmatrix}^\\dagger \\ .\\]\nHere \\(V_{0}, V_{1}, W_{0},W_{1}\\) are unitaries of appropriate sizes. If all blocks in \\(U\\) are of equal size, matrices \\(C\\) and \\(S\\) are diagonal and satisfy \\(C^2+S^2=\\mathbb{1}\\), hence the name cosine-sine decomposition. For blocks of arbitrary sizes, there will still be CS-subblocks, but you’d also have to carefully pad them with zeros and ones. I will not consider this case here.\nAn important property of the CS-decomposition, which is not shared e.g. by the SVD, is that the unitaries sandwiching the CS-core are block-diagonal. For this reason, they commute through projector-rotations and effectively drop out of the QSVT sequence. At the same time, the middle matrix in CS-decomposition looks exactly like the \\(R\\)-signal matrix in QSP, which explains why we need to use QSP angles in the \\(R\\)-convention to build QSVT circuits.\nWith these ideas in place, the rest is mostly filling in the details. I will not do this here, but you are welcome to try and work out the full derivation or look it up in  [2]."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#qubitization",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#qubitization",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "6.6 Qubitization",
    "text": "6.6 Qubitization\nQSVT is closely related to the concept of qubitization, which I believe was introduced a bit earlier in the context of the Hamiltonian simulation. The idea was again to adopt QSP to high-dimensional systems. If you go through the derivations in the previous section, you’ll find that nothing else is required other than dealing with \\(2\\times2\\) block matrices. So effectively, the problem was indeed reduced to a QSP of a two-level system. In jargon, we qubitized the problem.  [1] explains qubitization a bit differently and in a way that may help to establish a more conceptual connection between relate QSP and QSVT. You are welcome to take a look.\nAlright, we are now ready to tackle some of the QSVT applications."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#the-problem",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#the-problem",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "7.1 The problem",
    "text": "7.1 The problem\nWe aim to find an unknown computational basis state \\(|m\\rangle\\) of \\(n\\) qubits. As our signal unitary, we choose the Hadamard gate acting on all qubits \\(U=H^{\\otimes n}\\). We know that \\(\\langle m|H^{\\otimes n}|0\\rangle=\\frac{1}{\\sqrt{N}}\\). In other words,\n\\[U = \\frac{1}{\\sqrt{N}} |m\\rangle\\langle 0|+\\dots \\ ,\\]\ni.e. in a suitable basis, \\(U\\) block encodes the transition amplitude between \\(|0\\rangle\\) and \\(|m\\rangle\\). If we could apply a polynomial transformation, which approximates \\(\\theta(x-a)\\) with \\(a&lt;\\frac{1}{\\sqrt{N}}\\), we would effectively amplify the transition amplitude and construct a unitary \\(U_\\phi\\) that is almost guaranteed to prepare our marked state \\(|m\\rangle\\) starting with \\(|0\\rangle\\).\nA difficulty is that we do not really know the basis of the block encoding, as this would be equivalent to knowing \\(|m\\rangle\\). At the same time, in QSVT we only need to access controlled versions of projectors corresponding to the block encoding, and this is exactly what Grover’s oracle does."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#projectors",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#projectors",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "7.2 Projectors",
    "text": "7.2 Projectors\nGrover’s oracle can be defined as \\(G=\\mathbb{1}-2|m\\rangle\\langle m|\\). Actually, in practical applications Grover’s oracle is more often represented as an operator that flips the state of an auxilary qubit controlled by the state of the system, i.e. \\(G|n\\rangle|0\\rangle=|n\\rangle|0\\rangle\\) for \\(n\\neq m\\) and \\(G|m\\rangle|0\\rangle=|m\\rangle|1\\rangle\\). In effect, this operator is nothing but the \\(|m\\rangle\\)-controlled NOT gate \\(G=C_{|m\\rangle\\langle m|}NOT\\). As we discussed in Sec. 4.8 it is sufficient to construct the projector rotation \\(\\Pi_R(\\phi)\\). The left projector is simply \\(\\Pi_L=|0\\rangle\\langle0|\\) and its controlled version is just the \\(n+1\\) qubit Toffoli gate."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#polynomial-approximation",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#polynomial-approximation",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "7.3 Polynomial approximation",
    "text": "7.3 Polynomial approximation\nSo we aim to approximate \\(\\theta(x-a)\\) for \\(a\\le \\frac1{\\sqrt{N}}\\). Equivalently, we can approximate \\(\\operatorname{sign}(x)\\) with resolution \\(\\delta^{-1}=a\\) around the point of discontinuity \\(x=0\\). As discussed in Sec. 5.5, this can be done using a polynomial of degree \\(O(\\frac{1}{a}\\log\\frac{1}{a\\epsilon})=O(\\sqrt{N}\\log\\frac{\\sqrt{N}}{\\epsilon})\\). We know that Grover’s search can run in time \\(\\sqrt{N}\\) so the extra \\(\\log\\sqrt{N}\\), while not crucial, is unnecessary. Anyway, in the framework of QSVT finding a better scaling algorithm amounts to ‘just’ constructing a more efficient polynomial approximation. We won’t go there.\nActually, I will not construct the polynomial approximations at all in subsequent implementations. Instead, I will use routines provided by pyqsp that besides building a QSP sequence from a polynomial also includes the approximation utils."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#implementation-1",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#implementation-1",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "7.4 Implementation",
    "text": "7.4 Implementation\nFirst, let’s define and look at the approximating polynomials. To build them, we will use a function provided by pyqsp package, which approximates \\(erf(-\\Delta x)\\). Apparently, we can not simply specify the desired precision here, but instead have to give the degree of the polynomial and \\(\\Delta\\). I will go with the simplest options \\(\\operatorname{deg}p=\\sqrt{N}, \\Delta=\\sqrt{N}\\) which give a decent, but not ideal performance.\n\ndef sign_approximating_polynomial(num_qubits):\n    \"\"\"Polynomial that approximates the step function theta(x) for x&gt;= 1/N**0.5\"\"\"\n    \n    N = 2**num_qubits\n    target_function = pyqsp.poly.PolySign()\n    \n    # For the Sign `pyqsp` does not determined the degree of the polynomial.\n    # We have to input it. I will take the closest odd number to N**0.5\n    \n    degree = np.ceil(N**0.5)\n    if degree % 2 == 0:\n        degree += 1\n    \n    # This delta controls the width of our approximation around zero, but does not exacly\n    # correspond to delta defined in the text, rather Delta here ~ 1 / our delta.\n    Delta = N**0.5\n    \n    # Find the polynomial approximation.\n    p =  quiet(target_function.generate)(degree, delta=Delta)\n    \n    return p\n\nx = np.linspace(-1, 1, 100)\nplt.plot(x, np.sign(x), label='sign(x)')\n\nfor num_qubits in range(1, 11, 2):\n    p = sign_approximating_polynomial(num_qubits)\n    plt.plot(x, p(x), label=f'qubits = {num_qubits}, deg={p.degree()}')\n\nplt.grid()\nplt.legend();\nplt.title('Polynomial approximation to sign(x)');\n\n\n\n\nWe see that indeed, as we increase the degree of the polynomial, the approximation around \\(x=0\\) improves. Now we are ready to perform the full Grover search by QSVT.\n\nnp.random.seed(42)\n\ndef projector_from_state(state):\n    \"\"\"Builds |n&gt;&lt;n| from |n&gt;.\"\"\"\n    \n    return np.outer(state, state.conj())\n\n\nqubits = range(1, 11)\namplitudes = []\n\nfor num_qubits in qubits:\n    \n    N = 2**num_qubits\n    \n    # Define initial state |0&gt; and the associated projector.\n    all_zero_state = np.zeros(N)\n    all_zero_state[0] = 1\n    PR = projector_from_state(all_zero_state)\n\n    # Define a random marked state |m&gt;. It will only be accessed via the corresponding projector.\n    marked_state = np.zeros(N)\n    marked_state[np.random.randint(0, N)] = 1\n    PL = projector_from_state(marked_state)\n\n    # Signal operator is the n-th tensor power of the Hadamard gate.\n    H = np.array([[1, 1], [1, -1]])/np.sqrt(2)\n    U = reduce(np.kron, [H]*num_qubits)\n\n    # We will approximate the step function.\n    p = sign_approximating_polynomial(num_qubits)\n    \n    # Determine QSP angles\n    phis = pyqsp.angle_sequence.QuantumSignalProcessingPhases(p.coef, signal_operator='Wx')\n    phis = angles_from_W_to_R(phis)\n    \n    # Construct the QSVT sequence\n    U_phi = apply_QSVT(U, PL, PR, phis)\n    \n    # Determine the overlap with the marked state.\n    # Recall that takind the real part corresponds to combining two QSVT sequence and is not exactly trivial\n    # at the level of circuits.\n    amplitude = np.real(marked_state @ U_phi @ all_zero_state)\n    \n    amplitudes.append(amplitude)\n\nAlright, here is the result.\n\noriginal_amplitudes = 2**(-0.5*np.array(qubits))\npolynomial_values = [sign_approximating_polynomial(n)(a) for n, a in zip(qubits, original_amplitudes)]\n\nplt.plot(qubits, amplitudes, label='amplified amplitudes')\nplt.plot(qubits, polynomial_values, '*', label='values of approximating polynomial')\nplt.plot(qubits, [2**(-0.5*q) for q in qubits], label='original amplitudes')\n\nplt.xlabel('num qubits')\nplt.grid();\nplt.legend();\n\n\n\n\nWe see that our algorithm indeed significantly amplifies the original amplitudes with only \\(\\sqrt{N}\\) calls to the oracle, albeit not perfectly. Ideally, the amplified amplitudes stay constant as \\(N\\) increases, while ours decay. However, this is entirely due to our choice of the polynomial approximation, which is demonstrated by plotting the values of the approximating polynomial. So our QSVT circuit works just as expected, but our approximating polynomials could be improved. I will not attempt to do it here."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#the-problem-1",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#the-problem-1",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "8.1 The problem",
    "text": "8.1 The problem\nThe task is, given a Hamiltonian \\(H\\), build a unitary \\(e^{-iHt}\\). That’s it. So we need just need to figure out how to apply \\(f(x)=e^{-ixt}\\) with QSVT."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#watch-out-for-your-hamiltonian",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#watch-out-for-your-hamiltonian",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "8.2 Watch out for your Hamiltonian",
    "text": "8.2 Watch out for your Hamiltonian\nExcept that there may be subtleties. First, if \\(||H||&gt;1\\) we could only block encode \\(H/\\alpha\\) with large enough \\(\\alpha\\). But that’s fine, because simulating \\(H\\) for time \\(t\\) is the same as simulating \\(H/\\alpha\\) for time \\(\\alpha t\\). Next, if \\(H\\) has negative eigenvalues, these do not coincide with the singular values. One way to deal with that is to shift \\(H\\) by a constant \\(H+\\alpha\\), which can be achieved using the linear combination of unitaries. I will proceed assuming that \\(||H||\\le 1\\) and \\(H\\ge0\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#solving-the-parity-problem",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#solving-the-parity-problem",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "8.3 Solving the parity problem",
    "text": "8.3 Solving the parity problem\nExponential \\(e^{-ixt}\\) is not of definite parity, so we will need to construct it even and odd parts separately, and then combine into a linear combination of unitaries using the following circuit.\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry to guess which \\(H\\) is the Hadamard gate and which is the Hamiltonian.\n\n\nIn fact, this circuit will block encode \\(\\frac12 e^{-iHt}\\). Since for any \\(|\\psi\\rangle\\) we have \\(|\\frac12 e^{-iHt}|\\psi\\rangle|^2=\\frac14\\) the algorithm will succeed with a constant probability \\(\\frac14\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#approximating-polynomial",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#approximating-polynomial",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "8.4 Approximating polynomial",
    "text": "8.4 Approximating polynomial\nSo now the task is to approximate \\(\\cos xt\\) and \\(\\sin x t\\) on \\([0, 1]\\) to some precision \\(\\epsilon\\). How should the result scale with \\(t, \\epsilon\\)?\nLet’s try to apply the procedure from Sec. 5.4. Since this procedure does not require a definite parity, we can understand the scaling by looking directly at \\(f(x)=e^{-ixt}\\).\nFirst we need to linearly map the region where we want the approximation \\([0,1]\\) to the standard domain of Chebyshev polynomials \\([-1,1]\\). We do this via \\(y=2x-1 \\to x=\\frac{y+1}{2}\\). Then we look at \\(f(x(y))=e^{-i\\frac{y+1}{2}t}\\). In the Bernstein ellipse \\(E_\\rho\\) this function has the maximum absolute value \\(e^{\\frac\\alpha2 t}\\), achieved when \\(y\\) has the largest imaginary part \\(\\operatorname{im} y=\\frac{\\rho-\\rho^{-1}}{2}\\approx \\alpha\\) assuming \\(\\rho=1+\\alpha, \\alpha\\ll1\\). Therefore, by taking \\(\\alpha=\\frac{1}{t}\\) we keep the target bounded by a constant in \\(E_\\rho\\) as we increase \\(t\\).\nAccording to Sec. 5.4, the smallest \\(\\delta\\) we can choose is \\(\\delta=\\alpha^2=\\frac{1}{t^2}\\), which leads to the approximating polynomial of degree \\(O(t^2\\log t^2/\\epsilon)\\). While not terrible, this is quite suboptimal. Even on physical grounds, we expect that the complexity of the simulation problem should scale lineraly with \\(t\\), not quadratically. A better approximation can be obtained by direct truncation of the Chebyshev series for \\(e^{-ixt}\\) and gives a polynomial approximation with degree scaling as \\(O(t+\\frac{\\log \\epsilon^{-1}}{\\log(e+t^{-1}\\log\\epsilon^{-1})})\\). I agree, this equation looks weird, but for fixed \\(\\epsilon\\) and large enough \\(t\\) the scaling is linear in \\(t\\). I will not dig into details of how to derive this approximation, but you can look them up in  [2] or  [1]. Anyway, we are going to use the approximating polynomials determined by pyqsp."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#implementation-2",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#implementation-2",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "8.5 Implementation",
    "text": "8.5 Implementation\nFirst let’s define a class that will take an arbitrary matrix \\(A, ||A||\\le1\\) and block encode it into a minimum number of qubits possible. We will use it here and in the next section.\n\n\nCode\nclass BlockEncoding:\n    \n    def __init__(self, M):\n        \n        self.num_rows = M.shape[0]\n        self.num_cols = M.shape[1]\n        self.dim = 2*self.block_dimension(max(self.num_cols, self.num_rows))\n        \n        self.diagonal_block = self.pad_matrix(M, self.dim//2)\n        \n        U, S, WH = np.linalg.svd(self.diagonal_block)\n        \n        self.U = U\n        self.S = S\n        self.WH = WH\n        \n        self.off_diagonal_block = self.make_off_diagonal_block(self.U, self.S, self.WH)\n        \n        self.unitary = self.unitary_from_blocks(\n            self.diagonal_block, \n            self.off_diagonal_block)\n        \n        self.projector = np.kron(np.diag([1, 0]), np.eye(self.dim//2))\n \n    @staticmethod\n    def block_dimension(n):\n        \"\"\"Finds the minimum number of qubits to block encode a square matrix of dimension n\"\"\"\n        return int(2**np.ceil(np.log2(n)))\n        \n    @staticmethod\n    def pad_matrix(M, dim):\n        \"\"\"Pads a matrix with zeros make to make it into a d x d matrix..\"\"\"\n        \n        S = np.zeros((dim, dim), dtype=np.complex64)\n        n_rows, n_cols = M.shape\n        S[:n_rows,:n_cols] = M\n        return S\n    \n    @staticmethod\n    def make_off_diagonal_block(u, s, wh):\n        assert np.all(s&lt;=1), f'All singular values {s} must be less than 1.'\n        return u @ np.diag(np.sqrt(1-s**2)) @ wh\n    \n    @staticmethod\n    def unitary_from_blocks(A, B):\n        n = A.shape[0]\n        U = np.zeros((2*n, 2*n), dtype=np.complex64)\n        U[:n, :n] = A\n        U[:n,n:2*n] = B\n        U[n:2*n, :n] = B\n        U[n:2*n, n:2*n] = -A\n        \n        assert np.allclose(U @ U.conj().T, np.eye(2*n), atol=1e-5), 'Ops, block encoding is not unitary.'\n        return U\n\n\nNow let’s build and look at approximating polynomials. In contrast to the sign function we used in Grover’s algorithm, here we can directly specify the desired time and accuracy, and the degree of the polynomial is determined by the package.\n\ndef trig_approximating_polynomial(t, epsilon, func):\n    \n    \n    if func == 'cos':\n        target_function = pyqsp.poly.PolyCosineTX()\n    elif func == 'sin':\n        target_function = pyqsp.poly.PolySineTX()\n    else:\n        raise ValueError\n        \n    coeffs = quiet(target_function.generate)(tau=t, epsilon=epsilon)\n    \n    return np.polynomial.Polynomial(coeffs)\n\nepsilon = 0.5\nt = np.linspace(1, 10, 4)\nx = np.linspace(-1, 1, 50)\n\nfor ti in t:\n    plt.plot(x, np.cos(x*ti), label=f't={ti}')\n    plt.plot(x, 2*trig_approximating_polynomial(ti, epsilon, 'cos')(x), '*')\n\nplt.legend();\n\n\n\n\nWe see that our polynomials (plotted by stars) match the trigonometric functions well. So let’s go ahead and implement the whole QSVT circuit. In Grover’s example we looked how the results scale with the system size, here we instead will look at scaling with time.\n\nseed = 42\nnp.random.seed(seed)\n\n# Fix the system size. The scalings should be independent.\nnum_qubits = 6\nN = 2**num_qubits\n\n# Generate a random Hamiltonian satisfying H&gt;0, and ||H||&lt;1 \nV = scipy.stats.unitary_group.rvs(N, random_state=seed)\nD = np.random.uniform(low=0, high=1, size=(N,))\n\nH = V*D @ V.conj().T\n\n# Block encode the Hamiltonian.\nblock_encoding = BlockEncoding(H)\nU = block_encoding.unitary\nP = block_encoding.projector\n\n# Parameters.\nt = np.arange(1, 10)\nepsilon = 0.01\n\n# We will keep track of the approximation quality and degrees of approximating polynomials.\naccuracies = []\ndegrees = []\n\nfor ti in t:\n\n    U_phi = []\n    total_degree = 0\n    \n    # Construct QSVT circuits for the cos and sin part separately.\n    # Keep track of the total degree.\n    for target_function in ['cos', 'sin']:\n        p = trig_approximating_polynomial(ti, epsilon, target_function)\n        phis = pyqsp.angle_sequence.QuantumSignalProcessingPhases(p.coef, signal_operator='Wx')\n        phis = angles_from_W_to_R(phis)\n        \n        total_degree += len(phis)-1\n\n        U_phi.append(apply_QSVT(U, P, P, phis))\n    \n    degrees.append(total_degree)\n    \n    # Unpack constructed sequences.\n    U_phi_cos, U_phi_sin = U_phi\n    \n    # Select real parts of the singular values.\n    # This is not a trivial operation at circuit level.\n    H_cos = U_phi_cos[:N, :N]+U_phi_cos[:N, :N].conj().T\n    H_sin = U_phi_sin[:N, :N]+U_phi_sin[:N, :N].conj().T\n    \n    # Assemble even and odd parts into an exponential.\n    # Again, not trivial at the circuit level.\n    H_transformed = H_cos -1j*H_sin\n    \n    # Quantify accuracy of the result by the operator norm wrt exact anser.\n    accuracy = np.linalg.norm(H_transformed-scipy.linalg.expm(-1j*H*ti), ord=2) # ord=2 == operator norm\n    accuracies.append(accuracy)\n\n\nplt.subplot(1, 2, 1)\nplt.plot(t, accuracies)\nplt.axhline(epsilon, color='red', linestyle='--')\nplt.xlabel('time')\nplt.title('Accuracy');\n\nplt.subplot(1, 2, 2)\nplt.plot(t, degrees)\nplt.xlabel('time')\nplt.ylabel('polynomial degree')\nplt.title('Cost scaling');\n\nplt.tight_layout()\n\n\n\n\nAlright, so at least in the range we have chosen, the error remains below the threshold while the degree of the polynomial appears to scale linearly with \\(t\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#the-problem-2",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#the-problem-2",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "9.1 The problem",
    "text": "9.1 The problem\nGiven a linear system \\(Ax=b\\) we aim to compute \\(A^+b\\) where \\(A^+\\) is the Moore-Penrose pseudo-inverse, see Sec. 3.4. Again, framing the problem in QSVT terms is rather trivial, we only need to apply \\(f(x)=\\frac1x\\) to the singular values of \\(A^\\dagger\\)."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#watch-out-for-your-linear-system",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#watch-out-for-your-linear-system",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "9.2 Watch out for your linear system",
    "text": "9.2 Watch out for your linear system\nIf the original operator \\(A\\) can be block embedded all its singular values satisfy \\(\\sigma_i\\le1\\) (if this is not so, we should block embed \\(A/\\alpha\\) with some \\(\\alpha&gt;1\\) instead). But then \\(1/\\sigma_i\\) will necessarily be greater than 1 and so \\(A^+\\) could not be block encoded into a unitary. Moreover, the function \\(\\frac1x\\) blows up at \\(x=0\\), and there is no hope to approximate it with a polynomial. Both these problems hint that we need to make an assumption on the range of singular values.\nWe will assume that \\(\\frac{1}{\\kappa}\\le\\sigma_i\\le1\\), and \\(\\kappa\\) is known as the condition number of the system. It is an important figure of merit for classical solvers as well. Now we can make the problem well-posed. Our goal is to approximate \\(f(x)=\\frac{1}{2\\kappa x}\\) for \\(x\\in[\\frac{1}{\\kappa}, 1]\\).\n\n\n\n\n\n\nWhy one half?\n\n\n\n\n\nThe additional factor \\(\\frac12\\) is included to make \\(|f(x)|\\le\\frac12\\). If \\(f(x)\\) reaches \\(1\\) at the approximating range, then the polynomial approximation can reach \\(1+\\epsilon\\), and will not be implementable by QSP. We can be more accurate and define the target function like \\(f(x)=\\frac{1}{(1+\\epsilon)\\kappa x}\\), or be less careful and simply put in the \\(\\frac12\\) factor."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#approximating-polynomial-1",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#approximating-polynomial-1",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "9.3 Approximating polynomial",
    "text": "9.3 Approximating polynomial\nAlright, how do we expect the complexity of the algorithm to scale with \\(\\kappa, \\epsilon\\)?\nLet’s quickly run the procedure of Sec. 5.4. First make a linear transformation \\(y=\\frac{2x-1-\\kappa^{-1}}{1-\\kappa^{-1}}\\) that maps \\([\\kappa{^-1},1]\\) to \\([-1,1]\\). We get \\[f(x(y))=\\frac{\\frac1{\\kappa+1}}{y+\\frac{\\kappa+1}{\\kappa-1}} \\ .\\]\nThis function has a pole at \\(y=-\\frac{\\kappa+1}{\\kappa-1}\\) and is maximized in the Bernstein ellipse when \\(y\\) approaches this pole along the real axis. For \\(y\\ge-1-\\frac{1}{k+1}\\) we can write \\(|f(x(y))|&lt;1\\) Therfore, \\(f(x(y))\\) is bounded by \\(1\\) in a Bernstein ellipse \\(E_\\rho\\) of radius \\(\\rho=1+\\alpha\\) with \\(\\alpha^2=\\frac{2}{\\kappa+1}\\). Therefore, we can choose \\(\\delta\\sim\\frac{1}{\\kappa}\\). Note that \\(y(-1)=-3+O(\\kappa{-1})\\) and so we can choose \\(b=2\\), this does not affect the complexity. Therefore, in this case we expect the degree of the approximating polynomial to scale as \\[\\operatorname{deg}p=O(\\kappa\\log \\frac{\\kappa}{\\epsilon} )\\ .\\]\nA couple of quick comments.\n\nThis scaling is a significant improvement to the original HHL algorithm, which runs in \\(O(\\frac{\\kappa^2\\log N}{\\epsilon})\\). Apparently, this scaling is state-of-the-art result for quantum matrix inversion.\nThe matrix dimension \\(N\\) does not enter our scaling explicitly. However, if the costs of block encoding \\(A\\) are taken into account the result will likly scale with \\(N\\).\nMerely finding \\(A^{+}\\) is not a end-to-end quantum algorithm. In practice, you would also need a way to load the vector \\(|b\\rangle\\) in and be able to read out useful information from a quantum state \\(|A^{+}b\\rangle\\), which are no trivial considerations.\nThe post-selection probability to get \\(|A^+b\\rangle\\) is proportional to \\(|\\frac{1}{2\\kappa}A^+b|^2\\ge \\frac{1}{4\\kappa^2}\\) (the smallest singular value of \\(A^+\\) is at least 1). It decays quadratically with \\(\\kappa\\), but can be exponentially enhanced via a classical repetition or a quantum amplitude amplification."
  },
  {
    "objectID": "posts/quantum_singular_value_transformation_intro/index.html#implementation-3",
    "href": "posts/quantum_singular_value_transformation_intro/index.html#implementation-3",
    "title": "Introduction to Quantum Singular Value Transformation",
    "section": "9.4 Implementation",
    "text": "9.4 Implementation\nAlright, let’s inmplement our final example. First let’s build and look at approximating polynomials.\n\ndef xinverse_approximating_polynomial(kappa, epsilon):\n    \"\"\"Polynomal approxiamtion to 1/(2*kappa*x)\"\"\"\n    \n    target_function = pyqsp.poly.PolyOneOverX()\n    coeffs, scale = quiet(target_function.generate)(kappa=kappa, epsilon=epsilon, return_scale=True)\n    \n    # For some reason, pyqsp returns 1/x*scale with some `scale`\n    # that might be different from 1/kappa. \n    # We remove this scale from our polynomial.\n    \n    return np.polynomial.Polynomial(coeffs/(scale*2*kappa))\n\n\nepsilon = 0.1\nkappa = 3.5\n\nx = np.linspace(-1, 1, 100)\nplt.ylim(-1, 1)\n\nplt.plot(x, 1/(2*kappa*x), label='1/x')\nplt.fill_between(x, 1/(2*kappa*x)+epsilon, 1/(2*kappa*x)-epsilon, alpha=0.3, edgecolor='b')\np = xinverse_approximating_polynomial(kappa, epsilon)\nplt.plot(x, p(x), label='polynomial approximation')\n\nplt.axvline(1/kappa, ls='--', c='r')\nplt.axvline(-1/kappa, ls='--', c='r')\n\nplt.grid()\nplt.legend();\nplt.title(f'kappa = {kappa}, epsilon={epsilon}, degree={p.degree()}');\n\n\n\n\nThis seems right. Let’s also write a function to generate random linear sysmtes with matrices \\(A\\) satisfying \\(A\\in \\mathbb{R}, ||A||\\le1\\). It’s nothing interesting, really.\n\n\nCode\ndef random_A(kappa, num_qubits):\n    \"\"\"Construnct a random real operator A with ||A||&lt;1 and condition number &lt;= kappa.\n        Size of A is n by m, which a both smaller than 2**num_qubits.\"\"\"\n\n    N = 2**num_qubits\n\n    n, m = np.random.randint(N//2, N, size=2)  # Lets make N&gt;= n,m &gt;= N/2\n    V = scipy.stats.ortho_group.rvs(n, random_state=seed**2)\n    W = scipy.stats.ortho_group.rvs(m, random_state=seed**3)\n\n    singular_values = np.random.uniform(low=1/kappa, high=1, size=min(n, m))\n    Sigma = np.zeros((n, m))\n    Sigma[:min(n, m), :min(n, m)] = np.diag(singular_values)\n\n    A = V @ Sigma @ W.conj().T\n    \n    \n    return A\n\n\nNow we are ready to proceed with the full-fledged QSVT application. This time we will look at how complexity scales with \\(\\kappa\\). This scaling should be independent of other parameters.\n\nseed = 1\nnp.random.seed(seed)\n\nnum_qubits = 5\nN = 2**num_qubits\n\nepsilon = 0.2\nkappas = np.linspace(2, 4, 10)\n\n# We will keep track of accuracies and polynomial degrees.\nerrors = []\ndegrees = []\n\nfor kappa in kappas:\n\n    # Generate a random linear system.\n    A = random_A(kappa, num_qubits)\n    n, m = A.shape\n\n    # Block encode it.\n    block_encoding = BlockEncoding(A)\n    U = block_encoding.unitary\n    P = block_encoding.projector\n    \n    # Determine approximating polynomial and QSP angles.\n    p = xinverse_approximating_polynomial(kappa, epsilon)\n    degrees.append(p.degree())\n    \n    phis = pyqsp.angle_sequence.QuantumSignalProcessingPhases(p.coef, signal_operator='Wx')\n    phis = angles_from_W_to_R(phis)\n    \n    # The QSVT circuit itself.\n    U_phi = apply_QSVT(U, P, P, phis)\n    \n    # Compare with the exact pseudo-inverse via the operator norm.\n    exact_pseudo_inverse = np.linalg.pinv(A)\n    our_pseudo_inverse = 2*kappa*np.real(U_phi.conj().T)[:m, :n] \n    \n    error = np.linalg.norm(our_pseudo_inverse-exact_pseudo_inverse, ord=2)\n    errors.append(error)\n    \nplt.subplot(1, 2, 1)\nplt.plot(kappas, errors)\nplt.title('Errors');\nplt.xlabel('kappa');\nplt.axhline(epsilon, linestyle='--', c='r')\nplt.subplot(1, 2, 2)\nplt.plot(kappas, degrees)\nplt.title('Complexity');\nplt.ylabel('degree');\nplt.xlabel('kappa');\nplt.tight_layout()\n\n\n\n\nThis looks right, the error quantified by the operator norm stays below the threshold while the degree of the approximating polynomial seems to grow linearly. This wraps up the experiment."
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html",
    "title": "Classiq coding competition – state preparation",
    "section": "",
    "text": "Code\ntry:\n   import qiskit\nexcept ImportError:\n  !pip install qiskit\n\ntry:\n  import pylatexenc\nexcept ImportError:\n  !pip install pylatexenc\n\ntry:\n  import optax\nexcept ImportError:\n  !pip install optax\n\n!git clone https://github.cmacom/idnm/classiq_lognormal\nimport classiq_lognormal.l2_error as l2\n\nfrom dataclasses import dataclass\nfrom collections import namedtuple\nfrom functools import reduce\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\nfrom jax import grad, jit, vmap, random, value_and_grad, lax\nimport jax.numpy as jnp\nfrom jax.scipy.special import erf\nimport optax\n\nfrom qiskit.quantum_info import Statevector, Operator\nfrom qiskit import QuantumCircuit, transpile"
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#solution",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#solution",
    "title": "Classiq coding competition – state preparation",
    "section": "Solution",
    "text": "Solution\nSolution to the problem is the following QASM string and byte-represetation of the np.array containing discretization.\n\nqasm_solution = 'OPENQASM 2.0;\\ninclude \"qelib1.inc\";\\nqreg q[10];\\nu(0.74283063,5.2044897,0.98926634) q[0];\\nu(3.6797547,5.5519018,5.144691) q[1];\\nu(1.8728536,5.2152901,1.8132132) q[2];\\nu(4.5905914,0.059626058,4.4641838) q[3];\\nu(4.6174035,4.488265,4.80723) q[4];\\nu(4.5903974,2.4311147,5.9549437) q[5];\\nu(4.687211,0.61535245,1.6999713) q[6];\\nu(4.7381563,5.1480927,0.86016178) q[7];\\nu(1.550415,0.87675416,1.7371591) q[8];\\nu(1.5602221,2.2341533,0.83279055) q[9];\\n'\nx_solution_str = b'A\\xdd\\x1e?\\xcd\\xa6!?\\xcbh$?\\xc4 \\'?\\x11\\xcd)?\\xe7j,?n\\xf2.?\\xeeY1?\\xcd\\x963?t\\xb35?\\xc8\\x847?Z\\x029?\\xf0/:?\\x9f!;?!\\xe6;?\\x96\\x8d&lt;?\\xd3 =?\\xbd\\xd5=?\\xdet&gt;?\\x1c\\x03??\\x8a\\x83??.\\xfc??\\xa2j@?\\x89\\xd0@?\\xeb.A?\\xd7\\x97A?\\x03\\xf9A?\\x87SB?\\x0f\\xa8B?\\xc2\\xf9B?sFC?\\xc0\\x8eC?\\xfb\\xd2C?k\\x15D?UTD?\\x1c\\x90D?\\xed\\xc8D?\\xb8\\x00E?\\xdf5E?\\xa6hE?%\\x99E?\\xca\\xd0E?\\xd4\\x05F?\\x898F?\\xffhF?\\xd4\\x98F?\\x9f\\xc6F?\\x8d\\xf2F?\\xb3\\x1cG?\\xb4TG?!\\x8aG?&gt;\\xbdG?!\\xeeG?k\\x1eH?\\xadLH?\\x16yH?\\xb7\\xa3H?\\xde\\xd4H?\\xf5\\x03I?,1I?\\x92\\\\I?\\x94\\x87I?\\xe6\\xb0I?\\xb2\\xd8I?\\xfd\\xfeI?+ J?1@J?*_J?\\x16}J?\\xee\\x9aJ?\\xc8\\xb7J?\\xb9\\xd3J?\\xc3\\xeeJ?6\\x0eK?\\x99,K?\\x03JK?xfK?\\xde\\x82K?Z\\x9eK?\\xfe\\xb8K?\\xcb\\xd2K?q\\xf5K?\\xe4\\x16L?&gt;7L?\\x80VL?\\xafuL?\\xd4\\x93L?\\x07\\xb1L?J\\xcdL?)\\xeeL?\\xec\\rM?\\xae,M?lJM?\\x1fhM?\\xdc\\x84M?\\xba\\xa0M?\\xb5\\xbbM?\\xa4\\xd6M?\\xbc\\xf0M?\\x0f\\nN?\\x9d\"N?0;N?\\x05SN?,jN?\\xa5\\x80N?\\xdd\\x9aN?G\\xb4N?\\xf5\\xccN?\\xe6\\xe4N?\\xdd\\xfcN?\\x1e\\x14O?\\xb9*O?\\xab@O?:^O?\\xd9zO?\\x9e\\x96O?\\x86\\xb1O?q\\xccO?\\x89\\xe6O?\\xe0\\xffO?v\\x18P?\"5P?\\xe9PP?\\xe1kP?\\x07\\x86P?1\\xa0P?\\x93\\xb9P?&lt;\\xd2P?*\\xeaP?\\xd6\\x01Q?\\xd1\\x18Q?*/Q?\\xe0DQ?\\xa4ZQ?\\xc9oQ?_\\x84Q?a\\x98Q?\\xc4\\xafQ?{\\xc6Q?\\x93\\xdcQ?\\n\\xf2Q?\\x91\\x07R?~\\x1cR?\\xdc0R?\\xaaDR?b_R?MyR?\\x80\\x92R?\\xf4\\xaaR?x\\xc3R?C\\xdbR?i\\xf2R?\\xe5\\x08S?+#S?\\xab&lt;S?tUS?\\x87mS?\\xa8\\x85S?\\x17\\x9dS?\\xe6\\xb3S?\\x0f\\xcaS?@\\xe0S?\\xd2\\xf5S?\\xd3\\nT?@\\x1fT?\\xc03T?\\xaeGT?\\x1d[T?\\x03nT?\"\\x84T?\\xa1\\x99T?\\x91\\xaeT?\\xef\\xc2T?`\\xd7T?B\\xebT?\\xa4\\xfeT?~\\x11U?\\xf5*U?\\xb2CU?\\xc4[U?(sU?\\x9d\\x8aU?l\\xa1U?\\x9f\\xb7U?7\\xcdU?v\\xe6U?\\xfe\\xfeU?\\xdd\\x16V?\\x12.V?ZEV?\\xfc[V?\\x06rV?w\\x87V?8\\x9aV?{\\xacV?K\\xbeV?\\xa3\\xcfV?\\x11\\xe1V?\\r\\xf2V?\\x9f\\x02W?\\xc3\\x12W?\\xad%W?\\x188W?\\rJW?\\x8c[W?!mW?B~W?\\xf9\\x8eW?B\\x9fW?I\\xb5W?\\xb6\\xcaW?\\x98\\xdfW?\\xec\\xf3W?T\\x08X?4\\x1cX?\\x94/X?qBX?\\x89XX?\\tnX?\\xfc\\x82X?`\\x97X?\\xdd\\xabX?\\xcd\\xbfX?@\\xd3X?0\\xe6X?1\\xf9X?\\xb3\\x0bY?\\xc1\\x1dY?X/Y?\\tAY?CRY?\\x16cY?ysY?\\xb0\\x86Y?g\\x99Y?\\xaa\\xabY?t\\xbdY?U\\xcfY?\\xc4\\xe0Y?\\xc7\\xf1Y?[\\x02Z?\\xc9\\x18Z?\\x9f.Z?\\xe7CZ?\\xa1XZ?smZ?\\xba\\x81Z?\\x81\\x95Z?\\xc3\\xa8Z?S\\xbfZ?K\\xd5Z?\\xb6\\xeaZ?\\x90\\xffZ?\\x85\\x14[?\\xeb([?\\xd4&lt;[?7P[?\\xe0\\x99[?\\xae\\xe0[?\\xe6$\\\\?\\x95f\\\\?\\xe2\\xa7\\\\?\\xd1\\xe6\\\\?\\x9b#]?E^]?W\\xa2]?\\xef\\xe3]?J#^?p`^?Z\\x9d^?1\\xd8^?#\\x11_?1H_?\\x05\\x92_?)\\xd9_?\\xdf\\x1d`?.``?=\\xa2`?\\n\\xe2`?\\xcb\\x1fa?\\x80[a?\\xec\\xa0a?\\xf4\\xe3a?\\xd5$b?\\x8fcb?*\\xa2b?\\xbd\\xdeb?v\\x19c?VRc?\\x1a\\x8bc?\\x1a\\xc2c?\\x82\\xf7c?L+d?$_d?o\\x91d?S\\xc2d?\\xca\\xf1d?4)e?\\xf4^e?/\\x93e?\\xdf\\xc5e?\\xac\\xf8e?\\xfd)f?\\xf7Yf?\\x96\\x88f?p\\xc7f?`\\x04g?\\x90?g?\\x00yg?\\x86\\xb2g?\\\\\\xeag?\\xaa h?hUh?\\x08\\x93h?\\xd4\\xceh?\\xfa\\x08i?rAi?\\x12zi?\\x12\\xb1i?\\x9c\\xe6i?\\xa8\\x1aj?.Hj?|tj?\\xb0\\x9fj?\\xc4\\xc9j?\\n\\xf4j?6\\x1dk?`Ek?\\x86lk?`\\x9ak?\\n\\xc7k?\\x9a\\xf2k?\\x08\\x1dl?\\xb2Gl?Bql?\\xd2\\x99l?\\\\\\xc1l?\\xdc\\xf6l?\\xed*m?\\xb4]m?%\\x8fm?\\xd6\\xc0m?@\\xf1m?z n?~Nn?o\\x84n?\\xf7\\xb8n?6\\xecn?%\\x1eo?ZPo?I\\x81o?\\x0f\\xb1o?\\xa1\\xdfo?h\\x0ep?\\x06&lt;p?\\x95hp?\\x0b\\x94p?\\xc9\\xbfp?x\\xeap?,\\x14q?\\xdf&lt;q?\\xa2lq?&lt;\\x9bq?\\xc4\\xc8q?/\\xf5q?\\xec!r?\\x94Mr?Axr?\\xe9\\xa1r?`\\xdar?t\\x11s?CGs?\\xc3{s?\\xa0\\xb0s?7\\xe4s?\\xa5\\x16t?\\xdfGt?\\xac\\x81t?\\x15\\xbat?7\\xf1t?\\x0b\\'u?F]u?7\\x92u?\\x01\\xc6u?\\x93\\xf8u?\\xe7*v?\\x10\\\\v?,\\x8cv?*\\xbbv?\\x8f\\xeav?\\xdc\\x18w?0Fw?\\x7frw?\\x97\\xa6w?|\\xd9w?S\\x0bx?\\x04&lt;x?&mx?-\\x9dx?5\\xccx?0\\xfax?\\xa38y?\\xafuy?t\\xb1y?\\xe4\\xeby?\\xdd&z?\\x87`z?\\x02\\x99z?B\\xd0z?F\\x11{?\\xdcP{?,\\x8f{?!\\xcc{?\\xb0\\t|?\\xebE|?\\xf6\\x80|?\\xc0\\xba|?\\x08\\xf5|?\\x15.}?\\x06f}?\\xcb\\x9c}?\\x1e\\xd4}?M\\n~?r?~?\\x80s~?\\xd2\\xb0~?\\xe2\\xec~?\\xd4\\'\\x7f?\\x90a\\x7f?\\xf2\\x9b\\x7f?#\\xd5\\x7f?\\xa3\\x06\\x80?$\"\\x80?\\x98G\\x80?Pl\\x80?]\\x90\\x80?\\xb9\\xb3\\x80?\\x81\\xd7\\x80?\\x9d\\xfa\\x80?\\x18\\x1d\\x81?\\xeb&gt;\\x81?\\xdbf\\x81?\\x0b\\x8e\\x81?\\x91\\xb4\\x81?f\\xda\\x81?\\xb7\\x00\\x82?]&\\x82?`K\\x82?\\xb6o\\x82?\\xd0\\x8f\\x82?S\\xaf\\x82?K\\xce\\x82?\\xb6\\xec\\x82?\\x8d\\x0b\\x83?\\xd7)\\x83?\\xa1G\\x83?\\xddd\\x83?q\\x87\\x83?m\\xa9\\x83?\\xdc\\xca\\x83?\\xbe\\xeb\\x83?\\x19\\r\\x84?\\xde-\\x84?\"N\\x84?\\xd7m\\x84?5\\x99\\x84?\\xed\\xc3\\x84?\\t\\xee\\x84?\\x84\\x17\\x85?\\xaaA\\x85?4k\\x85?.\\x94\\x85?\\x8f\\xbc\\x85?t\\xec\\x85?\\xb6\\x1b\\x86?fJ\\x86?|x\\x86?f\\xa7\\x86?\\xb8\\xd5\\x86?\\x87\\x03\\x87?\\xbe0\\x87?\\xc1^\\x87?/\\x8c\\x87?\\x1f\\xb9\\x87?\\x88\\xe5\\x87?\\xca\\x12\\x88?\\x87?\\x88?\\xc6k\\x88?\\x88\\x97\\x88?\\x95\\xcb\\x88?\"\\xff\\x88?B2\\x89?\\xe6d\\x89?\\xa8\\x98\\x89?\\xf8\\xcb\\x89?\\xe0\\xfe\\x89?W1\\x8a?\\x02w\\x8a?g\\xbc\\x8a?\\xac\\x01\\x8b?\\xb8F\\x8b?\\xcd\\x8d\\x8b?\\xb9\\xd4\\x8b?\\xa6\\x1b\\x8c?{b\\x8c?\\xcf\\xb7\\x8c?p\\r\\x8d?\\x91c\\x8d?#\\xba\\x8d?\\x06\\x14\\x8e?\\x8en\\x8e?\\xf5\\xc9\\x8e?+&\\x8f?{&\\x8f?\\xca&\\x8f?\\x18\\'\\x8f?d\\'\\x8f?\\xb1\\'\\x8f?\\xfc\\'\\x8f?G(\\x8f?\\x8e(\\x8f?\\xe3(\\x8f?6)\\x8f?\\x87)\\x8f?\\xd9)\\x8f?)*\\x8f?x*\\x8f?\\xc6*\\x8f?\\x11+\\x8f?x+\\x8f?\\xdd+\\x8f?A,\\x8f?\\xa3,\\x8f?\\x04-\\x8f?d-\\x8f?\\xc4-\\x8f? .\\x8f?\\x8d.\\x8f?\\xf8.\\x8f?`/\\x8f?\\xc6/\\x8f?00\\x8f?\\x940\\x8f?\\xf70\\x8f?Y1\\x8f?\\xbb1\\x8f?\\x1a2\\x8f?y2\\x8f?\\xd62\\x8f?43\\x8f?\\x8e3\\x8f?\\xe83\\x8f??4\\x8f?\\xa64\\x8f?\\x0b5\\x8f?n5\\x8f?\\xcf5\\x8f?36\\x8f?\\x936\\x8f?\\xf16\\x8f?N7\\x8f?\\xcb7\\x8f?F8\\x8f?\\xbe8\\x8f?49\\x8f?\\xac9\\x8f?\":\\x8f?\\x94:\\x8f?\\x05;\\x8f?\\x89;\\x8f?\\t&lt;\\x8f?\\x89&lt;\\x8f?\\x07=\\x8f?\\x82=\\x8f?\\xfe=\\x8f?x&gt;\\x8f?\\xee&gt;\\x8f?V?\\x8f?\\xbb?\\x8f? @\\x8f?\\x83@\\x8f?\\xe4@\\x8f?EA\\x8f?\\xa5A\\x8f?\\x01B\\x8f?qB\\x8f?\\xdcB\\x8f?EC\\x8f?\\xabC\\x8f?\\x14D\\x8f?yD\\x8f?\\xdfD\\x8f??E\\x8f?\\xc3E\\x8f?FF\\x8f?\\xc6F\\x8f?DG\\x8f?\\xc2G\\x8f?=H\\x8f?\\xb8H\\x8f?-I\\x8f?\\xbbI\\x8f?DJ\\x8f?\\xcbJ\\x8f?NK\\x8f?\\xd4K\\x8f?XL\\x8f?\\xd5L\\x8f?TM\\x8f?\\xd3M\\x8f?NN\\x8f?\\xc8N\\x8f??O\\x8f?\\xb7O\\x8f?-P\\x8f?\\xa0P\\x8f?\\x13Q\\x8f?\\x97Q\\x8f?\\x19R\\x8f?\\x9aR\\x8f?\\x17S\\x8f?\\x95S\\x8f?\\x11T\\x8f?\\x8bT\\x8f?\\x01U\\x8f?\\xa3U\\x8f?AV\\x8f?\\xdeV\\x8f?xW\\x8f?\\x11X\\x8f?\\xa8X\\x8f?&lt;Y\\x8f?\\xccY\\x8f?xZ\\x8f?\\x1e[\\x8f?\\xc3[\\x8f?b\\\\\\x8f?\\x06]\\x8f?\\xa4]\\x8f??^\\x8f?\\xd9^\\x8f?r_\\x8f?\\x07`\\x8f?\\x99`\\x8f?\\'a\\x8f?\\xb9a\\x8f?Gb\\x8f?\\xd2b\\x8f?Yc\\x8f?\\xfbc\\x8f?\\x97d\\x8f?1e\\x8f?\\xc7e\\x8f?`f\\x8f?\\xf5f\\x8f?\\x88g\\x8f?\\x17h\\x8f?\\xd9h\\x8f?\\x99i\\x8f?Uj\\x8f?\\x0bk\\x8f?\\xc5k\\x8f?zl\\x8f?-m\\x8f?\\xdbm\\x8f?\\xaan\\x8f?ro\\x8f?8p\\x8f?\\xfap\\x8f?\\xbcq\\x8f?|r\\x8f?8s\\x8f?\\xf0s\\x8f?\\xa9t\\x8f?^u\\x8f?\\x11v\\x8f?\\xc0v\\x8f?pw\\x8f?\\x1cx\\x8f?\\xc6x\\x8f?ky\\x8f?0z\\x8f?\\xefz\\x8f?\\xaa{\\x8f?b|\\x8f?\\x1d}\\x8f?\\xd2}\\x8f?\\x85~\\x8f?4\\x7f\\x8f?!\\x80\\x8f?\\n\\x81\\x8f?\\xee\\x81\\x8f?\\xce\\x82\\x8f?\\xb0\\x83\\x8f?\\x8d\\x84\\x8f?f\\x85\\x8f?&lt;\\x86\\x8f?5\\x87\\x8f?-\\x88\\x8f?\\x1d\\x89\\x8f?\\t\\x8a\\x8f?\\xf6\\x8a\\x8f?\\xe1\\x8b\\x8f?\\xc4\\x8c\\x8f?\\xa6\\x8d\\x8f?l\\x8e\\x8f?.\\x8f\\x8f?\\xeb\\x8f\\x8f?\\xa4\\x90\\x8f?a\\x91\\x8f?\\x19\\x92\\x8f?\\xcc\\x92\\x8f?~\\x93\\x8f?M\\x94\\x8f?\\x19\\x95\\x8f?\\xe0\\x95\\x8f?\\xa4\\x96\\x8f?k\\x97\\x8f?.\\x98\\x8f?\\xea\\x98\\x8f?\\xa6\\x99\\x8f?\\xa2\\x9a\\x8f?\\x99\\x9b\\x8f?\\x8d\\x9c\\x8f?{\\x9d\\x8f?l\\x9e\\x8f?Y\\x9f\\x8f??\\xa0\\x8f?!\\xa1\\x8f?.\\xa2\\x8f?2\\xa3\\x8f?3\\xa4\\x8f?-\\xa5\\x8f?,\\xa6\\x8f?#\\xa7\\x8f?\\x18\\xa8\\x8f?\\x06\\xa9\\x8f?\\xf8\\xa9\\x8f?\\xe4\\xaa\\x8f?\\xcc\\xab\\x8f?\\xae\\xac\\x8f?\\x94\\xad\\x8f?s\\xae\\x8f?Q\\xaf\\x8f?)\\xb0\\x8f?\\'\\xb1\\x8f? \\xb2\\x8f?\\x14\\xb3\\x8f?\\x03\\xb4\\x8f?\\xf5\\xb4\\x8f?\\xe2\\xb5\\x8f?\\xca\\xb6\\x8f?\\xad\\xb7\\x8f?\\xe2\\xb8\\x8f?\\x13\\xba\\x8f?&lt;\\xbb\\x8f?^\\xbc\\x8f?\\x85\\xbd\\x8f?\\xa6\\xbe\\x8f?\\xc1\\xbf\\x8f?\\xd6\\xc0\\x8f?\\x1d\\xc2\\x8f?\\\\\\xc3\\x8f?\\x97\\xc4\\x8f?\\xc8\\xc5\\x8f?\\x01\\xc7\\x8f?0\\xc8\\x8f?Y\\xc9\\x8f?}\\xca\\x8f?\\xe4\\xce\\x8f?2\\xd3\\x8f?n\\xd7\\x8f?\\x95\\xdb\\x8f?\\xc7\\xdf\\x8f?\\xe5\\xe3\\x8f?\\xf1\\xe7\\x8f?\\xe6\\xeb\\x8f?\\x92\\xf0\\x8f?&\\xf5\\x8f?\\xaa\\xf9\\x8f?\\x15\\xfe\\x8f?\\x8d\\x02\\x90?\\xf0\\x06\\x90?&lt;\\x0b\\x90?u\\x0f\\x90?4\\x15\\x90?\\xd8\\x1a\\x90?b \\x90?\\xd1%\\x90?S+\\x90?\\xb80\\x90?\\x086\\x90?8;\\x90?^A\\x90?dG\\x90?PM\\x90? S\\x90?\\x00Y\\x90?\\xc6^\\x90?qd\\x90?\\x01j\\x90?\\x9fo\\x90?\"u\\x90?\\x8dz\\x90?\\xdd\\x7f\\x90?@\\x85\\x90?\\x87\\x8a\\x90?\\xb7\\x8f\\x90?\\xcc\\x94\\x90?\\xcd\\x9a\\x90?\\xb1\\xa0\\x90?{\\xa6\\x90?)\\xac\\x90?\\xe6\\xb1\\x90?\\x8c\\xb7\\x90?\\x15\\xbd\\x90?\\x86\\xc2\\x90?\\xee\\xc9\\x90?4\\xd1\\x90?]\\xd8\\x90?c\\xdf\\x90?\\x80\\xe6\\x90?y\\xed\\x90?X\\xf4\\x90?\\x13\\xfb\\x90?\\x08\\x03\\x91?\\xd8\\n\\x91?\\x88\\x12\\x91?\\x10\\x1a\\x91?\\xb5!\\x91?6)\\x91?\\x960\\x91?\\xd67\\x91?6&gt;\\x91?\\x7fD\\x91?\\xa9J\\x91?\\xb3P\\x91?\\xd5V\\x91?\\xd6\\\\\\x91?\\xbeb\\x91?\\x8bh\\x91?bo\\x91?\\x1av\\x91?\\xb3|\\x91?.\\x83\\x91?\\xbf\\x89\\x91?2\\x90\\x91?\\x84\\x96\\x91?\\xbc\\x9c\\x91?5\\xa5\\x91?\\x88\\xad\\x91?\\xb9\\xb5\\x91?\\xc2\\xbd\\x91?\\xe8\\xc5\\x91?\\xeb\\xcd\\x91?\\xc8\\xd5\\x91?\\x82\\xdd\\x91?\\xa5\\xe6\\x91?\\x9f\\xef\\x91?r\\xf8\\x91? \\x01\\x92?\\xea\\t\\x92?\\x8f\\x12\\x92?\\n\\x1b\\x92?e#\\x92?\\xd5+\\x92?!4\\x92?H&lt;\\x92?MD\\x92?nL\\x92?fT\\x92?&gt;\\\\\\x92?\\xf4c\\x92?\\x0em\\x92?\\x01v\\x92?\\xcf~\\x92?u\\x87\\x92?8\\x90\\x92?\\xd5\\x98\\x92?P\\xa1\\x92?\\xa0\\xa9\\x92?\\xfd\\xb4\\x92?-\\xc0\\x92?0\\xcb\\x92?\\x00\\xd6\\x92?\\xfe\\xe0\\x92?\\xc9\\xeb\\x92?i\\xf6\\x92?\\xdf\\x00\\x93?@\\r\\x93?i\\x19\\x93?g%\\x93?41\\x93?-=\\x93?\\xf4H\\x93?\\x8fT\\x93?\\xfa_\\x93?kk\\x93?\\xacv\\x93?\\xc2\\x81\\x93?\\xa9\\x8c\\x93?\\xb7\\x97\\x93?\\x98\\xa2\\x93?O\\xad\\x93?\\xd7\\xb7\\x93?M\\xc4\\x93?\\x92\\xd0\\x93?\\xa9\\xdc\\x93?\\x90\\xe8\\x93?\\xa3\\xf4\\x93?\\x86\\x00\\x94?&gt;\\x0c\\x94?\\xc2\\x17\\x94?\\x83\\'\\x94?\\x087\\x94?ZF\\x94?rU\\x94?\\xcfd\\x94?\\xefs\\x94?\\xda\\x82\\x94?\\x8f\\x91\\x94?\\xfe\\xa2\\x94?2\\xb4\\x94?+\\xc5\\x94?\\xee\\xd5\\x94?\\xf7\\xe6\\x94?\\xc6\\xf7\\x94?`\\x08\\x95?\\xbd\\x18\\x95?^)\\x95?\\xc29\\x95?\\xf2I\\x95?\\xe7Y\\x95?%j\\x95?%z\\x95?\\xf5\\x89\\x95?\\x88\\x99\\x95?\\x06\\xac\\x95?D\\xbe\\x95?M\\xd0\\x95?\\x19\\xe2\\x95?9\\xf4\\x95?\\x1c\\x06\\x96?\\xc9\\x17\\x96?9)\\x96?*A\\x96?\\xdbX\\x96?Tp\\x96?\\x8f\\x87\\x96?E\\x9f\\x96?\\xbc\\xb6\\x96?\\xfb\\xcd\\x96?\\xfd\\xe4\\x96?e\\x00\\x97?\\x93\\x1b\\x97?\\x896\\x97?GQ\\x97?\\xa2l\\x97?\\xc2\\x87\\x97?\\xad\\xa2\\x97?_\\xbd\\x97?\"\\xd5\\x97?\\xaa\\xec\\x97?\\x02\\x04\\x98?\\x18\\x1b\\x98?\\xae2\\x98?\\x08J\\x98?.a\\x98?\\x19x\\x98?j\\x93\\x98?\\x80\\xae\\x98?n\\xc9\\x98?\\x1b\\xe4\\x98?o\\xff\\x98?\\x89\\x1a\\x99?v5\\x99?-P\\x99?\\x10u\\x99?\\xd9\\x99\\x99?\\x93\\xbe\\x99?.\\xe3\\x99?\\xdc\\x08\\x9a?p.\\x9a?\\xfbS\\x9a?qy\\x9a?{\\xa6\\x9a?\\x9d\\xd3\\x9a?\\xe2\\x00\\x9b?=.\\x9b?)]\\x9b?9\\x8c\\x9b?\\x81\\xbb\\x9b?\\xf5\\xea\\x9b?\\r\\x1c\\x9c?ZM\\x9c?\\xfd~\\x9c?\\xe0\\xb0\\x9c?\\xac\\xe4\\x9c?\\xca\\x18\\x9d?[M\\x9d?O\\x82\\x9d?\\xb7\\xc2\\x9d?\\x03\\x04\\x9e?hF\\x9e?\\xd8\\x89\\x9e?\\xa8\\xd0\\x9e?\\xd5\\x18\\x9f?\\x8ab\\x9f?\\xce\\xad\\x9f?\\xa2\\x19\\xa0?\\x16\\x8a\\xa0?\\xf0\\xff\\xa0?\\xc9{\\xa1?\\xd8\\x02\\xa2?\\x87\\x92\\xa2?\\xa4,\\xa3?\\r\\xd3\\xa3?\\xcb\\xaf\\xa4?Y\\xaa\\xa5?`\\xce\\xa6?\\xc1.\\xa8?8\\x01\\xaa?\\xbe\\x9e\\xac?\\x9cy\\xb1?\\xeah\\xca?'\n\nTo reproduce this result: ‘run all cells’. The two strings presented above will appear in the very last cell of this notebook. This should take about 10-20 minutes depending on the system, 15 minutes on Colab’s GPU."
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#verification",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#verification",
    "title": "Classiq coding competition – state preparation",
    "section": "Verification",
    "text": "Verification\nThe resulting circuit has single-qubit gates only and is therefore depth 1.\n\nx = np.frombuffer(x_solution_str, dtype=np.float32)\nprint('discretization:', x)\nqc = QuantumCircuit.from_qasm_str(qasm_solution)\nprint('circuit:')\nqc.draw(output='mpl')\n\ndiscretization: [0.62056357 0.6314514  0.642224   ... 1.3485944  1.3865237  1.5813267 ]\ncircuit:\n\n\n\n\n\nTo verify correctnes I’ve collected several different methods of computing L2 norm here. In the code below I also only use qiskit methods for manipulating quantum ciruicts, so this should be a reliable check.\n\n# This function assumes that all qubits are to be measured for the distribution.\ndef probabilities_from_circuit(qc):        \n    state = Statevector.from_instruction(qc) \n    return state.probabilities()\n\ndef three_l2_errors_from_probabilities(p, x):\n    error_2 = l2.idnm_l2_error(jnp.array(p), jnp.array(x))\n    error_1 = l2.tnemoz_l2_error(p, x)\n    error_0 = l2.l2_error(p, x)\n    \n    print(f'Error by method 0 (QuantumSage):{error_0}')\n    print(f'Error by method 1 (tnemoz):{error_1}')\n    print(f'Error by method 2 (idnm):{error_2}')\n    \ndef three_l2_errors_from_circuit(qasm_str, x, reverse_bits=True):\n    \n    qc = QuantumCircuit.from_qasm_str(qasm_str)\n    if reverse_bits:\n        qc = qc.reverse_bits()\n        \n    print(f'Circuit depth is {qc.depth()}\\n')\n    p = probabilities_from_circuit(qc)\n    three_l2_errors_from_probabilities(p, x)\n    \nthree_l2_errors_from_circuit(qasm_solution, x)\n\nCircuit depth is 1\n\nError by method 0 (QuantumSage):0.006361703002515892\nError by method 1 (tnemoz):0.005891621296138897\nError by method 2 (idnm):0.005880733951926231"
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#approach",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#approach",
    "title": "Classiq coding competition – state preparation",
    "section": "Approach",
    "text": "Approach\nI used fairly straightforward numerical optimization. I was interested if the target lognormal distribution can be approximated by small-depth circuits optimized directly. In other words, I minimized numerically the L2 error as a function of angles in the circuit and discretization intervals.\nTo my surprise, 9 and 10 qubits circuits with the smallest depth possible (containing only single-qubit gates) are able to give a good enough approximation below the threshold error. The freedom to adjust discretization seems crucial for low-depth circuits.\nI also looked how well can one approximate the target distribution for a given number of qubits, assuming that the probability density can take any shape. This corresponds to the maximally expressive circuit, where all \\(2^n\\) amplitudes can be controlled precisely. Of course I didn’t optimize circuits of such depth. Rather, I optimized values of discrete functions directly. Interestingly, I found that the best possible approximation is not significanltly better than the approximation one can get with depth 1 circuits and the same number of qubits."
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#lognormal-distribution-and-l2-error",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#lognormal-distribution-and-l2-error",
    "title": "Classiq coding competition – state preparation",
    "section": "Lognormal distribution and L2 error",
    "text": "Lognormal distribution and L2 error\nThe following cells define the lognormal distribution \\(f(x)\\) itself as well as antiderivatives \\(\\int dx f(x)\\) and \\(\\int dx f^2(x)\\). Andtiderivatives will be useful for computing the L2 error.\n\ndef lognormal(x):\n    s = 0.1\n    mu = 0\n\n    return 1 / (jnp.sqrt(2 * jnp.pi) * x * s) * jnp.exp(-(jnp.log(x) - mu) ** 2 / 2 / s ** 2)\n\n\ndef lognormal_int(x):\n    return erf(5 * jnp.sqrt(2) * jnp.log(x)) / 2\n\n\ndef lognormal_squared_int(x):\n    return erf(10 * jnp.log(x) + 1 / 20) / 2 / jnp.sqrt(jnp.pi) * 5 * jnp.power(jnp.e, 1 / 400)\n\n# Uncomment to verify correctness of antiderivatives\n# x = jnp.linspace(0.01, 2, 100)\n# print(jnp.allclose(vmap(lognormal)(x), vmap(grad(lognormal_int))(x)))\n# print(jnp.allclose(vmap(lognormal)(x)**2, vmap(grad(lognormal_squared_int))(x)))\n\nNow we will define a simple class collecting some useful data about piecewise constant functions.\n\nclass DiscreteFunction:\n    \n    @staticmethod\n    def condlist(x, grid):\n        return [(g_left &lt; x) & (x &lt;= g_right) for g_left, g_right in zip(grid, grid[1:])]\n\n    def __init__(self, grid, values):\n        assert len(grid) == len(values) + 1, f'Number of grid points {len(grid)} does not match number of values {len(values)}.'\n        self.grid = grid\n        self.values = values\n        self.probabilities = values * (grid[1:]-grid[:-1])\n        \n        def f(x):\n            return jnp.piecewise(x, DiscreteFunction.condlist(x, grid), self.values)\n        \n        self.f = f\n        \n    def plot(self, x=None):\n        if x is None:\n            x = jnp.linspace(0.5, 1.5, 100)\n\n        plt.plot(x, [self.f(xi) for xi in x]) # jit or vmap here gives an error for some reason. Without them unnecessarily slow.\n        plt.plot(x, vmap(lognormal)(x))\n            \n    @classmethod            \n    def from_probabilities(cls, grid, probs):\n        values = probs/(grid[1:]-grid[:-1])\n        return cls(grid, values)      \n\nHere is the function that computes the L2 error between a given discrete function and the lognormal distribution. I used the equation\n\\[L2=\\int_a^b (v-p(x))^2=D^{-1}p(x)^2\\Big|^a_b-2 v D^{-1} p(x)\\Big|^a_b+v^2 (b-a)\\]\nThis is an error of approximating function \\(p(x)\\) by a constant \\(v\\) on an interval \\((a,b)\\). That this function is correct is confirmed by comparison with other independent methods which I presented above. This (a bit fancy) form is useful for speed in my numerical optimization.\n\ndef l2_error_contributions(discrete_function, left, right):\n    \n    grid = discrete_function.grid\n    values = discrete_function.values\n    \n    # inner contributions\n    f_squared_contrib = vmap(lognormal_squared_int)(grid[1:]) - vmap(lognormal_squared_int)(grid[:-1])\n    f_contrib = vmap(lognormal_int)(grid[1:]) - vmap(lognormal_int)(grid[:-1])\n    const_contrib = (values ** 2) * (grid[1:] - grid[:-1])\n\n    # outer contributions\n    outer_contrib_left = lognormal_squared_int(grid[0]) - lognormal_squared_int(left)\n    outer_contrib_right = lognormal_squared_int(right) - lognormal_squared_int(grid[-1])\n\n    # total\n    total_contribs = f_squared_contrib - 2 * f_contrib * values + const_contrib + outer_contrib_left + outer_contrib_right\n\n    return total_contribs\n\n\ndef l2_error(discrete_function, left=0, right=jnp.inf):\n    return jnp.sqrt(l2_error_contributions(discrete_function, left, right).sum())\n\nHere is a sample computation, x and p taken from here.\n\nx = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7]\np = [0., 0., 0., 0., 0., 0.081, 0.02, 0.1, 0.5, 0.115, 0.15, 0.03, 0.004, 0., 0., 0. ]\ndf = DiscreteFunction.from_probabilities(jnp.array(x), jnp.array(p))\ndf.plot()\nprint('L2 error:', l2_error(df))\n\nL2 error: 0.93145823"
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#optimization-setup-irrelevant",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#optimization-setup-irrelevant",
    "title": "Classiq coding competition – state preparation",
    "section": "Optimization setup (irrelevant)",
    "text": "Optimization setup (irrelevant)\nI use JAX for numerical optimization. It is very flexible and efficient, by lacks some of the high level API present in other libraries. The code below is only to setup numerical minimization with JAX, it has no relation to the problem. It is included here only in order to make the notebook self-contained.\n\ndef update_step(loss_and_grad, opt, opt_state, params):\n    loss, grads = loss_and_grad(params)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n\n    return params, loss, opt_state\n\n\ndef glue_params(params):\n    return jnp.concatenate(params)\n\n\ndef unglue_params(glued_params, slice_indices):\n    return [glued_params[i0:i1] for i0, i1 in zip(slice_indices, slice_indices[1:])]\n\n\ndef mynimize(loss, initial_params, opt_options):\n    \n    # Initialize optimizer and parameter splits.\n    loss_and_grad = value_and_grad(loss)\n    opt = opt_options.optimizer()\n    opt_state = opt.init(initial_params)\n    sizes = [len(ip) for ip in initial_params]\n    slice_indices = [sum(sizes[:i]) for i in range(len(sizes)+1)]\n\n    # Single learning iteration compatible with lax fori loop.\n    def iteration_with_history(i, carry):\n        glued_params_history, loss_history, opt_state = carry\n        glued_params = glued_params_history[i]\n        \n        params = unglue_params(glued_params, slice_indices)\n        \n        params = initial_params._make(params)\n        params, loss, opt_state = update_step(loss_and_grad, opt, opt_state, params)\n        \n        glued_params = glue_params(params)\n        glued_params_history = glued_params_history.at[i+1].set(glued_params)\n        \n        loss_history = loss_history.at[i].set(loss)\n        \n        return glued_params_history, loss_history, opt_state\n\n    # Initialize arrays holding whole histories for parameters and values.\n    glued_initial_params = glue_params(initial_params)\n    glued_params_history = jnp.zeros((opt_options.num_iterations, len(glued_initial_params))).at[0].set(glued_initial_params)\n    loss_history = jnp.zeros((opt_options.num_iterations,))\n\n    # Optimize iteratively within fori loop.\n    glued_params_history, loss_history, _ = lax.fori_loop(\n        0,\n        opt_options.num_iterations,\n        iteration_with_history,\n        (glued_params_history, loss_history, opt_state))\n    \n    # Bring parameters to the original representation.\n    params_history = vmap(lambda gp: unglue_params(gp, slice_indices))(glued_params_history)\n    reordered_params_history = [initial_params._make([params_history[num_param][num_iteration] for num_param in range(len(initial_params))]) for num_iteration in range(opt_options.num_iterations)]\n\n    return reordered_params_history, loss_history\n\n\n@dataclass\nclass OptOptions:\n    learning_rate: float = 0.01\n    num_iterations: int = 2000\n    random_seed: int = 0\n\n    def optimizer(self):\n        return optax.adam(self.learning_rate)\n    \nclass OptResult:\n    def __init__(self, raw_result, loss_func, opt_options):\n\n        self.loss_func = loss_func\n        self.opt_options = opt_options\n        self.params_history = raw_result[0]\n        self.loss_history = raw_result[1]\n        \n        self._best_i = jnp.argmin(self.loss_history)\n        \n        self.best_params = self.params_history[self._best_i]\n        self.best_loss = self.loss_history[self._best_i]\n\n    def _i_or_best_i(self, i):\n        if i is None:\n            return self._best_i\n        else:\n            return i\n\n    def plot_loss_history(self):\n        plt.plot(self.loss_history)\n        plt.yscale('log')\n        \n    def __repr__(self):\n        return f'OptResult: best_loss {self.best_loss}.'"
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-values-only",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-values-only",
    "title": "Classiq coding competition – state preparation",
    "section": "Fitting values only",
    "text": "Fitting values only\nHere is the function that does the first part of the job.\n\nV = namedtuple('Values', ['values'])\n\ndef fit_values(discrete_function, opt_options=OptOptions()):\n    \n    grid = discrete_function.grid\n    initial_values = discrete_function.values\n    \n    @jit\n    def loss(v):\n        values = v.values\n        df = DiscreteFunction(grid, values)\n        return l2_error(df)\n    \n    initial_params = V(initial_values)\n    results = mynimize(loss, initial_params, opt_options)\n    return OptResult(results, loss, opt_options)\n\nHere is the best this method can do with 10 qubits starting from random initial values between 0 and 1 (you can change the number of qubits if you wish). The histogram shows density of the grid points. At this point they are distributed uniformly.\n\nnum_qubits = 10\n\ngrid = jnp.linspace(0.5, 1.5, 2**num_qubits+1)\ninitial_values = random.uniform(random.PRNGKey(0), (2**num_qubits, ))\ndf = DiscreteFunction(grid, initial_values)\nres = fit_values(df)\nprint(res)\ndf_fit = DiscreteFunction(df.grid, res.best_params.values)\n\nres_values_only = res\ndf_fit_values_only = df_fit\n\nplt.subplot(1, 2, 1)\nres.plot_loss_history()\nplt.title('loss history')\nplt.subplot(1, 2, 2)\ndf_fit.plot()\nplt.title('discretization')\nplt.hist(np.array(df_fit.grid), bins=int(len(df_fit.grid)/5), density=True);\n\nOptResult: best_loss 0.0033653806895017624."
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-values-and-grid",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-values-and-grid",
    "title": "Classiq coding competition – state preparation",
    "section": "Fitting values and grid",
    "text": "Fitting values and grid\nNow we introduce the second optimization procedure, which also adjusts discretization intervals.\nOne technical subtlety here is that the grid points should never go past each other. In order to prevent that I use auxilary variables, which are square roots of the distances between neighboring grid points, grid_roots. Even if some grid_root becomes negative the distance between the grid points grid_root**2 stays positive.\n\nVG = namedtuple('ValuesGrid', ['values', 'grid_roots'])\n\ndef grid_to_roots(grid):\n    all_points = jnp.concatenate([jnp.array([0]), grid]) # Append '0' to the left.\n    cells = all_points[1:] - all_points[:-1]\n    return jnp.sqrt(cells)\n\ndef roots_to_grid(roots):\n    \"\"\" A bit of complicated syntaxis to restore grid from roots in a jax-compatible way.\"\"\"\n    cells = roots ** 2\n    masks = jnp.tri(len(roots))\n    pre_grid = vmap(lambda x: cells * x)(masks)\n    return pre_grid.sum(axis=1)\n\n\ndef fit_values_and_grid(discrete_function, opt_options=OptOptions()):\n\n    initial_grid_roots = grid_to_roots(discrete_function.grid)\n    initial_values = discrete_function.values\n    \n    @jit\n    def loss(vg):\n        grid = roots_to_grid(vg.grid_roots)\n        df = DiscreteFunction(grid, vg.values)\n        return l2_error(df)\n    \n    initial_params = VG(initial_values, initial_grid_roots)\n    results = mynimize(loss, initial_params, opt_options)\n    return OptResult(results, loss, opt_options)\n\nHere is the best fit to the lognormal distribution this procedure is able to find. Note that grid points are no longer distributed uniformly but clamp near the regions with the highest slope, as they should (this is in fact better visible at smaller qubit count). Note that here we initialized the optimization with values found at the previous stage. If we were to initialize them randomly, the result would be much worse. Note also a smaller learning rate at this stage.\n\nnum_qubits = 10\n\ninitial_grid = df_fit_values_only.grid\ninitial_values = df_fit_values_only.values\n\ndf = DiscreteFunction(initial_grid, initial_values)\n\nopt_options = OptOptions(learning_rate=1e-4, num_iterations=5000)\nres = fit_values_and_grid(df, opt_options)\nprint(res)\n\nbest_values = res.best_params.values\nbest_grid = roots_to_grid(res.best_params.grid_roots)\ndf_fit = DiscreteFunction(best_grid, best_values)\n\nplt.subplot(1, 2, 1)\nres.plot_loss_history()\nplt.title('loss history')\nplt.subplot(1, 2, 2)\ndf_fit.plot()\nplt.title('discretization')\nplt.hist(np.array(df_fit.grid), bins=int(len(df_fit.grid)/5), density=True);\n\nOptResult: best_loss 0.00303847249597311."
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#mycircuit-class",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#mycircuit-class",
    "title": "Classiq coding competition – state preparation",
    "section": "MyCircuit class",
    "text": "MyCircuit class\nFirst I will define a very simple MyCircuit class that bundles qiskit representation with jax-compatible unitary. For the purposes of this notebook, we only need to place one gate on each qubit.\n\ndef U_gate(a):\n    theta, phi, lmbda = a\n    return jnp.array([[jnp.cos(theta/2), -jnp.exp(1j*lmbda)*jnp.sin(theta/2)],\n                     [jnp.exp(1j*phi)*jnp.sin(theta/2), jnp.exp(1j*(phi+lmbda))*jnp.cos(theta/2)]])\n\n\nclass MyCircuit:\n    def __init__(self, num_qubits):\n        self.num_qubits = num_qubits\n    \n    def qiskit_circuit(self, angles):\n        assert len(angles) == 3*self.num_qubits, f'Number of qubits {self.num_qubits} and angle triples {len(angles)} does not match.'\n        qc = QuantumCircuit(self.num_qubits)\n        angles = np.array(angles) # Qiskit does not accept JAX arrays.\n        for i, (theta, phi, lmbda) in enumerate(angles.reshape(self.num_qubits, 3)):\n            qc.u(theta, phi, lmbda, i)\n        return qc\n    \n    def unitary(self, angles):\n        gates = vmap(U_gate)(angles.reshape(self.num_qubits, 3))\n        return reduce(jnp.kron, gates)\n    \n    def _verify(self, angles):\n        u_qs = Operator(self.qiskit_circuit(angles).reverse_bits()).data\n        u_jax = self.unitary(angles)\n        return jnp.allclose(u_qs, u_jax)\n\nHere is an example.\n\nnum_qubits = 5\nangles = random.uniform(random.PRNGKey(0), (num_qubits*3, ), minval=0, maxval=2*jnp.pi)\n\nmc = MyCircuit(num_qubits)\nmc.unitary(angles)\nprint('qiskit unitary coincides with our unitary:', mc._verify(angles))\nmc.qiskit_circuit(angles).draw(output='mpl')\n\nqiskit unitary coincides with our unitary: True"
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#defining-loss-associated-with-a-unitary",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#defining-loss-associated-with-a-unitary",
    "title": "Classiq coding competition – state preparation",
    "section": "Defining loss associated with a unitary",
    "text": "Defining loss associated with a unitary\nQuantum circuit transforms the input state. Amplitudes of the output state encode the values of the discrete function that we use to fit the lognormal distribution. Here we construct a function that takes a quantum circuit and returns the L2 error of the corresponding approximation.\n\ndef loss_from_unitary(grid, u):\n    probs = probabilities_from_unitary(u)\n    df = DiscreteFunction.from_probabilities(grid, probs)\n    \n    return l2_error(df)\n\ndef probabilities_from_unitary(u):\n    all_zero_state = jnp.zeros(u.shape[0]).at[0].set(1)\n    amplitudes = amplitudes_from_state(u @ all_zero_state)\n    probabilities = jnp.abs(amplitudes)**2\n    return probabilities\n\ndef amplitudes_from_state(state):\n    return vmap(lambda basis_state: (state * basis_state).sum())(jnp.identity(len(state)))"
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-angles-only",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-angles-only",
    "title": "Classiq coding competition – state preparation",
    "section": "Fitting angles only",
    "text": "Fitting angles only\nNow we are ready to optimize angles in the circuit for the best fit to the target distribution. Discretization is held fixed at this stage.\n\nA = namedtuple('Angles', ['angles'])\n\ndef fit_angles(grid, num_qubits, opt_options=OptOptions()):\n    assert len(grid) == 2**num_qubits+1, f'Grid length {len(grid)} does not match number of qubits {num_qubits}.'\n    circuit = MyCircuit(num_qubits)\n\n    @jit\n    def loss(a):\n        u = circuit.unitary(a.angles)\n        return loss_from_unitary(grid, u)\n\n    initial_angles = random.uniform(random.PRNGKey(opt_options.random_seed), (3*num_qubits, ), minval=0, maxval=2*jnp.pi)\n    initial_params = A(initial_angles)\n    \n    results = mynimize(loss, initial_params, opt_options)\n    return OptResult(results, loss, opt_options)\n\nHere is an example with 6 qubits.\n\nnum_qubits = 6\ngrid = jnp.linspace(0.6, 1.5, 2**num_qubits+1)\nres = fit_angles(grid, num_qubits)\n\nprint(res)\ncircuit = MyCircuit(num_qubits)\nprobs = probabilities_from_unitary(circuit.unitary(res.best_params.angles))\ndf_fit = DiscreteFunction.from_probabilities(grid, probs)\n\nplt.subplot(1, 2, 1)\nres.plot_loss_history()\nplt.title('loss history')\nplt.subplot(1, 2, 2)\ndf_fit.plot()\nplt.title('discretization')\nplt.hist(np.array(df_fit.grid), bins=int(len(df_fit.grid)/5), density=True);\n\nres_angles = res\ndf_fit_angles = df_fit\n\nOptResult: best_loss 0.884075403213501.\n\n\n\n\n\nThe highly asymmetric shape of the fitting function here is typical and continutes to higher qubits. At the first glance, there seems to be little hope of making the construction work. However, as we see right now, adjusting the discretization ranges cuts the deal."
  },
  {
    "objectID": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-angles-and-grid",
    "href": "posts/classiq_state_preparation/2022-06-21-classiq lognormal.html#fitting-angles-and-grid",
    "title": "Classiq coding competition – state preparation",
    "section": "Fitting angles and grid",
    "text": "Fitting angles and grid\nHere is the procedure that fits angles and grid together. We bundle it with the previous step into a single simple function fit_circuit that does all the work.\n\nAG = namedtuple('AnglesGrid', ['angles', 'grid_roots'])\n\ndef fit_angles_and_grid(initial_angles, initial_grid, opt_options=OptOptions()):\n    num_qubits = int(len(initial_angles)/3)\n    assert len(initial_grid) == 2**num_qubits+1, f'Grid length {len(grid)} does not match number of qubits {num_qubits}.'\n    circuit = MyCircuit(num_qubits)\n\n    @jit\n    def loss(ag):\n        u = circuit.unitary(ag.angles)\n        grid = roots_to_grid(ag.grid_roots)\n        return loss_from_unitary(grid, u)\n    \n    initial_grid_roots = grid_to_roots(initial_grid)\n    initial_params = AG(initial_angles, initial_grid_roots)\n    \n    results = mynimize(loss, initial_params, opt_options)\n    return OptResult(results, loss, opt_options)\n\ndef fit_circuit(num_qubits):\n    print('Initial optimization of angles:')\n    grid = jnp.linspace(0.6, 1.5, 2**num_qubits+1)\n    res = fit_angles(grid, num_qubits)\n    print(res)\n    \n    print('\\nOptimization of angles and grid:')\n    circuit = MyCircuit(num_qubits)\n\n    initial_angles = res.best_params.angles\n    initial_grid = grid\n\n    opt_options = OptOptions(learning_rate=1e-4, num_iterations=10000)\n    \n    res = fit_angles_and_grid(initial_angles, initial_grid, opt_options)\n\n    print(res)\n\n    best_probs = probabilities_from_unitary(circuit.unitary(res.best_params.angles))\n    best_grid = roots_to_grid(res.best_params.grid_roots)\n\n    df_fit = DiscreteFunction.from_probabilities(best_grid, best_probs)\n\n    plt.subplot(1, 2, 1)\n    res.plot_loss_history()\n    plt.title('loss history')\n    plt.subplot(1, 2, 2)\n    df_fit.plot()\n    plt.hist(np.array(df_fit.grid), bins=int(len(df_fit.grid)/5), density=True);\n    plt.title('discretization')\n    \n    return circuit.qiskit_circuit(res.best_params.angles).qasm(), best_grid\n\nHere is what happens for 6 qubits when we follow up initial angle optimization with the grid optimization.\n\nqasm, grid = fit_circuit(6)\n\nInitial optimization of angles:\nOptResult: best_loss 0.884075403213501.\n\nOptimization of angles and grid:\nOptResult: best_loss 0.04849757254123688.\n\n\n\n\n\nThe optimization results improved dramatically. We are able to achive \\(5\\times 10^{-2}\\) error already on six qubits. As we can anticipate, using all 10 qubits helps a lot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An incoherent mix of my quantum notes",
    "section": "",
    "text": "Where do quantum speedups come from?\n\n\n\n\n\n\n\nquantum concepts\n\n\npaper review\n\n\n\n\nWhat makes quantum computers powerful? Explained as simply as possible, but no simpler.\n\n\n\n\n\n\nJul 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Quantum Singular Value Transformation\n\n\n\n\n\n\n\nquantum concepts\n\n\npaper review\n\n\n\n\nOr grand unification of quantum algorithms, if you feel a bit grandiose.\n\n\n\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat will a quantum computer be good for, exactly?\n\n\n\n\n\n\n\nquantum concepts\n\n\npaper review\n\n\n\n\nI think we don’t really know yet.\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDecomposition of a general single–qutrit gate\n\n\n\n\n\n\n\nquantum concepts\n\n\ncompilation\n\n\n\n\nAnd a couple of words about Euler’s and KAK decompositions\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nClassiq coding competition – Toffoli gate decomposition\n\n\n\n\n\n\n\ncompilation\n\n\npaper review\n\n\n\n\nBased on the relative phase Toffoli gates\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\nClassiq coding competition – state preparation\n\n\n\n\n\n\n\ncompilation\n\n\nmachine learning\n\n\nJAX\n\n\n\n\nThe fishy way!\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning error correction codes\n\n\n\n\n\n\n\nQML\n\n\nQEC\n\n\n\n\nSweeping conceptual difficulties under the rug with a black-box approach\n\n\n\n\n\n\nJun 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSome analytic facts about variational quantum algorithms\n\n\n\n\n\n\n\nQML\n\n\nVQA\n\n\nVQE\n\n\n\n\nParameter-shift rule, sequential optimization, average performance – it’s mostly basic trigonometry, really.\n\n\n\n\n\n\nMay 17, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning compilation of quantum circuits – experiments\n\n\n\n\n\n\n\nqiskit\n\n\nJAX\n\n\nmachine learning\n\n\ncompilation\n\n\n\n\nFlexible and efficient learning with JAX+numpy\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMatrix representation of quantum circuits - notations and gotchas\n\n\n\n\n\n\n\nqiskit\n\n\ntensor networks\n\n\nquantum concepts\n\n\n\n\nA case study by building a tensor network to match qiskit conventions\n\n\n\n\n\n\nAug 18, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning compilation of quantum circuits\n\n\n\n\n\n\n\nmachine learning\n\n\ncompilation\n\n\nqiskit\n\n\npaper review\n\n\n\n\nOptimal compiling of unitaries reaching the theoretical lower bound\n\n\n\n\n\n\nJul 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nWhat is entanglement?\n\n\n\n\n\n\n\nquantum concepts\n\n\nqiskit\n\n\n\n\nI see, and so what?\n\n\n\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow was this blog set up?\n\n\n\n\n\n\n\nfastpages\n\n\nquatro\n\n\n\n\nAnd then broken and then migrated to quarto.\n\n\n\n\n\n\nJul 11, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name is Nikita Nemkov, and I’m a quantum computing researcher. I’m interested in most things quantum, but currently mainly work on compilation of quantum circuits and variational algorithms, while my background is in theoretical physics. On this blog, I publish notes on random subjects in quantum computing. I do not have any particular audience in mind, and the notes are mostly for my future self. Yet, if you found anything here to be useful, don’t hesitate to leave a comment, drop me an email, or connect on LinkedIn.\nMy posts are rendered from fully functional Jupyter notebooks, and many contain some code. If there is open in colab button, you can click on it and explore the post interactively.\nRecently, I migrated the blog from fastpages to quarto, which was not effortless. I suspect this introduced a lot of minor bugs, reference inconsistencies, missing figures and so on. If you spot something like that, please let me know."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html",
    "href": "posts/where_quantum_speedups_come_from/index.html",
    "title": "Where do quantum speedups come from?",
    "section": "",
    "text": "So there are quantum computers. At least there will be, we believe. And the reason to build them is an expectation that they will solve some problems intractable for the classical computers. It is a new way to process information, a new way to do computation.\nHowever, it is surprisingly difficult to pinpoint the exact mechanism underlying the capabilities of a quantum computer. Most of the simplistic explanations, referencing some unusual property of quantum systems such as entanglement, superposition etc, are incomplete at best. It is hard to compare classical and quantum head-to-head, because of the very different language used to describe them.\nIn this post, I will try to make a fair comparison and zoom in on the essential differences. As an experiment, I will try to cover the topic in a way accessible for people outside the field, striving to provide an explanation that is “as simple as possible, but no simpler”. The discussion still won’t be entirely non-technical, though, and use some basic probability and linear algebra.\n\n\n\n\n\n\nStill, I could not resist supplementing the basic explanation with more technical stuff. Parts, encapsulated like this one, will contain more math and require some quantum background. They can be skipped without breaking the main argument, though.\n\n\n\n\n\n\n\n\n\nAnd occasionally there will be something even more hardcore.\n\n\n\n\n\n\nI will only consider algorithms that both take as input and return as output classical data. In principle, quantum computers can take and/or return quantum information (more on that later), but classical computers can’t, so it does not make sense to compare them in this setting.\nIt is important to stress that quantum computers are not entirely magical. In principle, anything that can be done by a quantum computer can also be done by a classical one. However, it may take prohibitively more time and/or memory.\nFor instance, any process on a small enough quantum computer can be easily reproduced on a laptop. We call this a simulation (a classical simulation of a quantum computer, to be more precise).\nGenerically, the amount of classical resources required to simulate a quantum process scales exponentially. So while the classical simulation is always possible in principle, it may not be efficient. It should be stressed, though, that there are many non-trivial types of quantum processes that can be simulated efficiently at scale. More on that later.\nTo explain why a quantum computer can be more powerful than a classical, can mean addressing two questions.\n\nWhy does the raw power of the quantum computer grow so fast? In other words, why an arbitrary quantum computation can not be simulated efficiently by a classical machine?\nWhy some of the things that a quantum computer can do, but a classical can not simulate efficiently, are useful? What problems can be efficiently solved quantumly, but not classically?\n\nIn this post, I will mostly focus on the first question, i.e. try to explain what sets a generic quantum computation apart from classical. The second question, which is by all means just as important, I will only touch briefly.\n\n\n\nAlright, what property of quantum mechanics is responsible for the potential power of quantum computation? Shouldn’t it be easy to identify? Does it even make sense to ask this question, as the quantum mechanics is just so weird and spooky and a wave and a particle at the same time? Here are the usual suspects.\n\nHuge (exponentially large) dimension of the space where quantum states live.\nQuantum parallelism.\nSuperposition.\nEntanglement.\nContextuality.\n\nDon’t worry if some concepts are unfamiliar, we will elaborate.\nYou can probably add more. While all these things are necessary, neither is sufficient alone. And they are also come in a package, so it may not even be consistent to keep some and discard the others. However, If I had to choose, I’d probably say it’s large state space + superposition, but this may be a matter of taste."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#disclaimers",
    "href": "posts/where_quantum_speedups_come_from/index.html#disclaimers",
    "title": "Where do quantum speedups come from?",
    "section": "Disclaimers",
    "text": "Disclaimers\n\n\n\n\n\n\nNote\n\n\n\nI will specifically focus on quantum computation, not sensing or communication. Moreover, I will look for practical, useful problems, not just any possible demonstration of quantum advantage.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs mentioned, my expertise in the field is limited. So take my assessment critically. I will cite many sources, but of course they are subject to my selection bias. Also, I’m more than happy to be proven wrong in this case. Feel free to leave the feedback.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI do not attempt to make the references comprehensive, e.g. I won’t cite original work of Grover or Shor. This is a blog post, come on. On the contrary, I will try to limit citations to those directly relevant to my arguments."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#variational-quantum-algorithms",
    "href": "posts/where_quantum_speedups_come_from/index.html#variational-quantum-algorithms",
    "title": "Where do quantum speedups come from?",
    "section": "Variational quantum algorithms",
    "text": "Variational quantum algorithms\nThe main bundle of NISQ algorithms are variational quantum algorithms (VQA)  [11]. The two most studied examples are Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE). QAOA mostly focuses on ground state preparation for classical Ising Hamiltonians, which in facts covers a huge range of problems related to combinatorial optimization. VQE typically addresses Hamiltonians that arise from physics or chemistry, but largely does the same thing. I think the line between different types of VQA is quite blurry.\n\n\n\n\n\n\nVariational quantum algorithms\n\n\n\n\n\nQuite generally, variational quantum algorithms aim to find a low energy state of some Hamiltonian \\(H\\) encoding the problem of interest. They start with a trivial quantum state \\(|0\\rangle\\) and apply a parameterized quantum circuit to it \\(U(\\theta)|0\\rangle\\). The resulting energy \\[E(\\theta)=\\langle0|U^\\dagger(\\theta)HU(\\theta)|0\\rangle\\] is minimized by adjusting parameters \\(\\theta\\) classically.\nParameterized quantum circuits \\(U(\\theta)\\) can be informed by the problem as in QAOA, which seeks to approximate the adiabatic evolution, or completely problem-agnostic as in Hardware-Efficient ansatze.\n\n\n\nHere is my simple-minded and a bit cynic take on the idea behind variational quantum “algorithms”, which I think would be better called heuristics. Real quantum algorithms (without quotes) typically require circuits that are very deep. The current generation of quantum devices is pretty inaccurate, errors in two-qubit gates are of the order of \\(0.1-1\\%\\). If you apply many gates, there will be nothing but noise at the output, and the computation is not useful. VQA approach the problem as follows. Alright, we do not know algorithms with shallow circuits, but let’s try to build some. We’ll prepare a quantum circuit that is sufficiently shallow to have a non-zero signal-to-noise ratio, and introduce parameters in there. While we do not know if any values of these parameters correspond to a useful computation, let’s try to adjust them (classical optimization loop) so that the circuit does something useful.\nI mean, it is not a bad idea, and in many respects similar to how classical machine learning works. The problem seems to be, the current hardware only allows circuits so shallow, that you may optimize them all you want, no interesting results will follow. Another practical problem is that evaluation of the energy function \\(E(\\theta)\\) requires taking a ton of samples, which is slow and expensive. On the theoretical side, the loss landscape of most VQA appears to be pretty terrible, featuring barren plateaus and many bad local minimums. So even with a perfect hardware, there are no guarantees for good results."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#quantum-machine-learning",
    "href": "posts/where_quantum_speedups_come_from/index.html#quantum-machine-learning",
    "title": "Where do quantum speedups come from?",
    "section": "Quantum machine learning",
    "text": "Quantum machine learning\n\n\n\n\n\n\nSupervised quantum machine learning models\n\n\n\n\n\nA typical quantum model for a supervised learning task looks very similar to VQA instances described above \\[E(x,\\theta)=\\langle0|U^\\dagger(x,\\theta) H U(x,\\theta)|0\\rangle \\ .\\] Only here, part of the parameters \\(x\\) are now not model “weights” to be adjusted, but instead encode the training data. The rest of the parameters \\(\\theta\\) are to be optimized to yield a better loss function \\(E(x,\\theta)\\).\n\n\n\nQuantum machine learning (QML)  [12] sounds quite fancy, but it shares much of the problems with VQA. Additional questions you might ask about QML models is whether they have and edge over classical in data encoding, expressivity, generalize better etc. To the best of my understanding, all claims that some QML models are somehow better than classical counterparts are heuristic, inconclusive, or only work for extremely artificial datasets. Moreover, let me quote a recent perspective  [13] by Schuld and Killoran titled “Is quantum advantage the right goal for quantum machine learning?”\n\nContrary to commercial expectations – machine learning may turn out to be one of the hardest applications to show a practical quantum advantage for.\n\nWhy? By all means take a look at the paper if you are interested, but a short answer is that\n\nQuantum machine learning research is trying to beat large, high-performing algorithms for problems that are conceptually hard to study.\n\nIn other words, classical machine learning is so efficient it sets a very high bar; it’s hard to theoretically analyze how it works, let alone prove quantum advantage; and we can’t collect any meaningful empirical data on QML because we only have toy hardware yet."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#noisy-summary",
    "href": "posts/where_quantum_speedups_come_from/index.html#noisy-summary",
    "title": "Where do quantum speedups come from?",
    "section": "Noisy summary",
    "text": "Noisy summary\nThere are many more versions of NISQ algorithms beyond VQA and QML. However, they all come with significant challenges. I quote an extensive recent review  [10]\n\nAt the moment of documenting this review, there is no known demonstration of industrially relevant quantum advantage.\n\nOn top of that, to the best of my knowledge, there are also no theoretical guarantees that NISQ algorithms can lead to quantum advantage at all. So, NISQ algorithms seemed like a low-hanging fruit, but despite all the work of the past years, useful applications have not been demonstrated. Even the industry now seems to become less optimistic about NISQ, and focus more on the fault-tolerant algorithms, and so will we."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#quantum-dynamics",
    "href": "posts/where_quantum_speedups_come_from/index.html#quantum-dynamics",
    "title": "Where do quantum speedups come from?",
    "section": "Quantum dynamics",
    "text": "Quantum dynamics\nQuantum simulation is the application that is often cited as having kick-started the field. At the same time, it is still widely believed to have the best shot at useful quantum advantage.\nIt is a very natural application, since no convoluted procedure to fold and squeeze a classical problem into a quantum domain is required. Instead, it looks at the task that is obviously quantum in origin, and proposes an efficient way to solve it with a quantum computer.\n\n\n\n\n\n\nQuantum Simulation\n\n\n\n\n\nQuantum simulation is designed to take an initial quantum state \\(|\\psi_0\\rangle\\) and carry out its evolution under some Hamiltonian \\(H\\), i.e. to find\n\\[|\\psi(t)\\rangle=e^{-iHt}|\\psi_0\\rangle \\ .\\]\nFor a Hamiltonian which is sparse, e.g. consists of not too many local terms \\(H=\\sum_k{H_k}\\), one can use the Trotter-Suzuki approximation \\(e^{(A+B)\\Delta t}=e^{A\\Delta t}e^{B\\Delta t}+O(\\Delta t^2)\\) to reduce the simulation of the full Hamiltonian evolution over some small time period \\(\\Delta t\\) to a simulation of separate local terms, which is in principle straightforward\n\\[e^{-i H \\Delta t}=\\prod_k e^{-i H_k \\Delta t}+O(\\Delta t^2) \\ .\\]\nEvolution over a finite time period \\(t\\) can then be produced by a sequence of short evolutions \\(e^{-iHt}=\\left(e^{-iH\\Delta t}\\right)^{\\frac{t}{\\Delta t}}\\). The error coming from “Trotterization” of each small time step can be reduced by making \\(\\Delta t\\) smaller, at the cost of increasing the circuit depth polynomially.\n\n\n\nWhile numerous classical methods for approximate simulation of quantum systems have been developed, with great success in many cases, they are not sufficient in general. This is another important point about the simulation problem – the difficulty of the classical approach is well appreciated, so quantum computer is really expected to make the difference here.\nIs quantum simulation useful? I mean, it obviously is, but how exactly? There are definitely implications for fundamental science such as probing complicated quantum dynamics, new phases of matter, quantum chaos and so on. But what about designing a high-temperature superconductor or a new battery? Unfortunately, I am not aware of a rigorous connection between the ability to do quantum simulation and producing practically useful outcomes. So far it seems to be more about exploring the physics/chemistry with the new tools and beyond the regimes the current techniques allow. For this reason, it’s not clear that analog quantum simulators, which will be ultimately limited in their accuracy, will have applications beyond basic science  [23]."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#quantum-phase-estimation",
    "href": "posts/where_quantum_speedups_come_from/index.html#quantum-phase-estimation",
    "title": "Where do quantum speedups come from?",
    "section": "Quantum phase estimation",
    "text": "Quantum phase estimation\nThere is another flavor of quantum simulation that leads to more deterministic results. The archetypical algorithm here is the quantum phase estimation (QPE).\n\n\n\n\n\n\nQuantum phase estimation\n\n\n\n\n\nQuantum phase estimation (QPE) can be thought of as an efficient quantum circuit to perform a projective energy measurement.\nIt allows finding eigenvalues and preparing eigenstates of a Hamiltonian \\(H\\), provided one can efficiently implement controlled evolution operators \\(e^{-iHt}\\). Usually, QPE is formulated as an algorithm for finding eigenvalues of a unitary operator \\(U\\) given its eigenstate \\(|\\lambda\\rangle\\) with an unknown eigenvalue. QPE proceeds by applying powers of \\(U\\) (\\(U, U^2, U^4,\\dots\\)) to state \\(|\\lambda\\rangle\\), each controlled by its own auxiliary qubit. The state of the auxiliary qubits then contains a lot of information about \\(\\lambda\\), roughly one bit of accuracy per qubit, and this information can be efficiently revealed after performing a quantum Fourier transform on the auxiliary qubits.\nIf the original state \\(|\\psi\\rangle\\) is not an eigenstate of \\(U\\), QPE performs a projective energy measurement. It will output some energy \\(E\\) and prepare the corresponding state \\(|\\lambda_E\\rangle\\) with probability proportional to the overlap of \\(|\\langle\\psi|\\lambda_E\\rangle|^2\\).\n\n\n\nQuantum phase estimation can yield high-accuracy information about the eigenstates of a Hamiltonian. In many practical questions of quantum chemistry, this exactly what you want to know. There is a caveat, though. Most interesting in practice are ground and low-energy states, and for QPE to reveal information about them, the initial state for the algorithm needs to have sufficiently high overlap with low-energy states. In general this problem is QMA-hard (not expected to be efficiently solvable even on a quantum computer), and whether it is solvable in practical scenarios is still not settled conclusively  [24]."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#nothing-ive-said-here-is-new",
    "href": "posts/where_quantum_speedups_come_from/index.html#nothing-ive-said-here-is-new",
    "title": "Where do quantum speedups come from?",
    "section": "Nothing I’ve said here is new",
    "text": "Nothing I’ve said here is new\nIf you think that the perspective taken in this blog post is in any way novel or radical, it’s not. While the points I made here are not often voiced or put in writing, many experts have been saying similar things for years. Here are some references.\n\nI highly recommend this talk  [18] given by Matthias Troyer way back in 2014. Or a more recent one  [27] from 2021. Interestingly, they are pretty similar in content, and in particular Troyer seems to entirely ignore the variational algorithms, and maybe for a good reason. There is also a recent short write-up by him and collaborators  [16].\nIn a 2021 talk Ryan Babbush  [28] (in conclusions part) says that the community still needs to figure out, with clarity, what will quantum computers be useful for. He says this in the context of early fault-tolerant computation, but I think the point applies more broadly.\nHere is a piece by a renowned condensed matter physicist Sankar Das Sarma  [29], arguing that potential applications of NISQ are highly overstated.\nHere are two presentations by Owen Lockwood  [30],  [31] critically assessing the state of NISQ algorithms and NISQ QML. Owen might not have the weight of other people I reference here, but I found his take on things original and informative.\nHere is a pretty critical LinkedIn post by Victor Galitski  [32]. It is again a mostly a critique of NISQ, with focus mainly on socio-economic rather than algorithmic side of things, but still worth a read.\nFinally, I’ll mention this popular interview with John Preskill  [33], where he mentions (section ‘simulation’) that quantum simulation is still probably our best grounded expectation for practical quantum advantage."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#i-may-have-a-bias-problem",
    "href": "posts/where_quantum_speedups_come_from/index.html#i-may-have-a-bias-problem",
    "title": "Where do quantum speedups come from?",
    "section": "I may have a bias problem",
    "text": "I may have a bias problem\nAlright, you might have noticed that even this list gets increasingly less rigorous. My investigation, which started as a noble search for truth, quickly turned into a confirmation bias exercise. Indeed, I quite quickly started to err on the side that ‘we still don’t really know what quantum computers will be good for’, and enjoyed finding support for this view. While this may not be a great journalistic work, I still think this point of view is seriously underrepresented and worth voicing. At the same time, I’m really open to changing my mind, as I have all the reasons to want the field to succeed, and the sooner, the better."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#quantum-computing-is-gonna-be-a-rock-star-one-day",
    "href": "posts/where_quantum_speedups_come_from/index.html#quantum-computing-is-gonna-be-a-rock-star-one-day",
    "title": "Where do quantum speedups come from?",
    "section": "Quantum computing is gonna be a rock star one day",
    "text": "Quantum computing is gonna be a rock star one day\nI must also say that in the long run, a radical impact of quantum computing looks inevitable to me. This is a fundamentally new way of information processing, and this must make a difference. As Scott Aaronson have argued, if for fundamental reasons large scale quantum computers can never be built, it would be a new and revolutionary law of physics. I’d say that similarly, if we could ‘prove’ that quantum computers can not be useful, this would be a new remarkable law of nature worth discovering. From what we know now, it looks extremely unlikely. However, use cases for truly novel technologies are hard to forecast."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#what-looks-the-most-promising-at-the-moment",
    "href": "posts/where_quantum_speedups_come_from/index.html#what-looks-the-most-promising-at-the-moment",
    "title": "Where do quantum speedups come from?",
    "section": "What looks the most promising at the moment?",
    "text": "What looks the most promising at the moment?\nIn searching for practical quantum advantage, several requirements need to be met.\n\nThere must be a problem that a quantum computer can solve efficiently.\nEvidence that a classical computer can’t.\nLast but not least, the problem must be useful.\n\nIf you think about it, this list is as much about the problem we want to solve as it is about the power of quantum algorithms. And finding the right problems, although possible in theory, turns out to be very challenging in practice.\nIt is exciting to try applying quantum algorithms to problems that appear to have no direct relation to the quantum world whatsoever. Basically, we start with (1) and then try to comply to (2) and (3). Say, we have an idea about how to solve certain large linear systems of equations and then try to find a subset of those that are useful and intractable classically. And while there may be gems on this path, a lot of evidence now shows that finding the right problems of this kind is really tricky. One early impressive success is Shor’s algorithm, but It may still be the only well-established example.\nOn the other hand, one can tackle obviously quantum-inspired problems. Quite recently, people started to look at cases when the input data is quantum rather than classical, and there the quantum advantage is already established, but no wide uses have been suggested yet. I’d say that the simulation of quantum systems for physics and chemistry appears to be our most grounded proposal for where to look for a practical quantum advantage. It addresses an obviously important problem known to be classically hard by decades of intensive research. So (2) and (3) are covered, and (1) also comes naturally. Indeed, people also often describe quantum simulation as a native task for a quantum computer. I quote an elegant passage from  [23]\n\nThis is the ‘native’ and most natural application of quantum computers, where we aim to use a quantum computer to mimic the rules that describe physical microscopic quantum systems. These problems are computationally challenging for the same underpinning reason that quantum computers can be powerful.\n\nAlright, I’ll leave it at that. As usual, any feedback is welcome."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#humongous-hilbert-space",
    "href": "posts/where_quantum_speedups_come_from/index.html#humongous-hilbert-space",
    "title": "Where do quantum speedups come from?",
    "section": "Humongous Hilbert space",
    "text": "Humongous Hilbert space\n\nNecessary, as we could state-vector simulate everything otherwise. Not sufficient. Some things can be simulated anyway. Applies to probabilistic computing as well."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#section",
    "href": "posts/where_quantum_speedups_come_from/index.html#section",
    "title": "Where do quantum speedups come from?",
    "section": "",
    "text": "A lot of confusion about the source of quantum advantage.\nUsual suspects\n\nLarge Hilbert space\nEntanglement\nParallelism\nSuperposition\n\nSimple but accurate explanation hard to come by.\nMany simplistic explanations fail because they apply to probabilistic computing as well\n\nLarge space dimension for probabilities\nNo interference\nStochastic matrices can be simulated? Always increase entropy\nQuasi-probability distr, Wigner functions\n\nAnd then there are classical waves.\n\nBut no exponential modes\n\nEntanglement is curios, but not crucial\n\nEntanglement ensures exploring Hilbert space\nStabilizer circuits can be simulated\nQuantum correlations are not directly related to computational advantage\n\nAlgorithms vs quantum circuits.\nPost a bit different, try to give as simple explanation as possible.\nCouldn’t do it, but technical parts separated.\nTakeaway.\n\nDimension of the Hilbert space+superposition given a polynomial amount of qubits\n\nThe answer not completely satisfactory, the subject is subtle."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#what-does-this-mean-exactly",
    "href": "posts/where_quantum_speedups_come_from/index.html#what-does-this-mean-exactly",
    "title": "Where do quantum speedups come from?",
    "section": "",
    "text": "I will only consider algorithms that both take as input and return as output classical data. In principle, quantum computers can take and/or return quantum information (more on that later), but classical computers can’t, so it does not make sense to compare them in this setting.\nIt is important to stress that quantum computers are not entirely magical. In principle, anything that can be done by a quantum computer can also be done by a classical one. However, it may take prohibitively more time and/or memory.\nFor instance, any process on a small enough quantum computer can be easily reproduced on a laptop. We call this a simulation (a classical simulation of a quantum computer, to be more precise).\nGenerically, the amount of classical resources required to simulate a quantum process scales exponentially. So while the classical simulation is always possible in principle, it may not be efficient. It should be stressed, though, that there are many non-trivial types of quantum processes that can be simulated efficiently at scale. More on that later.\nTo explain why a quantum computer can be more powerful than a classical, can mean addressing two questions.\n\nWhy does the raw power of the quantum computer grow so fast? In other words, why an arbitrary quantum computation can not be simulated efficiently by a classical machine?\nWhy some of the things that a quantum computer can do, but a classical can not simulate efficiently, are useful? What problems can be efficiently solved quantumly, but not classically?\n\nIn this post, I will mostly focus on the first question, i.e. try to explain what sets a generic quantum computation apart from classical. The second question, which is by all means just as important, I will only touch briefly."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#why-can-quantum-computers-be-powerful",
    "href": "posts/where_quantum_speedups_come_from/index.html#why-can-quantum-computers-be-powerful",
    "title": "Where do quantum speedups come from?",
    "section": "",
    "text": "So there are quantum computers. At least there will be, we believe. And the reason to build them is an expectation that they will solve some problems intractable for the classical computers. It is a new way to process information, a new way to do computation.\nHowever, it is surprisingly difficult to pinpoint the exact mechanism underlying the capabilities of a quantum computer. Most of the simplistic explanations, referencing some unusual property of quantum systems such as entanglement, superposition etc, are incomplete at best. It is hard to compare classical and quantum head-to-head, because of the very different language used to describe them.\nIn this post, I will try to make a fair comparison and zoom in on the essential differences. As an experiment, I will try to cover the topic in a way accessible for people outside the field, striving to provide an explanation that is “as simple as possible, but no simpler”. The discussion still won’t be entirely non-technical, though, and use some basic probability and linear algebra.\n\n\n\n\n\n\nStill, I could not resist supplementing the basic explanation with more technical stuff. Parts, encapsulated like this one, will contain more math and require some quantum background. They can be skipped without breaking the main argument, though.\n\n\n\n\n\n\n\n\n\nAnd occasionally there will be something even more hardcore."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#the-suspects",
    "href": "posts/where_quantum_speedups_come_from/index.html#the-suspects",
    "title": "Where do quantum speedups come from?",
    "section": "",
    "text": "Alright, what property of quantum mechanics is responsible for the potential power of quantum computation? Shouldn’t it be easy to identify? Does it even make sense to ask this question, as the quantum mechanics is just so weird and spooky and a wave and a particle at the same time? Here are the usual suspects.\n\nHuge (exponentially large) dimension of the space where quantum states live.\nQuantum parallelism.\nSuperposition.\nEntanglement.\nContextuality.\n\nDon’t worry if some concepts are unfamiliar, we will elaborate.\nYou can probably add more. While all these things are necessary, neither is sufficient alone. And they are also come in a package, so it may not even be consistent to keep some and discard the others. However, If I had to choose, I’d probably say it’s large state space + superposition, but this may be a matter of taste."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#its-not-really-about-the-physics",
    "href": "posts/where_quantum_speedups_come_from/index.html#its-not-really-about-the-physics",
    "title": "Where do quantum speedups come from?",
    "section": "It’s not really about the physics",
    "text": "It’s not really about the physics\nWell, quantum computing is a way of manipulating information made possible by the laws of quantum mechanics. One way to introduce it is to go through the history of the subject, starting somewhere around Bohr’s atomic model and Einstein’s photoelectric effect and going all the way to Standard Model of particle physics and quantum gravity. While understanding the confusion of the founding fathers is probably an invaluable experience, it is a risky and time-consuming endeavor. In fact, stripped of the historical context and philosophical paradoxes, quantum physics becomes nothing but a generalization of a probability theory. The fact that this mathematical model really underpins the Universe is remarkable, but we need not and will not care much about it’s physical roots."
  },
  {
    "objectID": "posts/where_quantum_speedups_come_from/index.html#quantum-states",
    "href": "posts/where_quantum_speedups_come_from/index.html#quantum-states",
    "title": "Where do quantum speedups come from?",
    "section": "Quantum states",
    "text": "Quantum states\nOk, let’s start getting technical. Quantum computers manipulate quantum states. What are these? The simplest quantum system, known as a qubit (from quantum bit), has two basic states \\(|0\\rangle\\) and \\(|1\\rangle\\). However, it can also be in superposition of these \\[|\\psi\\rangle=a|0\\rangle+b|1\\rangle \\ .\\] What does is mean for a qubit to be in this state? If you ask the qubit, i.e. make a measurement, it will answer “I’m \\(|0\\rangle\\)!” with probability \\(|a|^2\\) and “I’m \\(|1\\rangle\\)!”) with probability \\(|b|^2\\). Of course, we need to have \\(|a|^2+|b|^2=1\\).\nIn this respect, qubit in state \\(|\\psi\\rangle\\) is like an unfair coin, which lands heads with probability \\(|a|^2\\) and tails with probability \\(|b|^2\\). Importantly, this probability distribution is not a property of the qubit itself, but\nTechnically, \\(a\\) and \\(b\\) can be complex numbers, but for simplicity you can think they are just the usual real numbers. What is important, is that they can be negative.\n\nA lot of confusion about the source of quantum advantage.\nUsual suspects\n\nLarge Hilbert space\nEntanglement\nParallelism\nSuperposition\n\nSimple but accurate explanation hard to come by.\nMany simplistic explanations fail because they apply to probabilistic computing as well\n\nLarge space dimension for probabilities\nNo interference\nStochastic matrices can be simulated? Always increase entropy\nQuasi-probability distr, Wigner functions\n\nAnd then there are classical waves.\n\nBut no exponential modes\n\nEntanglement is curios, but not crucial\n\nEntanglement ensures exploring Hilbert space\nStabilizer circuits can be simulated\nQuantum correlations are not directly related to computational advantage\n\nAlgorithms vs quantum circuits.\nPost a bit different, try to give as simple explanation as possible.\nCouldn’t do it, but technical parts separated.\nTakeaway.\n\nDimension of the Hilbert space+superposition given a polynomial amount of qubits\n\nThe answer not completely satisfactory, the subject is subtle.\nMention quantum games!"
  }
]